<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/">

<link rel="icon" src="./Wei Zhai_files/ustc_icon.png">
<link href="./Wei Zhai_files/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="./Wei Zhai_files/main.css" type="text/css">
<link rel="stylesheet" href="./Wei Zhai_files/nomenu.css" type="text/css">
<!--- <title>Wei Zhai</title> --->
<title>Wei Zhai</title>
<!-- MathJax -->
<script src="./Wei Zhai_files/latest.js.下载" async="">
</script>
<script type="text/x-mathjax-config;executed=true">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
<script type="text/javascript" src="./Wei Zhai_files/hidebib.js.下载"></script>

<script type="text/javascript">
function toggleNews() {
    var newsDiv = document.getElementById("more-news");
    var btn = document.getElementById("news-toggle-btn");
    
    if (newsDiv.style.display === "none") {
        newsDiv.style.display = "block";
        btn.innerHTML = "Hide details &#9650;"; 
    } else {
        newsDiv.style.display = "none";
        btn.innerHTML = "Show more news... &#9660;"; 
    }
}
</script>


<script type="text/javascript" async="" src="./Wei Zhai_files/MathJax.js.下载"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-chartest {display: block; visibility: hidden; position: absolute; top: 0; line-height: normal; font-size: 500%}
.mjx-chartest .mjx-char {display: inline}
.mjx-chartest .mjx-box {padding-top: 1000px}
.MJXc-processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MJXc-processed {display: none}
.mjx-test {font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow: hidden; height: 1px}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
#MathJax_CHTML_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.mjx-chtml .mjx-noError {line-height: 1.2; vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
.MJXc-TeX-unknown-R {font-family: STIXGeneral,'Cambria Math','Arial Unicode MS',serif; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: STIXGeneral,'Cambria Math','Arial Unicode MS',serif; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: STIXGeneral,'Cambria Math','Arial Unicode MS',serif; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: STIXGeneral,'Cambria Math','Arial Unicode MS',serif; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}




/* === Profile Section === */

.profile-section {
    display: flex;
    gap: 40px;               /* 图片和文字的间距 */
    align-items: flex-start; /* 顶部对齐 */
    margin-top: 40px;
    margin-bottom: 60px;     /* 离 News 的距离 */
    font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;
    color: #333;
}

/* 左侧：头像容器 */
.profile-avatar {
    flex: 0 0 240px;         /* 固定宽度 */
    text-align: center;
}

.profile-avatar img {
    width: 100%;
    max-width: 240px;
    border-radius: 8px;      /* 圆角 */
    box-shadow: 0 4px 12px rgba(0,0,0,0.1); /* 柔和阴影 */
    border: 1px solid #eee;
    transition: transform 0.3s ease;
}

.profile-avatar img:hover {
    transform: scale(1.02);  /* 鼠标悬停微放大 */
}

/* 右侧：信息容器 */
.profile-info {
    flex: 1;
}

/* 姓名 */
.profile-name {
    font-size: 34px;
    font-weight: 700;
    color: #1a1a1a;
    margin-bottom: 6px;
    line-height: 1.2;
    letter-spacing: -0.5px;
}

/* 职位/头衔 */
.profile-title {
    font-size: 18px;
    color: #555;
    margin-bottom: 18px;
    font-weight: 400;
}
.profile-title strong {
    color: #0066cc; /* USTC Blue */
    font-weight: 600;
}

/* 统一的 Bio 段落 */
.profile-bio {
    font-size: 15px;
    line-height: 1.7;        /* 增加行高，提升阅读舒适度 */
    color: #444;
    margin-bottom: 20px;
    text-align: justify;     /* 两端对齐，让右边缘像报纸一样整齐 */
}

/* 链接样式 (虚线下划线) */
.profile-bio a {
    color: #0066cc;
    text-decoration: none;
    border-bottom: 1px dashed #ccc;
    transition: all 0.2s;
}
.profile-bio a:hover {
    border-bottom-color: #0066cc;
    color: #004c99;
}

/* 按钮组 */
.profile-buttons {
    display: flex;
    flex-wrap: wrap;
    gap: 12px;
}

.link-btn {
    display: inline-block;
    padding: 6px 14px;
    background-color: #f4f6f8; /* 浅灰底 */
    color: #333;
    font-size: 14px;
    font-weight: 600;
    text-decoration: none;
    border-radius: 4px;
    border: 1px solid #e1e4e8;
    transition: all 0.2s;
}
.link-btn:hover {
    background-color: #333;    /* 悬停变深色 */
    color: #fff;
    border-color: #333;
}

/* 移动端适配：屏幕变窄时自动变上下排列 */
@media (max-width: 768px) {
    .profile-section {
        flex-direction: column;
        align-items: center;
        text-align: center;
    }
    .profile-avatar { margin-bottom: 25px; }
    .profile-bio { text-align: left; } /* 移动端文字保持左对齐更好读 */
    .profile-buttons { justify-content: center; }
}





/* === Modern News List (No Scrollbar) === */

.news-container {
    margin-bottom: 50px;
    font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;
}

/* 单条新闻行 */
.news-row {
    display: flex;
    align-items: baseline; /* 文字基线对齐 */
    padding: 6px 0;
    border-bottom: 1px dashed #eaeaea; /* 虚线分割 */
    transition: background-color 0.2s;
}
.news-row:hover {
    background-color: #fafafa; /* 鼠标悬停微高亮 */
}

/* 左侧日期 */
.news-date {
    flex: 0 0 85px; /* 固定宽度 */
    font-size: 15px;
    font-weight: 600;
    color: #444;
    font-family: Consolas, Monaco, monospace; /* 等宽字体让日期对得很齐 */
}

/* 右侧内容 */
.news-content {
    flex: 1;
    font-size: 15px;
    color: #333;
    line-height: 1.5;
}

/* 高亮样式 */
.h-red { color: #d9534f; font-weight: 700; }
.h-blue { color: #0066cc; font-weight: 700; }

/* 徽章 (可选，用于 Awards) */
.news-badge {
    display: inline-block;
    padding: 1px 6px;
    background: #eef0f3;
    color: #0066cc;
    border-radius: 4px;
    font-size: 11px;
    font-weight: bold;
    margin-left: 6px;
    vertical-align: middle;
}

/* === 折叠/展开控制 === */
#news-hidden {
    display: none; /* 默认隐藏旧新闻 */
}

/* 展开按钮样式 */
.news-more-btn {
    display: block;
    width: 100%;
    text-align: center;
    padding: 12px 0;
    margin-top: 15px;
    background-color: #f9f9f9;
    color: #666;
    font-size: 14px;
    font-weight: 600;
    text-decoration: none;
    border-radius: 6px;
    border: 1px solid #eee;
    transition: all 0.2s;
    cursor: pointer;
}
.news-more-btn:hover {
    background-color: #eef0f3;
    color: #333;
}




/* === Experience Section === */

.exp-section {
    margin-bottom: 50px;
    font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;
}

.exp-item {
    display: flex;
    margin-bottom: 0px; /* 项目之间的距离由 padding-bottom 控制，方便画线 */
}

/* 1. 左侧：时间区域 */
.exp-time {
    flex: 0 0 160px;       /* 固定宽度，确保对齐 */
    text-align: right;
    padding-right: 30px;   /* 离中间线的距离 */
    font-size: 15px;
    font-weight: 600;
    color: #555;           /* 深灰色时间 */
    padding-top: 2px;      /* 视觉上与右侧标题对齐 */
    line-height: 1.4;
}

/* 2. 右侧：内容区域 (带左边框线) */
.exp-content {
    flex: 1;
    border-left: 2px solid #eef0f3; /* 浅灰色时间轴线 */
    padding-left: 30px;    /* 内容离线的距离 */
    padding-bottom: 40px;  /* 每个经历下方的留白 */
    position: relative;    /* 用于定位圆点 */
}

/* 最后一个项目去掉左边框线 (可选，或者保留线延伸) */
/* .exp-item:last-child .exp-content { border-left: 2px solid transparent; } */

/* 3. 时间轴上的圆点 */
.exp-content::before {
    content: '';
    position: absolute;
    left: -6px;            /* (线宽2px / 2) - (圆点宽10px / 2) = -1 - 5 = -6px */
    top: 6px;              /* 根据字号微调垂直位置 */
    width: 10px;
    height: 10px;
    background: #fff;      /* 白底 */
    border: 2px solid #0066cc; /* 宝石蓝圆环 (呼应 Publication 的链接色) */
    border-radius: 50%;
    z-index: 1;
}

/* 4. 单位名称 */
.exp-org {
    font-size: 18px;
    font-weight: 700;
    color: #1a1a1a;
    line-height: 1.2;
    margin-bottom: 5px;
}
.exp-org a {
    color: inherit;        /* 继承父元素颜色 */
    text-decoration: none;
}
.exp-org a:hover {
    color: #0066cc;        /* 悬停变蓝 */
}

/* 5. 职位/学位 */
.exp-role {
    font-size: 16px;
    font-weight: 600;
    color: #333;           /* 次深色 */
    margin-bottom: 8px;
}

/* 6. 详细描述 (导师、部门等) */
.exp-desc {
    font-size: 14px;
    color: #666;
    line-height: 1.6;
}
.exp-desc a {
    color: #d9534f;
    text-decoration: none;
}



.year-header {
    font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;
    font-size: 22px;
    font-weight: 600;          /* 特粗 */
    color: #000;
    margin-top: 5px;
    margin-bottom: 5px;
    padding-bottom: 5px;       /* 文字到底部线的距离 */
    border-bottom: 3px solid #000; /* 底部黑线 */
    display: inline-block;     /* 让下划线只跟文字一样宽 */
}



</style>

<style>
/* === Highlight Works === */
.highlight-container {
    display: flex;
    overflow-x: auto;
    gap: 20px;
    padding: 15px 5px 25px 5px;
    scroll-behavior: smooth;
    -webkit-overflow-scrolling: touch;
}

/* === 卡片基础样式 === */
.highlight-card {
    flex: 0 0 240px;
    background: #fff;
    border: 1px solid #e0e0e0;
    border-radius: 8px;
    padding: 12px;
    text-align: left; /* 改为左对齐，阅读体验更好 */
    transition: all 0.3s ease;
    box-shadow: 0 2px 5px rgba(0,0,0,0.05);
    display: flex;
    flex-direction: column;
}

.highlight-card:hover {
    transform: translateY(-5px);
    box-shadow: 0 8px 15px rgba(0,0,0,0.15);
}

/* === 链接样式 === */
.highlight-card a {
    text-decoration: none;
    color: inherit; /* 继承父元素颜色 */
    display: flex;
    flex-direction: column;
    height: 100%;
}

/* === 图片样式 === */
.highlight-card img {
    width: 100%;
    height: 140px;
    object-fit: cover;
    border-radius: 5px;
    margin-bottom: 8px;
    border: 1px solid #f0f0f0;
}

/* === 论文题目样式 === */
.work-title {
    font-size: 14px;
    font-weight: bold;
    color: #2c3e50;
    line-height: 1.3;
    margin-bottom: 6px;
    /* 限制显示两行，超出省略 */
    display: -webkit-box;
    -webkit-line-clamp: 2;
    -webkit-box-orient: vertical;
    overflow: hidden;
}

.highlight-card a:hover .work-title {
    color: #0000FF; /* 悬停时题目变蓝 */
    text-decoration: underline;
}

/* === 会议/期刊名称样式 === */
.work-meta {
    font-size: 12px;
    color: #666;
    margin-top: auto; /* 将其推到底部 */
    line-height: 1.4;
}

/* === 亮点标注样式 (Spotlight, Best Paper 等) === */
.work-highlight {
    font-weight: bold;
    color: #d9534f; /* 红色高亮 */
    display: inline-block;
    margin-top: 2px;
}
.work-highlight.blue {
    color: #0000FF; /* 蓝色高亮 */
}

/* === 滚动条美化 === */
.highlight-container::-webkit-scrollbar { height: 8px; }
.highlight-container::-webkit-scrollbar-thumb { background: #ccc; border-radius: 4px; }
.highlight-container::-webkit-scrollbar-track { background: #f1f1f1; }
</style>





<style>
/* === 高端质感风格布局 === */

.paper-row {
    display: flex;
    padding: 22px 0;           /* 增加上下留白，显得更从容 */
    border-bottom: 1px solid #eaeaea; /* 分割线颜色变得更浅更细 */
    align-items: flex-start;
}

.paper-img {
    flex: 0 0 220px;
    margin-right: 28px;        /* 稍微拉大图文间距 */
    padding-top: 2px;
}

.paper-img img {
    width: 100%;
    border-radius: 6px;        /* 圆角稍微加大 */
    border: 1px solid #f2f2f2; /* 给图片加一个极淡的边框，增加精致感 */
    box-shadow: 0 3px 8px rgba(0,0,0,0.06); /* 阴影更柔和 */
    transition: transform 0.3s ease;
}

.paper-img:hover img {
    transform: scale(1.02);
    box-shadow: 0 6px 15px rgba(0,0,0,0.1);
}

.paper-content {
    flex: 1;
}

/* 1. 题目：深邃、稳重 */
.paper-title {
    display: block;
    font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; /* 确保使用现代无衬线字体 */
    font-size: 18px;           /* 稍微加大字号 */
    font-weight: 600;          /* 半粗体，比 bold 稍微细一点点，更显精致 */
    color: #d9534f;            /* 深炭灰，比纯黑更有质感 */
    margin-bottom: 6px;
    text-decoration: none;
    line-height: 1.35;
    letter-spacing: -0.2px;    /* 稍微收紧字间距，显得紧凑 */
}
.paper-title:hover {
    color: #0066cc;            /* 悬停变为宝石蓝 */
    text-decoration: none;     /* 悬停不去掉下划线，保持清爽，仅变色即可（或者你喜欢下划线可以加） */
}

/* 2. 作者：清晰，突出自己 */
.paper-authors {
    font-size: 15px;
    color: #444;               /* 深灰 */
    line-height: 1.5;
    margin-bottom: 6px;
}
/* 你的名字特殊样式 */
.me {
    color: #000;               /* 你的名字用纯黑 */
    font-weight: 700;
}

/* 3. 会议/期刊：优雅 */
.paper-venue {
    font-size: 15px;
    color: #222;
    margin-bottom: 10px;
}
.venue-name {
    font-style: italic;
    font-weight: 600;          /* 会议简称加粗 */
}

/* 4. 奖项/高亮：高级的红色 */
.highlight-tag {
    color: #d9534f;            /* 砖红色，不刺眼 */
    font-weight: 700;
    font-size: 14px;
    margin-left: 6px;
}
.highlight-tag.blue {
    color: #1a73e8;            /* 谷歌蓝，用于 Distinction 等 */
}

/* 5. 亮点描述：细腻 */
.paper-desc {
    font-size: 14px;
    color: #666;               /* 浅灰 */
    line-height: 1.6;          /* 行高拉大，增加可读性 */
    margin-bottom: 12px;
    font-weight: 400;
}
/* 描述中的强调词 */
.paper-desc b {
    color: #333;               /* 强调词颜色加深 */
    font-weight: 600;
}

/* 6. 链接：统一化 */
.paper-links a {
    font-size: 12px;
    font-weight: 600;
    color: #0066cc;            /* 宝石蓝 */
    text-decoration: none;
    margin-right: 14px;
    text-transform: uppercase; /* 全大写，显得整齐 */
    letter-spacing: 0.5px;
}
.paper-links a:hover {
    color: #004c99;            /* 悬停变深 */
    text-decoration: underline;
}




/* === Services, Awards & Teaching Section === */

.info-section {
    margin-bottom: 50px;
    font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;
}

/* --- 1. Professional Activities (双栏布局) --- */
.service-container {
    display: flex;
    flex-wrap: wrap;
    gap: 40px; /* 两栏之间的间距 */
}

.service-col {
    flex: 1;
    min-width: 300px; /* 移动端自动换行 */
}

.service-header {
    font-size: 18px;
    font-weight: 700;
    color: #1a1a1a;
    margin-bottom: 15px;
    padding-bottom: 8px;
    display: block;
}

.service-list {
    list-style: none;
    padding: 0;
    margin: 0;
}

.service-list li {
    font-size: 15px;
    color: #444;
    margin-bottom: 8px;
    padding-left: 15px;
    position: relative;
    line-height: 1.5;
}

.service-list li::before {
    content: '•';
    color: #0066cc; /* 蓝点装饰 */
    position: absolute;
    left: 0;
    font-weight: bold;
}

/* --- 2. Awards & Teaching Rows (左侧时间，右侧内容) --- */
.info-row {
    display: flex;
    padding: 12px 0;
    border-bottom: 1px solid #f5f5f5; /* 极淡的分割线 */
    align-items: baseline;
    transition: background 0.2s;
}
.info-row:hover {
    background-color: #fafbfd; /* 悬停微高亮 */
}
.info-row:last-child { border-bottom: none; }

.info-label {
    flex: 0 0 100px; /* 左侧固定宽度 (年份) */
    font-weight: 700;
    color: #555;
    font-size: 15px;
    font-family: Consolas, Monaco, monospace; /* 等宽字体数字 */
}

.info-content {
    flex: 1;
    font-size: 15px;
    color: #222;
    line-height: 1.5;
}

/* 奖项高亮徽章 */
.award-badge {
    display: inline-block;
    padding: 1px 8px;
    border-radius: 4px;
    font-size: 12px;
    font-weight: 700;
    margin-left: 8px;
    vertical-align: middle;
    text-transform: uppercase;
}
.badge-red { 
    background-color: #fff5f5; 
    color: #d9534f; 
    border: 1px solid #ffebeb;
}
.badge-blue { 
    background-color: #f0f7ff; 
    color: #0066cc; 
    border: 1px solid #e1f0ff;
}
.badge-gold {
    background-color: #fff9e6;
    color: #b58900;
    border: 1px solid #fff3cd;
}




</style>




</head>
<body><div id="MathJax_Message" style="display: none;"></div>
<div id="main-container">
<div id="layout-content"> <!-- nomenu  -->
<div id="toptitle">



<div class="profile-section">
    
    <div class="profile-avatar">
        <img src="./Wei Zhai_files/1.jpg" alt="Wei Zhai">
    </div>

    <div class="profile-info">
        
        <div class="profile-name">Wei Zhai (翟伟)</div>
        
        <div class="profile-title">
            Associate Researcher at <br>
            <strong>University of Science and Technology of China (USTC)</strong>
        </div>

        <div class="profile-bio">
            I am currently an Associate Researcher at the <a href="https://auto.ustc.edu.cn/" target="_blank">Department of Automation</a>, USTC. 
            I obtained my Ph.D. degree from USTC in 2022, advised by Prof. <a href="https://faculty.ustc.edu.cn/chazhengjun/zh_CN/index.htm" target="_blank">Zheng-Jun Zha</a> and Prof. <a href="https://staff.iaticetc.cn:1234/" target="_blank">Yang Cao</a>. 
            Prior to that, I received my B.S. degree from Southwest Jiaotong University in 2017. 
            I was fortunate to receive the <b>AAAI 2023 Distinguished Paper Award</b> and the <b>ACM MM 2025 MSMA Workshop Best Student Paper Award</b>.
            My research interests primarily lie in <b>Computer Vision</b>, <b>Embodied Intelligence</b>, and <b>Machine Learning</b>, with a specific focus on building efficient computational frameworks inspired by brain mechanisms, developing egocentric perception for interaction anticipation, and endowing embodied agents with generalizable 2D/3D vision skills in complex real-world scenes.
        </div>

        <div class="profile-buttons">
            <a href="mailto:wzhai056@ustc.edu.cn" class="link-btn">Email</a>
            <a href="https://faculty.ustc.edu.cn/zhaiwei/zh_CN/index.htm" target="_blank" class="link-btn">Homepage</a>
            <a href="https://scholar.google.ca/citations?user=UI5_qZcAAAAJ&hl=en" target="_blank" class="link-btn">Google Scholar</a>
            </div>

    </div>
</div>



<h2>News</h2>
<div class="news-container">

    <div class="news-row">
        <div class="news-date">11/2025</div>
        <div class="news-content">
            <span class="h-red">1</span> paper accepted by <span class="h-red">AAAI 2025</span>.
        </div>
    </div>

    <div class="news-row">
        <div class="news-date">10/2025</div>
        <div class="news-content">
            <span class="h-red">1</span> paper accepted by <span class="h-red">T-NNLS</span>.
        </div>
    </div>

    <div class="news-row">
        <div class="news-date">09/2025</div>
        <div class="news-content">
            <span class="h-red">3</span> papers accepted by <span class="h-red">NeurIPS 2025</span>.
            <span class="news-badge">1 Spotlight</span>
        </div>
    </div>

    <div class="news-row">
        <div class="news-date">09/2025</div>
        <div class="news-content">
            <span class="h-red">1</span> paper accepted by <span class="h-red">ACM MM MSMA Workshop</span>.
            <span class="news-badge">Best Student Paper</span>
        </div>
    </div>

    <div class="news-row">
        <div class="news-date">07/2025</div>
        <div class="news-content">
            <span class="h-red">1</span> paper accepted by <span class="h-red">Journal of Intelligent Computing and Networking</span>.
        </div>
    </div>

    <div class="news-row">
        <div class="news-date">07/2025</div>
        <div class="news-content">
            <span class="h-red">1</span> paper accepted by <span class="h-red">T-ASE</span>.
        </div>
    </div>

    <div class="news-row">
        <div class="news-date">06/2025</div>
        <div class="news-content">
            <span class="h-red">4</span> papers accepted by <span class="h-red">ICCV 2025</span>.
        </div>
    </div>

    <div class="news-row">
        <div class="news-date">06/2025</div>
        <div class="news-content">
            Won the <span class="h-blue">1st Place</span> in <span class="h-red">Efficient Event-based Eye-Tracking Challenge</span>.
        </div>
    </div>
    
    <div class="news-row">
        <div class="news-date">06/2025</div>
        <div class="news-content">
            Won the <span class="h-blue">1st Place</span> in <span class="h-red">Body Contact Estimation Challenge (RHOBIN2025 CVPR)</span>.
        </div>
    </div>

    <div class="news-row">
        <div class="news-date">05/2025</div>
        <div class="news-content">
            <span class="h-red">1</span> paper accepted by <span class="h-red">SCIENCE CHINA Information Sciences</span>.
        </div>
    </div>

    <div class="news-row">
        <div class="news-date">02/2025</div>
        <div class="news-content">
            <span class="h-red">5</span> papers accepted by <span class="h-red">CVPR 2025</span>.
            <span class="news-badge">1 Highlight</span>
        </div>
    </div>

    <div id="news-hidden">
        
        <div class="news-row">
            <div class="news-date">09/2024</div>
            <div class="news-content">
                <span class="h-red">1</span> paper accepted by <span class="h-red">NeurIPS 2024</span>.
            </div>
        </div>

        <div class="news-row">
            <div class="news-date">07/2024</div>
            <div class="news-content">
                <span class="h-red">1</span> paper accepted by <span class="h-red">ACM MM 2024</span>.
            </div>
        </div>

        <div class="news-row">
            <div class="news-date">07/2024</div>
            <div class="news-content">
                <span class="h-red">1</span> paper accepted by <span class="h-red">T-IP</span>.
            </div>
        </div>

        <div class="news-row">
            <div class="news-date">07/2024</div>
            <div class="news-content">
                <span class="h-red">1</span> paper accepted by <span class="h-red">ECCV 2024</span>.
            </div>
        </div>

        <div class="news-row">
            <div class="news-date">06/2024</div>
            <div class="news-content">
                Won the <span class="h-blue">2nd Place</span> in <span class="h-red">3D Contact Estimation Challenge (RHOBIN2024 CVPR)</span>.
            </div>
        </div>

        <div class="news-row">
            <div class="news-date">04/2024</div>
            <div class="news-content">
                <span class="h-red">1</span> paper accepted by <span class="h-red">Optics Express</span>.
            </div>
        </div>

        <div class="news-row">
            <div class="news-date">04/2024</div>
            <div class="news-content">
                <span class="h-red">1</span> paper accepted by <span class="h-red">T-AI</span>.
            </div>
        </div>

        <div class="news-row">
            <div class="news-date">03/2024</div>
            <div class="news-content">
                Won the <span class="h-blue">2nd Place</span> in <span class="h-red">Efficient Super-Resolution Challenge (NTIRE2024 CVPR)</span>.
            </div>
        </div>

        <div class="news-row">
            <div class="news-date">03/2024</div>
            <div class="news-content">
                Won the <span class="h-blue">1st Place</span> in <span class="h-red">Event-based Eye Tracking Task (AIS2024 CVPR)</span>.
            </div>
        </div>

        <div class="news-row">
            <div class="news-date">02/2024</div>
            <div class="news-content">
                <span class="h-red">1</span> paper accepted by <span class="h-red">CVPR 2024</span>.
            </div>
        </div>

        <div class="news-row">
            <div class="news-date">12/2023</div>
            <div class="news-content">
                <span class="h-red">1</span> paper accepted by <span class="h-red">AAAI 2024</span>.
            </div>
        </div>

        <div class="news-row">
            <div class="news-date">11/2023</div>
            <div class="news-content">
                <span class="h-red">1</span> paper accepted by <span class="h-red">IJCV</span>.
            </div>
        </div>

        <div class="news-row">
            <div class="news-date">10/2023</div>
            <div class="news-content">
                <span class="h-red">1</span> paper accepted by <span class="h-red">T-PAMI</span>.
            </div>
        </div>

        <div class="news-row">
            <div class="news-date">09/2023</div>
            <div class="news-content">
                <span class="h-red">1</span> paper accepted by <span class="h-red">IJCV</span>.
            </div>
        </div>

        <div class="news-row">
            <div class="news-date">07/2023</div>
            <div class="news-content">
                <span class="h-red">1</span> paper accepted by <span class="h-red">T-NNLS</span>.
            </div>
        </div>

        <div class="news-row">
            <div class="news-date">07/2023</div>
            <div class="news-content">
                <span class="h-red">2</span> papers accepted by <span class="h-red">ICCV 2023</span>.
            </div>
        </div>

        <div class="news-row">
            <div class="news-date">03/2023</div>
            <div class="news-content">
                <span class="h-red">2</span> papers accepted by <span class="h-red">CVPR 2023</span>.
            </div>
        </div>

        <div class="news-row">
            <div class="news-date">01/2023</div>
            <div class="news-content">
                <span class="h-red">1</span> paper accepted by <span class="h-red">AAAI 2023</span>.
                <span class="news-badge">Distinguished Paper</span>
            </div>
        </div>

        <div class="news-row">
            <div class="news-date">09/2022</div>
            <div class="news-content">
                <span class="h-red">1</span> paper accepted by <span class="h-red">NeurIPS 2022</span>.
            </div>
        </div>

    </div>
    <a href="javascript:toggleNewsHistory()" class="news-more-btn" id="news-btn-text">
        View History News... &#9660;
    </a>

</div>


<h2>Experience</h2>
<div class="exp-section">

    <div class="exp-item">
        <div class="exp-time">
            Jul 2024 - Present
        </div>
        <div class="exp-content">
            <div class="exp-org">
                <a href="http://www.ustc.edu.cn/" target="_blank">University of Science and Technology of China</a>
            </div>
            <div class="exp-role">Associate Researcher</div>
            <div class="exp-desc">
                <a href="https://auto.ustc.edu.cn/" target="_blank">Department of Automation</a>
            </div>
        </div>
    </div>

    <div class="exp-item">
        <div class="exp-time">
            Jul 2022 - Jun 2024
        </div>
        <div class="exp-content">
            <div class="exp-org">
                <a href="http://www.ustc.edu.cn/" target="_blank">University of Science and Technology of China</a>
            </div>
            <div class="exp-role">Postdoctoral Researcher</div>
            <div class="exp-desc">
                Department of Automation <br>
                Advisor: Prof. <a href="https://faculty.ustc.edu.cn/chazhengjun/zh_CN/index.htm">Zheng-Jun Zha</a> and Prof. <a href="https://staff.iaticetc.cn:1234/">Yang Cao</a>
            </div>
        </div>
    </div>

    <div class="exp-item">
        <div class="exp-time">
            Sep 2017 - Jun 2022
        </div>
        <div class="exp-content">
            <div class="exp-org">
                <a href="http://www.ustc.edu.cn/" target="_blank">University of Science and Technology of China</a>
            </div>
            <div class="exp-role">Ph.D. in Cyberspace Security</div>
            <div class="exp-desc">
                School of Cyberspace Security <br>
                Advisor: Prof. <a href="https://faculty.ustc.edu.cn/chazhengjun/zh_CN/index.htm">Zheng-Jun Zha</a> and Prof. <a href="https://staff.iaticetc.cn:1234/">Yang Cao</a>
            </div>
        </div>
    </div>

    <div class="exp-item">
        <div class="exp-time">
            Dec 2020 - Sep 2021
        </div>
        <div class="exp-content">
            <div class="exp-org">
                <a href="http://cybersec.ustc.edu.cn//" target="_blank">JD Explore Academy</a>
            </div>
            <div class="exp-role">Research Intern</div>
            <div class="exp-desc">
                Mentor: Prof. <a href="https://scholar.google.ca/citations?user=RwlJNLcAAAAJ&hl=en">Dacheng Tao</a> and Dr. <a href="https://scholar.google.ca/citations?user=9jH5v74AAAAJ&hl=en">Jing Zhang</a>
            </div>
        </div>
    </div>

    <div class="exp-item">
        <div class="exp-time">
            Sep 2013 - Jun 2017
        </div>
        <div class="exp-content">
            <div class="exp-org">
                <a href="https://www.swjtu.edu.cn/" target="_blank">Southwest Jiaotong University</a>
            </div>
            <div class="exp-role">B.S. in Computer Science</div>
            <div class="exp-desc">
                Outstanding Graduate of Southwest Jiaotong University (2017)
            </div>
        </div>
    </div>

</div>


<h2>Publications</h2>

<div class="year-header">2025</div>


<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/han2025e.png" alt="E-MaT">
    </div>

    <div class="paper-content">
        <a href="https:" class="paper-title">
            E-MaT: Event-oriented Mamba for Egocentric Point Tracking
        </a>
        
        <div class="paper-authors">
            Han Han, <span class="me">Wei Zhai*</span>, Baocai Yin, Yang Cao, Bin Li, Zheng-Jun Zha.
        </div>
        
        <div class="paper-venue">
            In <span class="venue-name">AAAI 2025</span>
        </div>

        <div class="paper-desc">
            We propose a <b>Mamba-based tracking framework</b> that leverages <b>event cameras</b> to capture global motion trends, significantly enhancing egocentric point tracking robustness under <b>fast motion</b> and high dynamic range conditions.
        </div>

        <div class="paper-links">
            <a href="https:">PDF</a>
            <a href="javascript:toggleblock('han2025e-bib')">BibTeX</a>
        </div>
        
        <div id="han2025e-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/wang2025towards.png" alt="De-raining Generalization">
    </div>

    <div class="paper-content">
        <a href="https://arxiv.org/abs/2506.02477" class="paper-title">
            Towards Better De-raining Generalization via Rainy Characteristics Memorization and Replay
        </a>
        
        <div class="paper-authors">
            Kunyu Wang, Xueyang Fu, Chengzhi Cao, Chengjie Ge, <span class="me">Wei Zhai</span>, Zheng-Jun Zha.
        </div>
        
        <div class="paper-venue">
            In <span class="venue-name">IEEE T-NNLS</span>
        </div>

        <div class="paper-desc">
            We introduce a continuous learning framework inspired by the <b>complementary learning system</b> of the human brain, utilizing <b>memory replay</b> and knowledge distillation to enable de-raining networks to generalize across varied real-world scenarios.
        </div>

        <div class="paper-links">
            <a href="https://arxiv.org/abs/2506.02477">PDF</a>
            <a href="javascript:toggleblock('wang2025towards-bib')">BibTeX</a>
        </div>
        
        <div id="wang2025towards-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{wang2025towards,
  title={Towards Better De-raining Generalization via Rainy Characteristics Memorization and Replay},
  author={Wang, Kunyu and Fu, Xueyang and Cao, Chengzhi and Ge, Chengjie and Zhai, Wei and Zha, Zheng-Jun},
  journal={arXiv preprint arXiv:2506.02477},
  year={2025}
}
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/liao2024ef.png" alt="EF-3DGS">
    </div>

    <div class="paper-content">
        <a href="https://arxiv.org/pdf/2410.15392" class="paper-title">
            EF-3DGS: Event-Aided Free-Trajectory 3D Gaussian Splatting
        </a>
        
        <div class="paper-authors">
            Bohao Liao, <span class="me">Wei Zhai*</span>, Zengyu Wan, Zhixin Cheng, Wenfei Yang, Yang Cao, Tianzhu Zhang, Zheng-Jun Zha.
        </div>
        
        <div class="paper-venue">
            In <span class="venue-name">NeurIPS 2025</span>
            <span class="highlight-tag">(Spotlight)</span>
        </div>

        <div class="paper-desc">
            We propose <b>EF-3DGS</b>, the first event-aided framework to handle <b>fast motion blur</b> and <b>high dynamic range</b> scenes by fusing events and frames, achieving significantly higher PSNR and lower trajectory error in high-speed scenarios.
        </div>

        <div class="paper-links">
            <a href="https://arxiv.org/pdf/2410.15392">PDF</a>
            <a href="https://lbh666.github.io/ef-3dgs/">Code</a>
            <a href="javascript:toggleblock('liao2024ef-bib-list')">BibTeX</a>
        </div>
        
        <div id="liao2024ef-bib-list" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{liao2024ef,
title={EF-3DGS: Event-Aided Free-Trajectory 3D Gaussian Splatting},
author={Liao, Bohao and Zhai, Wei and Wan, Zengyu and Zhang, Tianzhu and Cao, Yang and Zha, Zheng-Jun},
journal={arXiv preprint arXiv:2410.15392},
year={2024}
}
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/fang2025panoramic.png" alt="ViewPoint">
    </div>

    <div class="paper-content">
        <a href="https://arxiv.org/abs/2506.23513" class="paper-title">
            ViewPoint: Panoramic Video Generation with Pretrained Diffusion Models
        </a>
        
        <div class="paper-authors">
            Zixun Fang, Kai Zhu, Zhiheng Liu, Yu Liu, <span class="me">Wei Zhai</span>, Yang Cao, Zheng-Jun Zha.
        </div>
        
        <div class="paper-venue">
            In <span class="venue-name">NeurIPS 2025</span>
        </div>

        <div class="paper-desc">
            We propose a novel framework utilizing <b>pretrained perspective diffusion models</b> for generating panoramic videos via a new <b>ViewPoint map</b> representation, ensuring global spatial continuity and fine-grained visual details.
        </div>

        <div class="paper-links">
            <a href="https://arxiv.org/abs/2506.23513">PDF</a>
            <a href="https://becauseimbatman0.github.io/ViewPoint">Project</a>
            <a href="javascript:toggleblock('fang2025panoramic-bib')">BibTeX</a>
        </div>
        
        <div id="fang2025panoramic-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{fang2025panoramic,
  title={Panoramic Video Generation with Pretrained Diffusion Models},
  author={Fang, Zixun and Zhu, Kai and Liu, Zhiheng and Liu, Yu and Zhai, Wei and Cao, Yang and Zha, Zheng-Jun},
  journal={arXiv preprint arXiv:2506.23513},
  year={2025}
}
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/wang2025paid.jpg" alt="PAID">
    </div>
    <div class="paper-content">
        <a href="https://arxiv.org/abs/2506.02453" class="paper-title">
            PAID: Pairwise Angular-Invariant Decomposition for Continual Test-Time Adaptation
        </a>
        <div class="paper-authors">
            Kunyu Wang, Xueyang Fu, Yuanfei Bao, Chengjie Ge, Chengzhi Cao, <span class="me">Wei Zhai</span>, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">NeurIPS 2025</span>
        </div>
        <div class="paper-desc">
            We propose <b>PAID</b>, a prior-driven CTTA method that preserves the <b>pairwise angular structure</b> of pre-trained weights using <b>Householder reflections</b>, achieving consistent improvements in <b>continual test-time adaptation</b>.
        </div>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2506.02453">PDF</a>
            <a href="https://github.com/wangkunyu241/PAID">Code</a>
            <a href="javascript:toggleblock('wang2025paid-bib')">BibTeX</a>
        </div>
        <div id="wang2025paid-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{wang2025paid,
  title={PAID: Pairwise Angular-Invariant Decomposition for Continual Test-Time Adaptation},
  author={Wang, Kunyu and Fu, Xueyang and Bao, Yuanfei and Ge, Chengjie and Cao, Chengzhi and Zhai, Wei and Zha, Zheng-Jun},
  journal={arXiv preprint arXiv:2506.02453},
  year={2025}
}
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/huang2024leverage.png" alt="Affordance Ranking">
    </div>
    <div class="paper-content">
        <a href="https://arxiv.org/abs/2411.16082" class="paper-title">
            Learning Object Affordance Ranking with Task Context
        </a>
        <div class="paper-authors">
            Haojie Huang, Hongchen Luo*, <span class="me">Wei Zhai*</span>, Yang Cao, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">ACM MM 2025 MSMA Workshop</span>
            <span class="highlight-tag">(Best Student Paper)</span>
        </div>
        <div class="paper-desc">
            We introduce a <b>Context-embed Group Ranking Framework</b> to learn <b>object affordance ranking</b> by deeply integrating <b>task context</b>, supported by a new large-scale <b>task-oriented dataset</b>.
        </div>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2411.16082">PDF</a>
            <a href="https://wpy1999.github.io/IV-VAE/">Code</a>
            <a href="javascript:toggleblock('huang2024leverage-bib')">BibTeX</a>
        </div>
        <div id="huang2024leverage-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{huang2024leverage,
title={Leverage Task Context for Object Affordance Ranking},
author={Huang, Haojie and Luo, Hongchen and Zhai, Wei and Cao, Yang and Zha, Zheng-Jun},
journal={arXiv preprint arXiv:2411.16082},
year={2024}
}
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/yang2025sigman.gif" alt="SIGMAN">
    </div>
    <div class="paper-content">
        <a href="https://arxiv.org/abs/2504.06982" class="paper-title">
            SIGMAN: Scaling 3D Human Gaussian Generation with Millions of Assets
        </a>
        <div class="paper-authors">
            Yuhang Yang, Fengqi Liu, Yixing Lu, Qin Zhao, Pingyu Wu, <span class="me">Wei Zhai*</span>, Ran Yi, Yang Cao, Lizhuang Ma, Zheng-Jun Zha, Junting Dong*.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">ICCV 2025</span>
        </div>
        <div class="paper-desc">
            We present <b>SIGMAN</b>, a <b>latent space generation paradigm</b> for 3D human digitization utilizing a <b>UV-structured VAE</b> and <b>DiT</b>, trained on a newly constructed dataset of <b>1 million 3D Gaussian assets</b>.
        </div>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2504.06982">PDF</a>
            <a href="https://yyvhang.github.io/SIGMAN_3D/">Code</a>
            <a href="javascript:toggleblock('yang2025sigman-bib')">BibTeX</a>
        </div>
        <div id="yang2025sigman-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{yang2025sigman,
  title={SIGMAN: Scaling 3D Human Gaussian Generation with Millions of Assets},
  author={Yang, Yuhang and Liu, Fengqi and Lu, Yixing and Zhao, Qin and Wu, Pingyu and Zhai, Wei and Yi, Ran and Cao, Yang and Ma, Lizhuang and Zha, Zheng-Jun and others},
  journal={arXiv preprint arXiv:2504.06982},
  year={2025}
}
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/yu2025hero.png" alt="HERO">
    </div>
    <div class="paper-content">
        <a href="https://arxiv.org/abs/2503.08270" class="paper-title">
            HERO: Human Reaction Generation from Videos
        </a>
        <div class="paper-authors">
            Chengjun Yu, <span class="me">Wei Zhai*</span>, Yuhang Yang, Yang Cao, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">ICCV 2025</span>
        </div>
        <div class="paper-desc">
            We propose <b>HERO</b>, a framework for <b>human reaction generation</b> from RGB videos that extracts <b>interaction intention</b> and local visual cues, validated on a new <b>Video-Motion dataset</b>.
        </div>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2503.08270">PDF</a>
            <a href="https://jackyu6.github.io/HERO/">Code</a>
            <a href="javascript:toggleblock('yu2025hero-bib')">BibTeX</a>
        </div>
        <div id="yu2025hero-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@inproceedings{yu2025hero,
title={HERO: Human Reaction Generation from Videos},
author={Yu, Chengjun and Zhai, Wei and Yang, Yuhang and Cao, Yang and Zha, Zheng-Jun},
journal={arXiv preprint arXiv:2503.08270},
year={2025}
}
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/han2024event.png" alt="MATE">
    </div>
    <div class="paper-content">
        <a href="https://arxiv.org/abs/2412.01300" class="paper-title">
            MATE: Motion-Augmented Temporal Consistency for Event-based Point Tracking
        </a>
        <div class="paper-authors">
            Han Han, <span class="me">Wei Zhai*</span>, Yang Cao, Bin Li, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">ICCV 2025</span>
        </div>
        <div class="paper-desc">
            We introduce <b>MATE</b>, an <b>event-based point tracking</b> framework that resolves spatial sparsity and motion blur through <b>motion-augmented temporal consistency</b>, achieving significantly faster processing and higher precision.
        </div>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2412.01300">PDF</a>
            <a href="https://hh-xiaohu.github.io/Ev-TAP/">Code</a>
            <a href="javascript:toggleblock('han2024event-bib')">BibTeX</a>
        </div>
        <div id="han2024event-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{han2024event,
title={Event-Based Tracking Any Point with Motion-Augmented Temporal Consistency},
author={Han, Han and Zhai, Wei and Cao, Yang and Li, Bin and Zha, Zheng-jun},
journal={arXiv preprint arXiv:2412.01300},
year={2024}
}
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/wan2025emotive.png" alt="EMoTive">
    </div>
    <div class="paper-content">
        <a href="https://arxiv.org/abs/2503.11371" class="paper-title">
            EMoTive: Event-guided Trajectory Modeling for 3D Motion Estimation
        </a>
        <div class="paper-authors">
            Zengyu Wan, <span class="me">Wei Zhai*</span>, Yang Cao, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">ICCV 2025</span>
        </div>
        <div class="paper-desc">
            We propose <b>EMoTive</b>, an event-based framework for <b>3D motion estimation</b> that models spatio-temporal trajectories via <b>Event Kymograph</b> projection and <b>non-uniform parametric curves</b>.
        </div>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2503.11371">PDF</a>
            <a href="javascript:toggleblock('wan2025emotive-bib')">BibTeX</a>
        </div>
        <div id="wan2025emotive-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{wan2025emotive,
  title={EMoTive: Event-guided Trajectory Modeling for 3D Motion Estimation},
  author={Wan, Zengyu and Zhai, Wei and Cao, Yang and Zha, Zhengjun},
  journal={arXiv preprint arXiv:2503.11371},
  year={2025}
}
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/zhang2024pear.png" alt="PEAR">
    </div>
    <div class="paper-content">
        <a href="https://arxiv.org/pdf/2407.21510" class="paper-title">
            PEAR: Phrase-Based Hand-Object Interaction Anticipation
        </a>
        <div class="paper-authors">
            Zichen Zhang, Hongchen Luo, <span class="me">Wei Zhai</span>, Yang Cao, Yu Kang.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">SCIENCE CHINA Information Sciences (SCIS)</span>
        </div>
        <div class="paper-desc">
            We present <b>PEAR</b>, a model for <b>hand-object interaction anticipation</b> that jointly predicts intention and manipulation using <b>phrase-based cross-alignment</b>, supported by the <b>EGO-HOIP dataset</b>.
        </div>
        <div class="paper-links">
            <a href="https://arxiv.org/pdf/2407.21510">PDF</a>
            <a href="javascript:toggleblock('zhang2024pear-bib')">BibTeX</a>
        </div>
        <div id="zhang2024pear-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{zhang2024pear,
    title={PEAR: Phrase-Based Hand-Object Interaction Anticipation},
    author={Zhang, Zichen and Luo, Hongchen and Zhai, Wei and Cao, Yang and Kang, Yu},
    journal={arXiv preprint arXiv:2407.21510},
    year={2024}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/wu2025brat.png" alt="BRAT">
    </div>
    <div class="paper-content">
        <a href="https://openaccess.thecvf.com/content/CVPR2025W/EventVision/papers/Wu_BRAT_Bidirectional_Relative_Positional_Attention_Transformer_for_Event-based_Eye_tracking_CVPRW_2025_paper.pdf" class="paper-title">
            BRAT: Bidirectional Relative Positional Attention Transformer for Event-based Eye tracking
        </a>
        <div class="paper-authors">
            Yuliang Wu, Han Han, Jinze Chen, <span class="me">Wei Zhai*</span>, Yang Cao, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">CVPR 2025 Workshop</span>
            <span class="highlight-tag">(1st Place Challenge)</span>
        </div>
        <div class="paper-desc">
            We propose <b>BRAT</b>, a <b>Bidirectional Relative Positional Attention Transformer</b> for event-based <b>eye tracking</b> that fully exploits spatio-temporal sequences, winning 1st place in the <b>Efficient Event-based Eye-Tracking Challenge</b>.
        </div>
        <div class="paper-links">
            <a href="https://openaccess.thecvf.com/content/CVPR2025W/EventVision/papers/Wu_BRAT_Bidirectional_Relative_Positional_Attention_Transformer_for_Event-based_Eye_tracking_CVPRW_2025_paper.pdf">PDF</a>
            <a href="javascript:toggleblock('wu2025brat-bib')">BibTeX</a>
        </div>
        <div id="wu2025brat-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@inproceedings{wu2025brat,
  title={BRAT: Bidirectional Relative Positional Attention Transformer for Event-based Eye tracking},
  author={Wu, Yuliang and Han, Han and Chen, Jinze and Zhai, Wei and Cao, Yang and Zha, Zheng-jun},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={5136--5144},
  year={2025}
}
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/lu2024benchmarking.png" alt="CompreCap">
    </div>
    <div class="paper-content">
        <a href="https://arxiv.org/abs/2412.08614" class="paper-title">
            Benchmarking Large Vision-Language Models via Directed Scene Graph for Comprehensive Image Captioning
        </a>
        <div class="paper-authors">
            Fan Lu, Wei Wu, Kecheng Zheng*, Shuailei Ma, Biao Gong, Jiawei Liu, <span class="me">Wei Zhai*</span>, Yang Cao, Yujun Shen, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">CVPR 2025</span>
        </div>
        <div class="paper-desc">
            We introduce <b>CompreCap</b>, a benchmark for evaluating <b>detailed image captioning</b> in LVLMs using a <b>directed scene graph</b> to assess object coverage, attributes, and relationships comprehensively.
        </div>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2412.08614">PDF</a>
            <a href="https://github.com/LuFan31/CompreCap">Code</a>
            <a href="javascript:toggleblock('lu2024benchmarking-bib')">BibTeX</a>
        </div>
        <div id="lu2024benchmarking-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{lu2024benchmarking,
    title={Benchmarking Large Vision-Language Models via Directed Scene Graph for Comprehensive Image Captioning},
    author={Lu, Fan and Wu, Wei and Zheng, Kecheng and Ma, Shuailei and Gong, Biao and Liu, Jiawei and Zhai, Wei and Cao, Yang and Shen, Yujun and Zha, Zheng-Jun},
    journal={arXiv preprint arXiv:2412.08614},
    year={2024}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/shao2024great.png" alt="GREAT">
    </div>
    <div class="paper-content">
        <a href="https://arxiv.org/abs/2411.19626" class="paper-title">
            GREAT: Geometry-Intention Collaborative Inference for Open-Vocabulary 3D Object Affordance Grounding
        </a>
        <div class="paper-authors">
            Yawen Shao, <span class="me">Wei Zhai*</span>, Yuhang Yang, Hongchen Luo, Yang Cao, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">CVPR 2025</span>
        </div>
        <div class="paper-desc">
            We propose <b>GREAT</b>, a framework for <b>open-vocabulary 3D object affordance grounding</b> that combines <b>geometry attributes</b> with interaction intention reasoning, verified on the large-scale <b>PIADv2 dataset</b>.
        </div>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2411.19626">PDF</a>
            <a href="https://yawen-shao.github.io/GREAT/">Code</a>
            <a href="javascript:toggleblock('shao2024great-bib')">BibTeX</a>
        </div>
        <div id="shao2024great-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{shao2024great,
title={GREAT: Geometry-Intention Collaborative Inference for Open-Vocabulary 3D Object Affordance Grounding},
author={Shao, Yawen and Zhai, Wei and Yang, Yuhang and Luo, Hongchen and Cao, Yang and Zha, Zheng-Jun},
journal={arXiv preprint arXiv:2411.19626},
year={2024}
}
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/wu2024improved.png" alt="IV-VAE">
    </div>
    <div class="paper-content">
        <a href="https://arxiv.org/abs/2411.06449" class="paper-title">
            Improved Video VAE for Latent Video Diffusion Model
        </a>
        <div class="paper-authors">
            Pingyu Wu, Kai Zhu*, Yu Liu, Liming Zhao, <span class="me">Wei Zhai*</span>, Yang Cao, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">CVPR 2025</span>
        </div>
        <div class="paper-desc">
            We propose an <b>Improved Video VAE (IV-VAE)</b> featuring <b>Keyframe-based Temporal Compression</b> and <b>Group Causal Convolution</b> to resolve temporal-spatial conflicts in latent video diffusion models.
        </div>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2411.06449">PDF</a>
            <a href="https://wpy1999.github.io/IV-VAE/">Code</a>
            <a href="javascript:toggleblock('wu2024improved-bib')">BibTeX</a>
        </div>
        <div id="wu2024improved-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{wu2024improved,
title={Improved Video VAE for Latent Video Diffusion Model},
author={Wu, Pingyu and Zhu, Kai and Liu, Yu and Zhao, Liming and Zhai, Wei and Cao, Yang and Zha, Zheng-Jun},
journal={arXiv preprint arXiv:2411.06449},
year={2024}
}
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/yang2024mmar.png" alt="MMAR">
    </div>
    <div class="paper-content">
        <a href="https://arxiv.org/pdf/2410.10798?" class="paper-title">
            MMAR: Towards Lossless Multi-Modal Auto-Regressive Prababilistic Modeling
        </a>
        <div class="paper-authors">
            Jian Yang, Dacheng Yin, Yizhou Zhou, Fengyun Rao, <span class="me">Wei Zhai</span>, Yang Cao, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">CVPR 2025</span>
        </div>
        <div class="paper-desc">
            We introduce <b>MMAR</b>, a <b>lossless multi-modal auto-regressive</b> framework that uses continuous-valued image tokens and a lightweight <b>diffusion head</b> to unify image understanding and generation without information loss.
        </div>
        <div class="paper-links">
            <a href="https://arxiv.org/pdf/2410.10798?">PDF</a>
            <a href="javascript:toggleblock('yang2024mmar-bib')">BibTeX</a>
        </div>
        <div id="yang2024mmar-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{yang2024mmar,
    title={MMAR: Towards Lossless Multi-Modal Auto-Regressive Prababilistic Modeling},
    author={Yang, Jian and Yin, Dacheng and Zhou, Yizhou and Rao, Fengyun and Zhai, Wei and Cao, Yang and Zha, Zheng-Jun},
    journal={arXiv preprint arXiv:2410.10798},
    year={2024}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/wang2025efficient.png" alt="Efficient CTTA-OD">
    </div>
    <div class="paper-content">
        <a href="https://arxiv.org/abs/" class="paper-title">
            Efficient Test-time Adaptive Object Detection via Sensitivity-Guided Pruning
        </a>
        <div class="paper-authors">
            Kunyu Wang, Xueyang Fu, Xin Lu, Chengjie Ge, Chengzhi Cao, <span class="me">Wei Zhai</span>, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">CVPR 2025</span>
            <span class="highlight-tag">(Highlight)</span>
        </div>
        <div class="paper-desc">
            We propose an <b>efficient CTTA-OD</b> method utilizing <b>sensitivity-guided channel pruning</b> to selectively suppress domain-sensitive channels, reducing computational overhead while maintaining adaptation performance.
        </div>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/">PDF</a>
            <a href="javascript:toggleblock('wang2025efficient-bib')">BibTeX</a>
        </div>
        <div id="wang2025efficient-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">

</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/deng2024vmad.png" alt="VMAD">
    </div>
    <div class="paper-content">
        <a href="https://arxiv.org/pdf/2409.20146" class="paper-title">
            VMAD: Visual-enhanced Multimodal Large Language Model for Zero-Shot Anomaly Detection
        </a>
        <div class="paper-authors">
            Huilin Deng, Hongchen Luo, <span class="me">Wei Zhai</span>, Yang Cao, Yu Kang.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">IEEE T-ASE</span>
        </div>
        <div class="paper-desc">
            We present <b>VMAD</b>, a <b>Visual-enhanced MLLM</b> for <b>zero-shot anomaly detection</b> that incorporates defect-sensitive structure learning and locality-enhanced token compression, benchmarked on the <b>RIAD dataset</b>.
        </div>
        <div class="paper-links">
            <a href="https://arxiv.org/pdf/2409.20146">PDF</a>
            <a href="javascript:toggleblock('deng2024vmad-bib')">BibTeX</a>
        </div>
        <div id="deng2024vmad-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{deng2024vmad,
    title={VMAD: Visual-enhanced Multimodal Large Language Model for Zero-Shot Anomaly Detection},
    author={Deng, Huilin and Luo, Hongchen and Zhai, Wei and Cao, Yang and Kang, Yu},
    journal={arXiv preprint arXiv:2409.20146},
    year={2024}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/lu2023likelihood.png" alt="LSA">
    </div>
    <div class="paper-content">
        <a href="https://arxiv.org/abs/2312.01732" class="paper-title">
            Likelihood-Aware Semantic Alignment for Full-Spectrum Out-of-Distribution Detection
        </a>
        <div class="paper-authors">
            Fan Lu, Kai Zhu, Kecheng Zheng, <span class="me">Wei Zhai</span>, Yang Cao, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">Journal of Intelligent Computing and Networking</span>
        </div>
        <div class="paper-desc">
            We propose a <b>Likelihood-Aware Semantic Alignment (LSA)</b> framework for <b>full-spectrum OOD detection</b>, utilizing Gaussian sampling and <b>bidirectional prompt customization</b> to align image-text correspondence.
        </div>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2312.01732">PDF</a>
            <a href="https://github.com/LuFan31/LSA">Code</a>
            <a href="javascript:toggleblock('lu2023likelihood-bib')">BibTeX</a>
        </div>
        <div id="lu2023likelihood-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{lu2023likelihood,
    title={Likelihood-Aware Semantic Alignment for Full-Spectrum Out-of-Distribution Detection},
    author={Lu, Fan and Zhu, Kai and Zheng, Kecheng and Zhai, Wei and Cao, Yang},
    journal={arXiv preprint arXiv:2312.01732},
    year={2023}
    }
</pre>
        </div>
    </div>
</div>



<div class="year-header">2024</div>



<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/yang2024egochoir.gif" alt="EgoChoir">
    </div>
    <div class="paper-content">
        <a href="https://openreview.net/pdf?id=ea4oxkiMP7" class="paper-title">
            EgoChoir: Capturing 3D Human-Object Interaction Regions from Egocentric Views
        </a>
        <div class="paper-authors">
            Yuhang Yang, <span class="me">Wei Zhai*</span>, Chengfeng Wang, Chengjun Yu, Yang Cao, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">NeurIPS 2024</span>
        </div>
        <div class="paper-desc">
            We propose <b>EgoChoir</b> to capture <b>3D interaction regions</b> from egocentric views by harmonizing visual appearance, head motion, and 3D objects to jointly infer <b>human contact</b> and <b>object affordance</b>.
        </div>
        <div class="paper-links">
            <a href="https://openreview.net/pdf?id=ea4oxkiMP7">PDF</a>
            <a href="https://yyvhang.github.io/EgoChoir/">Code</a>
            <a href="javascript:toggleblock('yang2024egochoir-bib')">BibTeX</a>
        </div>
        <div id="yang2024egochoir-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{yang2024egochoir,
    title={EgoChoir: Capturing 3D Human-Object Interaction Regions from Egocentric Views},
    author={Yang, Yuhang and Zhai, Wei and Wang, Chengfeng and Yu, Chengjun and Cao, Yang and Zha, Zheng-Jun},
    journal={arXiv preprint arXiv:2405.13659},
    year={2024}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/dong2024unidense.png" alt="UniDense">
    </div>
    <div class="paper-content">
        <a href="https://dl.acm.org/doi/pdf/10.1145/3664647.3680831" class="paper-title">
            UniDense: Unleashing Diffusion Models with Meta-Routers for Universal Few-Shot Dense Prediction
        </a>
        <div class="paper-authors">
            Lintao Dong, <span class="me">Wei Zhai*</span>, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">ACM MM 2024</span>
        </div>
        <div class="paper-desc">
            We introduce <b>UniDense</b>, a framework utilizing <b>Meta-Routers</b> to select task-relevant computation pathways within a frozen <b>Stable Diffusion</b> model for efficient <b>universal few-shot dense prediction</b>.
        </div>
        <div class="paper-links">
            <a href="https://dl.acm.org/doi/pdf/10.1145/3664647.3680831">PDF</a>
            <a href="javascript:toggleblock('dong2024unidense-bib')">BibTeX</a>
        </div>
        <div id="dong2024unidense-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">

</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/wan2023event.png" alt="MV-Net">
    </div>
    <div class="paper-content">
        <a href="https://ieeexplore.ieee.org/abstract/document/10612786/" class="paper-title">
            Event-based Optical Flow via Transforming into Motion-dependent View
        </a>
        <div class="paper-authors">
            Zengyu Wan, Yang Wang, <span class="me">Wei Zhai*</span>, Ganchao Tan, Yang Cao, Zheng-Jun Zha*.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">IEEE T-IP</span>
        </div>
        <div class="paper-desc">
            We propose <b>MV-Net</b>, which transforms the orthogonal view into a <b>motion-dependent view</b> using an <b>Event View Transformation Module</b> to enhance event-based motion representation for <b>optical flow estimation</b>.
        </div>
        <div class="paper-links">
            <a href="https://ieeexplore.ieee.org/abstract/document/10612786/">PDF</a>
            <a href="javascript:toggleblock('wan2023event-bib')">BibTeX</a>
        </div>
        <div id="wan2023event-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{wan2024event,
    title={Event-based Optical Flow via Transforming into Motion-dependent View},
    author={Wan, Zengyu and Wang, Yang and Wei, Zhai and Tan, Ganchao and Cao, Yang and Zha, Zheng-Jun},
    journal={IEEE Transactions on Image Processing},
    year={2024},
    publisher={IEEE}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/zhang2023bidirectional.png" alt="BOT">
    </div>
    <div class="paper-content">
        <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07631.pdf" class="paper-title">
            Bidirectional Progressive Transformer for Interaction Intention Anticipation
        </a>
        <div class="paper-authors">
            Zichen Zhang, Hongchen Luo, <span class="me">Wei Zhai*</span>, Yang Cao, Yu Kang.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">ECCV 2024</span>
        </div>
        <div class="paper-desc">
            We present <b>BOT</b>, a <b>Bidirectional Progressive Transformer</b> that mutually corrects <b>hand trajectories</b> and <b>interaction hotspots</b> predictions to minimize error accumulation in interaction intention anticipation.
        </div>
        <div class="paper-links">
            <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07631.pdf">PDF</a>
            <a href="https://arxiv.org/pdf/2405.05552">Arxiv</a>
            <a href="javascript:toggleblock('zhang2023bidirectional-bib')">BibTeX</a>
        </div>
        <div id="zhang2023bidirectional-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{zhang2024bidirectional,
    title={Bidirectional Progressive Transformer for Interaction Intention Anticipation},
    author={Zhang, Zichen and Luo, Hongchen and Zhai, Wei and Cao, Yang and Kang, Yu},
    journal={arXiv preprint arXiv:2405.05552},
    year={2024}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/wu2024event.png" alt="AsynHDR">
    </div>
    <div class="paper-content">
        <a href="https://opg.optica.org/oe/fulltext.cfm?uri=oe-32-11-18527&id=549921" class="paper-title">
            Event-based Asynchronous HDR Imaging by Temporal Incident Light Modulation
        </a>
        <div class="paper-authors">
            Yuliang Wu, Ganchao Tan, Jinze Chen, <span class="me">Wei Zhai*</span>, Yang Cao, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">Optics Express</span>
        </div>
        <div class="paper-desc">
            We propose <b>AsynHDR</b>, a system integrating <b>DVS</b> with LCD panels for <b>temporal incident light modulation</b>, enabling pixel-asynchronous <b>High Dynamic Range (HDR) imaging</b>.
        </div>
        <div class="paper-links">
            <a href="https://opg.optica.org/oe/fulltext.cfm?uri=oe-32-11-18527&id=549921">PDF</a>
            <a href="javascript:toggleblock('wu2024event-bib')">BibTeX</a>
        </div>
        <div id="wu2024event-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{wu2024event,
    title={Event-based asynchronous HDR imaging by temporal incident light modulation},
    author={Wu, Yuliang and Tan, Ganchao and Chen, Jinze and Zhai, Wei and Cao, Yang and Zha, Zheng-Jun},
    journal={Optics Express},
    volume={32},
    number={11},
    pages={18527--18538},
    year={2024},
    publisher={Optica Publishing Group}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/deng2023priorited.png" alt="PLMNet">
    </div>
    <div class="paper-content">
        <a href="https://ieeexplore.ieee.org/abstract/document/10494062" class="paper-title">
            Prioritized Local Matching Network for Cross-Category Few-Shot Anomaly Detection
        </a>
        <div class="paper-authors">
            Huilin Deng, Hongchen Luo, <span class="me">Wei Zhai</span>, Yang Cao, Yanming Guo, Yu Kang.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">IEEE T-AI</span>
        </div>
        <div class="paper-desc">
            We propose <b>PLMNet</b> for <b>Cross-Category Few-shot Anomaly Detection</b>, utilizing a <b>Local Perception Network</b> and <b>Defect-sensitive Weight Learner</b> to establish fine-grained correspondence between query and normal samples.
        </div>
        <div class="paper-links">
            <a href="https://ieeexplore.ieee.org/abstract/document/10494062">PDF</a>
            <a href="javascript:toggleblock('deng2023prioritized-bib')">BibTeX</a>
        </div>
        <div id="deng2023prioritized-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{deng2024prioritized,
    title={Prioritized Local Matching Network for Cross-Category Few-Shot Anomaly Detection},
    author={Deng, Huilin and Luo, Hongchen and Zhai, Wei and Cao, Yang and Kang, Yu},
    journal={IEEE Transactions on Artificial Intelligence},
    year={2024},
    publisher={IEEE}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/yang2023lemon.png" alt="LEMON">
    </div>
    <div class="paper-content">
        <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_LEMON_Learning_3D_Human-Object_Interaction_Relation_from_2D_Images_CVPR_2024_paper.pdf" class="paper-title">
            LEMON: Learning 3D Human-Object Interaction Relation from 2D Images
        </a>
        <div class="paper-authors">
            Yuhang Yang, <span class="me">Wei Zhai*</span>, Hongchen Luo, Yang Cao, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">CVPR 2024</span>
        </div>
        <div class="paper-desc">
            We present <b>LEMON</b>, a unified model that learns <b>3D human-object interaction relations</b> from 2D images by mining <b>interaction intentions</b> and geometric correlations to jointly anticipate interaction elements.
        </div>
        <div class="paper-links">
            <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_LEMON_Learning_3D_Human-Object_Interaction_Relation_from_2D_Images_CVPR_2024_paper.pdf">PDF</a>
            <a href="https://arxiv.org/abs/2312.08963">Arxiv</a>
            <a href="https://yyvhang.github.io/LEMON/">Website</a>
            <a href="javascript:toggleblock('yang2023lemon-bib')">BibTeX</a>
        </div>
        <div id="yang2023lemon-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@inproceedings{yang2024lemon,
    title={LEMON: Learning 3D Human-Object Interaction Relation from 2D Images},
    author={Yang, Yuhang and Zhai, Wei and Luo, Hongchen and Cao, Yang and Zha, Zheng-Jun},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={16284--16295},
    year={2024}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/wang2024mambapupil.png" alt="Mambapupil">
    </div>
    <div class="paper-content">
        <a href="https://openaccess.thecvf.com/content/CVPR2024W/AI4Streaming/papers/Wang_MambaPupil_Bidirectional_Selective_Recurrent_Model_for_Event-based_Eye_Tracking_CVPRW_2024_paper.pdf" class="paper-title">
            Mambapupil: Bidirectional Selective Recurrent Model for Event-based Eye Tracking
        </a>
        <div class="paper-authors">
            Zhong Wang, Zengyu Wan, Han Han, Bohao Liao, Yuliang Wu, <span class="me">Wei Zhai*</span>, Yang Cao, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">CVPR 2024 Workshop</span>
            <span class="highlight-tag">(1st Place Challenge)</span>
        </div>
        <div class="paper-desc">
            We propose <b>MambaPupil</b>, a <b>bidirectional selective recurrent model</b> for <b>event-based eye tracking</b>, utilizing a <b>Linear Time-Varying State Space Module</b> to handle diverse eye movement patterns.
        </div>
        <div class="paper-links">
            <a href="https://openaccess.thecvf.com/content/CVPR2024W/AI4Streaming/papers/Wang_MambaPupil_Bidirectional_Selective_Recurrent_Model_for_Event-based_Eye_Tracking_CVPRW_2024_paper.pdf">PDF</a>
            <a href="javascript:toggleblock('wang2024mambapupil-bib')">BibTeX</a>
        </div>
        <div id="wang2024mambapupil-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@inproceedings{wang2024mambapupil,
    title={Mambapupil: Bidirectional selective recurrent model for event-based eye tracking},
    author={Wang, Zhong and Wan, Zengyu and Han, Han and Liao, Bohao and Wu, Yuliang and Zhai, Wei and Cao, Yang and Zha, Zheng-jun},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={5762--5770},
    year={2024}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/liang2023hypercorrelation.png" alt="HCE">
    </div>
    <div class="paper-content">
        <a href="https://ojs.aaai.org/index.php/AAAI/article/download/28117/28238" class="paper-title">
            Hypercorrelation Evolution for Video Class-Incremental Learning
        </a>
        <div class="paper-authors">
            Sen Liang, Kai Zhu*, Zhiheng Liu, <span class="me">Wei Zhai*</span>, Yang Cao.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">AAAI 2024</span>
        </div>
        <div class="paper-desc">
            We propose a <b>hierarchical aggregation strategy</b> and <b>correlation refinement mechanism</b> for <b>Video Class-Incremental Learning</b>, optimizing hierarchical matching matrices to alleviate catastrophic forgetting.
        </div>
        <div class="paper-links">
            <a href="https://ojs.aaai.org/index.php/AAAI/article/download/28117/28238">PDF</a>
            <a href="javascript:toggleblock('liang2023hypercorrelation-bib')">BibTeX</a>
        </div>
        <div id="liang2023hypercorrelation-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@inproceedings{liang2024hypercorrelation,
    title={Hypercorrelation Evolution for Video Class-Incremental Learning},
    author={Liang, Sen and Zhu, Kai and Zhai, Wei and Liu, Zhiheng and Cao, Yang},
    booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
    volume={38},
    number={4},
    pages={3315--3323},
    year={2024}
    }
</pre>
        </div>
    </div>
</div>


<div class="year-header">2023</div>


<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/luo2022grounded.png" alt="Grounded Affordance">
    </div>
    <div class="paper-content">
        <a href="https://link.springer.com/article/10.1007/s11263-023-01962-z" class="paper-title">
            Grounded Affordance from Exocentric View
        </a>
        <div class="paper-authors">
            Hongchen Luo, <span class="me">Wei Zhai</span>, Jing Zhang, Yang Cao, Dacheng Tao.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">International Journal of Computer Vision (IJCV)</span>
            <br><span style="color:#666; font-size:12px;">Journal version of "Learning Affordance Grounding from Exocentric Images" (CVPR 2022)</span>
        </div>
        <div class="paper-desc">
            We propose a <b>cross-view affordance knowledge transfer framework</b> to ground affordance from exocentric views by transferring affordance-specific features to egocentric views, supported by the <b>AGD20K dataset</b>.
        </div>
        <div class="paper-links">
            <a href="https://link.springer.com/article/10.1007/s11263-023-01962-z">PDF</a>
            <a href="https://arxiv.org/abs/2208.13196">Arxiv</a>
            <a href="https://github.com/lhc1224/Cross-View-AG">Code</a>
            <a href="javascript:toggleblock('luo2022grounded-bib')">BibTeX</a>
        </div>
        <div id="luo2022grounded-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{luo2024grounded,
    title={Grounded affordance from exocentric view},
    author={Luo, Hongchen and Zhai, Wei and Zhang, Jing and Cao, Yang and Tao, Dacheng},
    journal={International Journal of Computer Vision},
    volume={132},
    number={6},
    pages={1945--1969},
    year={2024},
    publisher={Springer}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/zhai2021on.png" alt="MPAP">
    </div>
    <div class="paper-content">
        <a href="https://ieeexplore.ieee.org/abstract/document/10286884/" class="paper-title">
            On Exploring Multiplicity of Primitives and Attributes for Texture Recognition in the Wild
        </a>
        <div class="paper-authors">
            <span class="me">Wei Zhai</span>, Yang Cao, Jing Zhang, Haiyong Xie, Dacheng Tao, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">IEEE T-PAMI</span>
            <br><span style="color:#666; font-size:12px;">Journal version of MPAP (ICCV 2019) and DSR-Net (CVPR 2020)</span>
        </div>
        <div class="paper-desc">
            We propose <b>MPAP</b>, a novel network for <b>texture recognition</b> that models the relation of bottom-up structure and top-down attributes in a multi-branch unified framework to capture <b>multiple primitives and attributes</b>.
        </div>
        <div class="paper-links">
            <a href="https://ieeexplore.ieee.org/abstract/document/10286884/">PDF</a>
            <a href="https://github.com/tiaotiao11-22/MPAP">Code</a>
            <a href="javascript:toggleblock('zhai2021on-bib')">BibTeX</a>
        </div>
        <div id="zhai2021on-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{zhai2023exploring,
    title={On exploring multiplicity of primitives and attributes for texture recognition in the wild},
    author={Zhai, Wei and Cao, Yang and Zhang, Jing and Xie, Haiyong and Tao, Dacheng and Zha, Zheng-Jun},
    journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
    year={2023},
    publisher={IEEE}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/zhai2022background.png" alt="BAS">
    </div>
    <div class="paper-content">
        <a href="https://link.springer.com/article/10.1007/s11263-023-01919-2" class="paper-title">
            Background Activation Suppression for Weakly Supervised Object Localization and Semantic Segmentation
        </a>
        <div class="paper-authors">
            <span class="me">Wei Zhai</span>, Pingyu Wu, Kai Zhu, Yang Cao, Feng Wu, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">International Journal of Computer Vision (IJCV)</span>
            <br><span style="color:#666; font-size:12px;">Journal version of BAS (CVPR 2022)</span>
        </div>
        <div class="paper-desc">
            We introduce <b>Background Activation Suppression (BAS)</b> for <b>weakly supervised object localization</b>, using an <b>Activation Map Constraint</b> to facilitate generator learning by suppressing background activation.
        </div>
        <div class="paper-links">
            <a href="https://link.springer.com/article/10.1007/s11263-023-01919-2">PDF</a>
            <a href="https://arxiv.org/abs/2309.12943">Arxiv</a>
            <a href="https://github.com/wpy1999/BAS-Extension">Code</a>
            <a href="javascript:toggleblock('zhai2022background-bib')">BibTeX</a>
        </div>
        <div id="zhai2022background-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{zhai2024background,
    title={Background activation suppression for weakly supervised object localization and semantic segmentation},
    author={Zhai, Wei and Wu, Pingyu and Zhu, Kai and Cao, Yang and Wu, Feng and Zha, Zheng-Jun},
    journal={International Journal of Computer Vision},
    volume={132},
    number={3},
    pages={750--775},
    year={2024},
    publisher={Springer}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/luo2021learning.png" alt="HAG-Net">
    </div>
    <div class="paper-content">
        <a href="https://ieeexplore.ieee.org/abstract/document/10246333/" class="paper-title">
            Learning Visual Affordance Grounding from Demonstration Videos
        </a>
        <div class="paper-authors">
            Hongchen Luo, <span class="me">Wei Zhai</span>, Jing Zhang, Yang Cao, Dacheng Tao.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">IEEE T-NNLS</span>
        </div>
        <div class="paper-desc">
            We propose <b>HAG-Net</b>, a hand-aided network that leverages <b>demonstration videos</b> and a <b>dual-branch structure</b> to learn visual affordance grounding by transferring knowledge from video to object branches.
        </div>
        <div class="paper-links">
            <a href="https://ieeexplore.ieee.org/abstract/document/10246333/">PDF</a>
            <a href="https://arxiv.org/abs/2108.05675">Arxiv</a>
            <a href="https://github.com/lhc1224/HAG-Net">Code</a>
            <a href="javascript:toggleblock('luo2021learning-bib')">BibTeX</a>
        </div>
        <div id="luo2021learning-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{luo2023learning,
    title={Learning visual affordance grounding from demonstration videos},
    author={Luo, Hongchen and Zhai, Wei and Zhang, Jing and Cao, Yang and Tao, Dacheng},
    journal={IEEE Transactions on Neural Networks and Learning Systems},
    year={2023},
    publisher={IEEE}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/wu2023spatial.png" alt="SAT">
    </div>
    <div class="paper-content">
        <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Spatial-Aware_Token_for_Weakly_Supervised_Object_Localization_ICCV_2023_paper.pdf" class="paper-title">
            Spatial-Aware Token for Weakly Supervised Object Localization
        </a>
        <div class="paper-authors">
            Pingyu Wu, <span class="me">Wei Zhai*</span>, Yang Cao, Jiebo Luo and Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">ICCV 2023</span>
        </div>
        <div class="paper-desc">
            We propose a <b>Spatial-Aware Token (SAT)</b> for <b>weakly supervised object localization</b> to resolve optimization conflicts in transformers by learning a task-specific token to condition localization.
        </div>
        <div class="paper-links">
            <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Spatial-Aware_Token_for_Weakly_Supervised_Object_Localization_ICCV_2023_paper.pdf">PDF</a>
            <a href="https://arxiv.org/abs/2303.10438">Arxiv</a>
            <a href="https://github.com/wpy1999/SAT">Code</a>
            <a href="javascript:toggleblock('wu2023spatial-bib')">BibTeX</a>
        </div>
        <div id="wu2023spatial-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@inproceedings{wu2023spatial,
    title={Spatial-aware token for weakly supervised object localization},
    author={Wu, Pingyu and Zhai, Wei and Cao, Yang and Luo, Jiebo and Zha, Zheng-Jun},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
    pages={1844--1854},
    year={2023}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/yang2023grounding.png" alt="IAG">
    </div>
    <div class="paper-content">
        <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Grounding_3D_Object_Affordance_from_2D_Interactions_in_Images_ICCV_2023_paper.pdf" class="paper-title">
            Grounding 3D Object Affordance from 2D Interactions in Images
        </a>
        <div class="paper-authors">
            Yuhang Yang, <span class="me">Wei Zhai*</span>, Hongchen Luo, Yang Cao, Jiebo Luo and Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">ICCV 2023</span>
        </div>
        <div class="paper-desc">
            We introduce a novel task of <b>grounding 3D object affordance</b> from 2D interactions using an <b>Interaction-driven 3D Affordance Grounding Network (IAG)</b> and the new <b>PIAD dataset</b>.
        </div>
        <div class="paper-links">
            <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Grounding_3D_Object_Affordance_from_2D_Interactions_in_Images_ICCV_2023_paper.pdf">PDF</a>
            <a href="https://arxiv.org/abs/2303.10438">Arxiv</a>
            <a href="https://github.com/yyvhang/IAGNet">Code</a>
            <a href="javascript:toggleblock('yang2023grounding-bib')">BibTeX</a>
        </div>
        <div id="yang2023grounding-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@inproceedings{yang2023grounding,
    title={Grounding 3d object affordance from 2d interactions in images},
    author={Yang, Yuhang and Zhai, Wei and Luo, Hongchen and Cao, Yang and Luo, Jiebo and Zha, Zheng-Jun},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
    pages={10905--10915},
    year={2023}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/wang2023robustness.png" alt="Robustness Benchmark">
    </div>
    <div class="paper-content">
        <a href="https://just.ustc.edu.cn/en/article/doi/10.52396/JUSTC-2022-0165" class="paper-title">
            Robustness Benchmark for Unsupervised Anomaly Detection Models
        </a>
        <div class="paper-authors">
            Pei Wang, <span class="me">Wei Zhai</span>, and Yang Cao.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">Journal of University of Science and Technology of China (JUSTC)</span>
        </div>
        <div class="paper-desc">
            We propose <b>MVTec-C</b>, a dataset to evaluate the <b>robustness</b> of <b>unsupervised anomaly detection</b> models, and a <b>Feature Alignment Module (FAM)</b> to reduce feature drift caused by corruptions.
        </div>
        <div class="paper-links">
            <a href="https://just.ustc.edu.cn/en/article/doi/10.52396/JUSTC-2022-0165">PDF</a>
            <a href="javascript:toggleblock('wang2023robustness-bib')">BibTeX</a>
        </div>
        <div id="wang2023robustness-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{wang2024robustness,
    title={Robustness benchmark for unsupervised anomaly detection models},
    author={Wang, Pei and Zhai, Wei and Cao, Yang},
    journal={JUSTC},
    volume={54},
    number={1},
    pages={0103--1},
    year={2024},
    publisher={JUSTC}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/luo2023leverage.png" alt="Interactive Affinity">
    </div>
    <div class="paper-content">
        <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Leverage_Interactive_Affinity_for_Affordance_Learning_CVPR_2023_paper.pdf" class="paper-title">
            Leverage Interactive Affinity for Affordance Learning
        </a>
        <div class="paper-authors">
            Hongchen Luo#, <span class="me">Wei Zhai#</span>, Jing Zhang, Yang Cao, and Dacheng Tao.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">CVPR 2023</span>
        </div>
        <div class="paper-desc">
            We propose to leverage <b>interactive affinity</b> for <b>affordance learning</b>, using a <b>pose-aided framework</b> and <b>keypoint heuristic perception</b> to transfer cues from human-object interactions to non-interactive objects.
        </div>
        <div class="paper-links">
            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Leverage_Interactive_Affinity_for_Affordance_Learning_CVPR_2023_paper.pdf">PDF</a>
            <a href="https://github.com/lhc1224/PIAL-Net">Code</a>
            <a href="javascript:toggleblock('luo2023leverage-bib')">BibTeX</a>
        </div>
        <div id="luo2023leverage-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@inproceedings{luo2023leverage,
    title={Leverage interactive affinity for affordance learning},
    author={Luo, Hongchen and Zhai, Wei and Zhang, Jing and Cao, Yang and Tao, Dacheng},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={6809--6819},
    year={2023}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/lu2023uncertainty.png" alt="SCOOD">
    </div>
    <div class="paper-content">
        <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Uncertainty-Aware_Optimal_Transport_for_Semantically_Coherent_Out-of-Distribution_Detection_CVPR_2023_paper.pdf" class="paper-title">
            Uncertainty-Aware Optimal Transport for Semantically Coherent Out-of-Distribution Detection
        </a>
        <div class="paper-authors">
            Fan Lu, Kai Zhu, <span class="me">Wei Zhai</span>, Kecheng Zheng, and Yang Cao.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">CVPR 2023</span>
        </div>
        <div class="paper-desc">
            We propose an <b>uncertainty-aware optimal transport</b> scheme for <b>Semantically Coherent OOD detection</b>, utilizing an energy-based transport mechanism to discern outliers from intended data distributions.
        </div>
        <div class="paper-links">
            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Uncertainty-Aware_Optimal_Transport_for_Semantically_Coherent_Out-of-Distribution_Detection_CVPR_2023_paper.pdf">PDF</a>
            <a href="https://github.com/LuFan31/ET-OOD">Code</a>
            <a href="javascript:toggleblock('lu2023uncertainty-bib')">BibTeX</a>
        </div>
        <div id="lu2023uncertainty-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@inproceedings{lu2023uncertainty,
    title={Uncertainty-Aware Optimal Transport for Semantically Coherent Out-of-Distribution Detection},
    author={Lu, Fan and Zhu, Kai and Zhai, Wei and Zheng, Kecheng and Cao, Yang},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={3282--3291},
    year={2023}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/dong2023exploring.png" alt="Ventral Stream">
    </div>
    <div class="paper-content">
        <a href="https://ojs.aaai.org/index.php/AAAI/article/download/25128/24900" class="paper-title">
            Exploring Tuning Characteristics of Ventral Stream's Neurons for Few-Shot Image Classification
        </a>
        <div class="paper-authors">
            Lintao Dong, <span class="me">Wei Zhai</span>, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">AAAI 2023</span>
            <span class="highlight-tag">(Oral, Distinguished Paper)</span>
        </div>
        <div class="paper-desc">
            We explore the <b>tuning characteristics</b> of <b>ventral stream neurons</b> for <b>few-shot image classification</b>, proposing <b>hierarchical feature regularization</b> to produce generic and robust features.
        </div>
        <div class="paper-links">
            <a href="https://ojs.aaai.org/index.php/AAAI/article/download/25128/24900">PDF</a>
            <a href="javascript:toggleblock('dong2023exploring-bib')">BibTeX</a>
        </div>
        <div id="dong2023exploring-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@inproceedings{dong2023exploring,
    title={Exploring Tuning Characteristics of Ventral Stream’s Neurons for Few-Shot Image Classification},
    author={Dong, Lintao and Zhai, Wei and Zha, Zheng-Jun},
    booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
    volume={37},
    number={1},
    pages={534--542},
    year={2023}
    }
</pre>
        </div>
    </div>
</div>


<div class="year-header">2022</div>


<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/zhai2022exploring.png" alt="FGA">
    </div>
    <div class="paper-content">
        <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/6cc31b44d88dce8380d36e81485cd07f-Paper-Conference.pdf" class="paper-title">
            Exploring Figure-Ground Assignment Mechanism in Perceptual Organization
        </a>
        <div class="paper-authors">
            <span class="me">Wei Zhai</span>, Yang Cao, Jing Zhang, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">NeurIPS 2022</span>
        </div>
        <div class="paper-desc">
            We explore the <b>figure-ground assignment mechanism</b> to empower CNNs for robust <b>perceptual organization</b>, utilizing a <b>Figure-Ground-Aided (FGA)</b> module to handle visual ambiguity.
        </div>
        <div class="paper-links">
            <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/6cc31b44d88dce8380d36e81485cd07f-Paper-Conference.pdf">PDF</a>
            <a href="javascript:toggleblock('zhai2022exploring-bib')">BibTeX</a>
        </div>
        <div id="zhai2022exploring-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{zhai2022exploring,
    title={Exploring figure-ground assignment mechanism in perceptual organization},
    author={Zhai, Wei and Cao, Yang and Zhang, Jing and Zha, Zheng-Jun},
    journal={Advances in Neural Information Processing Systems},
    volume={35},
    pages={17030--17042},
    year={2022}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/lu2022phrase.png" alt="CBCE-Net">
    </div>
    <div class="paper-content">
        <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/6cc31b44d88dce8380d36e81485cd07f-Paper-Conference.pdf" class="paper-title">
            Phrase-Based Affordance Detection via Cyclic Bilateral Interaction
        </a>
        <div class="paper-authors">
            Liangsheng Lu#, <span class="me">Wei Zhai#</span>, Hongchen Luo, Kang Yu, Yang Cao.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">IEEE T-AI</span>
        </div>
        <div class="paper-desc">
            We propose <b>CBCE-Net</b> for <b>phrase-based affordance detection</b>, utilizing a cyclic bilateral interaction module to align vision and language features, extended with the annotated <b>PAD dataset</b>.
        </div>
        <div class="paper-links">
            <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/6cc31b44d88dce8380d36e81485cd07f-Paper-Conference.pdf">PDF</a>
            <a href="https://arxiv.org/abs/2202.12076">Arxiv</a>
            <a href="https://github.com/lulsheng/CBCE-Net">Code</a>
            <a href="javascript:toggleblock('lu2022phrase-bib')">BibTeX</a>
        </div>
        <div id="luo2022phrase-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{lu2022phrase,
    title={Phrase-based affordance detection via cyclic bilateral interaction},
    author={Lu, Liangsheng and Zhai, Wei and Luo, Hongchen and Kang, Yu and Cao, Yang},
    journal={IEEE Transactions on Artificial Intelligence},
    year={2022},
    publisher={IEEE}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/zhai2022one.png" alt="OSAD-Net">
    </div>
    <div class="paper-content">
        <a href="https://link.springer.com/article/10.1007/s11263-022-01642-4" class="paper-title">
            One-Shot Affordance Detection in the Wild
        </a>
        <div class="paper-authors">
            <span class="me">Wei Zhai#</span>, Hongchen Luo#, Jing Zhang, Yang Cao, Dacheng Tao.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">International Journal of Computer Vision (IJCV)</span>
            <br><span style="color:#666; font-size:12px;">Journal version of "One-Shot Affordance Detection" (IJCAI 2021)</span>
        </div>
        <div class="paper-desc">
            We propose <b>OSAD-Net</b> for <b>one-shot affordance detection</b> by transferring human action purpose to unseen scenarios, benchmarked on the large-scale <b>PADv2 dataset</b>.
        </div>
        <div class="paper-links">
            <a href="https://link.springer.com/article/10.1007/s11263-022-01642-4">PDF</a>
            <a href="https://arxiv.org/pdf/2108.03658.pdf">Arxiv</a>
            <a href="https://github.com/lhc1224/OSAD_Net">Code</a>
            <a href="javascript:toggleblock('zhai2022one-bib')">BibTeX</a>
        </div>
        <div id="zhai2022one-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{zhai2022one,
    title={One-shot object affordance detection in the wild},
    author={Zhai, Wei and Luo, Hongchen and Zhang, Jing and Cao, Yang and Tao, Dacheng},
    journal={International Journal of Computer Vision},
    volume={130},
    number={10},
    pages={2472--2500},
    year={2022},
    publisher={Springer}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/zhai2022deep.png" alt="DTC-Net">
    </div>
    <div class="paper-content">
        <a href="https://ieeexplore.ieee.org/abstract/document/9815160" class="paper-title">
            Deep Texton-Coherence Network for Camouflaged Object Detection
        </a>
        <div class="paper-authors">
            <span class="me">Wei Zhai</span>, Yang Cao, Haiyong Xie, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">IEEE T-MM</span>
        </div>
        <div class="paper-desc">
            We propose <b>DTC-Net</b> for <b>camouflaged object detection</b>, utilizing <b>Local Bilinear</b> modules and <b>Spatial Coherence Organization</b> to leverage spatial statistical properties of textons.
        </div>
        <div class="paper-links">
            <a href="https://ieeexplore.ieee.org/abstract/document/9815160">PDF</a>
            <a href="javascript:toggleblock('zhai2022deep-bib')">BibTeX</a>
        </div>
        <div id="zhai2022deep-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{zhai2022deep,
    title={Deep texton-coherence network for camouflaged object detection},
    author={Zhai, Wei and Cao, Yang and Xie, HaiYong and Zha, Zheng-Jun},
    journal={IEEE Transactions on Multimedia},
    year={2022},
    publisher={IEEE}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/li2022location.png" alt="LCG-Net">
    </div>
    <div class="paper-content">
        <a href="https://ieeexplore.ieee.org/abstract/document/9817812" class="paper-title">
            Location-Free Camouflage Generation Network
        </a>
        <div class="paper-authors">
            Yangyang Li#, <span class="me">Wei Zhai#</span>, Yang Cao, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">IEEE T-MM</span>
        </div>
        <div class="paper-desc">
            We present <b>LCG-Net</b>, a <b>location-free camouflage generation</b> network that uses <b>Position-aligned Structure Fusion (PSF)</b> to efficiently generate camouflage in multi-appearance regions.
        </div>
        <div class="paper-links">
            <a href="https://ieeexplore.ieee.org/abstract/document/9817812">PDF</a>
            <a href="https://arxiv.org/abs/2203.09845">Arxiv</a>
            <a href="https://github.com/Tale17/LCG-Net">Code</a>
            <a href="javascript:toggleblock('li2022location-bib')">BibTeX</a>
        </div>
        <div id="li2022location-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{li2022location,
    title={Location-free camouflage generation network},
    author={Li, Yangyang and Zhai, Wei and Cao, Yang and Zha, Zheng-jun},
    journal={IEEE Transactions on Multimedia},
    year={2022},
    publisher={IEEE}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/luo2022learning.png" alt="Exocentric Affordance">
    </div>
    <div class="paper-content">
        <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Luo_Learning_Affordance_Grounding_From_Exocentric_Images_CVPR_2022_paper.pdf" class="paper-title">
            Learning Affordance Grounding from Exocentric Images
        </a>
        <div class="paper-authors">
            Hongchen Luo#, <span class="me">Wei Zhai#</span>, Jing Zhang, Yang Cao, and Dacheng Tao.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">CVPR 2022</span>
        </div>
        <div class="paper-desc">
            We propose a <b>cross-view knowledge transfer framework</b> for <b>affordance grounding</b> that extracts features from exocentric interactions to perceive affordance in egocentric views, introducing the <b>AGD20K dataset</b>.
        </div>
        <div class="paper-links">
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Luo_Learning_Affordance_Grounding_From_Exocentric_Images_CVPR_2022_paper.pdf">PDF</a>
            <a href="https://arxiv.org/pdf/2203.09905.pdf">Arxiv</a>
            <a href="https://github.com/lhc1224/Cross-View-AG">Code</a>
            <a href="javascript:toggleblock('luo2022learning-bib')">BibTeX</a>
        </div>
        <div id="luo2022learning-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@inproceedings{luo2022learning,
    title={Learning affordance grounding from exocentric images},
    author={Luo, Hongchen and Zhai, Wei and Zhang, Jing and Cao, Yang and Tao, Dacheng},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={2252--2261},
    year={2022}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/wu2022background.png" alt="BAS">
    </div>
    <div class="paper-content">
        <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Background_Activation_Suppression_for_Weakly_Supervised_Object_Localization_CVPR_2022_paper.pdf" class="paper-title">
            Background Activation Suppression for Weakly Supervised Object Localization
        </a>
        <div class="paper-authors">
            Pingyu Wu#, <span class="me">Wei Zhai#</span>, Yang Cao.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">CVPR 2022</span>
        </div>
        <div class="paper-desc">
            We propose <b>Background Activation Suppression (BAS)</b> for <b>Weakly Supervised Object Localization (WSOL)</b>, which uses an <b>Activation Map Constraint (AMC)</b> to suppress background activation and learn whole object regions.
        </div>
        <div class="paper-links">
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Background_Activation_Suppression_for_Weakly_Supervised_Object_Localization_CVPR_2022_paper.pdf">PDF</a>
            <a href="https://arxiv.org/pdf/2112.00580.pdf">Arxiv</a>
            <a href="https://github.com/wpy1999/BAS">Code</a>
            <a href="javascript:toggleblock('wu2022background-bib')">BibTeX</a>
        </div>
        <div id="wu2022background-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@inproceedings{wu2022background,
    title={Background activation suppression for weakly supervised object localization},
    author={Wu, Pingyu and Zhai, Wei and Cao, Yang},
    booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    pages={14228--14237},
    year={2022},
    organization={IEEE}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/zhu2022self.png" alt="SSRE">
    </div>
    <div class="paper-content">
        <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_Self-Sustaining_Representation_Expansion_for_Non-Exemplar_Class-Incremental_Learning_CVPR_2022_paper.pdf" class="paper-title">
            Self-Sustaining Representation Expansion for Non-Exemplar Class-Incremental Learnings
        </a>
        <div class="paper-authors">
            Kai Zhu, <span class="me">Wei Zhai</span>, Yang Cao, Jiebo Luo, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">CVPR 2022</span>
        </div>
        <div class="paper-desc">
            We propose a <b>self-sustaining representation expansion</b> scheme for <b>non-exemplar class-incremental learning</b>, featuring structure reorganization and <b>main-branch distillation</b> to maintain old features.
        </div>
        <div class="paper-links">
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_Self-Sustaining_Representation_Expansion_for_Non-Exemplar_Class-Incremental_Learning_CVPR_2022_paper.pdf">PDF</a>
            <a href="https://arxiv.org/pdf/2203.06359.pdf">Arxiv</a>
            <a href="https://github.com/zhukaii/SSRE">Code</a>
            <a href="javascript:toggleblock('zhu2022self-bib')">BibTeX</a>
        </div>
        <div id="zhu2022self-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@inproceedings{zhu2022self,
    title={Self-sustaining representation expansion for non-exemplar class-incremental learning},
    author={Zhu, Kai and Zhai, Wei and Cao, Yang and Luo, Jiebo and Zha, Zheng-Jun},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={9296--9305},
    year={2022}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/wang2022robust.png" alt="DANSE">
    </div>
    <div class="paper-content">
        <a href="https://ieeexplore.ieee.org/abstract/document/9697984" class="paper-title">
            Robust Object Detection via Adversarial Novel Style Exploration
        </a>
        <div class="paper-authors">
            Wen Wang, Jing Zhang, <span class="me">Wei Zhai</span>, Yang Cao, Dacheng Tao.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">IEEE T-IP</span>
        </div>
        <div class="paper-desc">
            We propose <b>DANSE</b>, a method for <b>robust object detection</b> that uses <b>adversarial novel style exploration</b> to discover diverse degradation styles and adapt models to open and compound degradation types.
        </div>
        <div class="paper-links">
            <a href="https://ieeexplore.ieee.org/abstract/document/9697984">PDF</a>
            <a href="javascript:toggleblock('wang2022robust-bib')">BibTeX</a>
        </div>
        <div id="wang2022robust-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{wang2022robust,
    title={Robust object detection via adversarial novel style exploration},
    author={Wang, Wen and Zhang, Jing and Zhai, Wei and Cao, Yang and Tao, Dacheng},
    journal={IEEE Transactions on Image Processing},
    volume={31},
    pages={1949--1962},
    year={2022},
    publisher={IEEE}
    }
</pre>
        </div>
    </div>
</div>

<div class="year-header">2021</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/luo2021one.png" alt="OS-AD">
    </div>
    <div class="paper-content">
        <a href="https://www.ijcai.org/proceedings/2021/0124.pdf" class="paper-title">
            One-Shot Affordance Detection
        </a>
        <div class="paper-authors">
            Hongchen Luo, <span class="me">Wei Zhai</span>, Jing Zhang, Yang Cao, Dacheng Tao.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">IJCAI 2021</span>
            <span class="highlight-tag">(Oral)</span>
        </div>
        <div class="paper-desc">
            We propose a <b>One-Shot Affordance Detection (OS-AD)</b> network that estimates action purpose and transfers it to detect common affordances in unseen scenarios, utilizing <b>collaboration learning</b>. 
        </div>
        <div class="paper-links">
            <a href="https://www.ijcai.org/proceedings/2021/0124.pdf">PDF</a>
            <a href="https://arxiv.org/pdf/2106.14747.pdf">Arxiv</a>
            <a href="https://github.com/lhc1224/OSAD_Net">Code</a>
            <a href="javascript:toggleblock('luo2021one-bib')">BibTeX</a>
        </div>
        <div id="luo2021one-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{luo2021one,
    title={One-shot affordance detection},
    author={Luo, Hongchen and Zhai, Wei and Zhang, Jing and Cao, Yang and Tao, Dacheng},
    journal={arXiv preprint arXiv:2106.14747},
    year={2021}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/li2021tri.png" alt="TAM-GCN">
    </div>
    <div class="paper-content">
        <a href="https://ietresearch.onlinelibrary.wiley.com/doi/pdfdirect/10.1049/cvi2.12017" class="paper-title">
            A Tri-Attention Enhanced Graph Convolutional Network for Skeleton-Based Action Recognition
        </a>
        <div class="paper-authors">
            Xingming Li, <span class="me">Wei Zhai</span>, Yang Cao.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">IET Computer Vision (IET-CV 2021)</span>
        </div>
        <div class="paper-desc">
            We introduce a <b>Tri-Attention Module (TAM)</b> for <b>skeleton-based action recognition</b> to guide GCNs in perceiving significant variations across body poses, joint trajectories, and evolving projections. 
        </div>
        <div class="paper-links">
            <a href="https://ietresearch.onlinelibrary.wiley.com/doi/pdfdirect/10.1049/cvi2.12017">PDF</a>
            <a href="javascript:toggleblock('li2021tri-bib')">BibTeX</a>
        </div>
        <div id="li2021tri-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{li2021tri,
    title={A tri-attention enhanced graph convolutional network for skeleton-based action recognition},
    author={Li, Xingming and Zhai, Wei and Cao, Yang},
    journal={IET Computer Vision},
    volume={15},
    number={2},
    pages={110--121},
    year={2021},
    publisher={Wiley Online Library}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/zhu2021self.png" alt="SPPR">
    </div>
    <div class="paper-content">
        <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhu_Self-Promoted_Prototype_Refinement_for_Few-Shot_Class-Incremental_Learning_CVPR_2021_paper.pdf" class="paper-title">
            Self-Promoted Prototype Refinement for Few-Shot Class-Incremental Learning
        </a>
        <div class="paper-authors">
            Kai Zhu, Yang Cao, <span class="me">Wei Zhai</span>, Jie Cheng, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">CVPR 2021</span>
        </div>
        <div class="paper-desc">
            We propose a <b>Self-Promoted Prototype Refinement</b> mechanism for <b>few-shot class-incremental learning</b>, utilizing random episode selection and dynamic relation projection to strengthen new class expression. 
        </div>
        <div class="paper-links">
            <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhu_Self-Promoted_Prototype_Refinement_for_Few-Shot_Class-Incremental_Learning_CVPR_2021_paper.pdf">PDF</a>
            <a href="https://arxiv.org/abs/2107.08918">Arxiv</a>
            <a href="https://github.com/zhukaii/SPPR">Code</a>
            <a href="javascript:toggleblock('zhu2021self-bib')">BibTeX</a>
        </div>
        <div id="zhu2021self-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@inproceedings{zhu2021self,
    title={Self-promoted prototype refinement for few-shot class-incremental learning},
    author={Zhu, Kai and Cao, Yang and Zhai, Wei and Cheng, Jie and Zha, Zheng-Jun},
    booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
    pages={6801--6810},
    year={2021}
    }
</pre>
        </div>
    </div>
</div>

<div class="year-header">2020</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/zhu2020self.png" alt="Self-Supervised Tuning">
    </div>
    <div class="paper-content">
        <a href="https://www.ijcai.org/proceedings/2020/0142.pdf" class="paper-title">
            Self-Supervised Tuning for Few-Shot Segmentation
        </a>
        <div class="paper-authors">
            Kai Zhu, <span class="me">Wei Zhai</span>, Yang Cao.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">IJCAI 2020</span>
            <span class="highlight-tag">(Oral)</span>
        </div>
        <div class="paper-desc">
            We present an <b>adaptive tuning framework</b> for <b>few-shot segmentation</b> that uses a novel <b>self-supervised inner-loop</b> to dynamically adjust latent features and augment category-specific descriptors. 
        </div>
        <div class="paper-links">
            <a href="https://www.ijcai.org/proceedings/2020/0142.pdf">PDF</a>
            <a href="javascript:toggleblock('zhu2020self-bib')">BibTeX</a>
        </div>
        <div id="zhu2020self-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@inproceedings{zhu2021self,
    title={Self-supervised tuning for few-shot segmentation},
    author={Zhu, Kai and Zhai, Wei and Cao, Yang},
    booktitle={Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence},
    pages={1019--1025},
    year={2021}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/wang2020deep.png" alt="IR Method">
    </div>
    <div class="paper-content">
        <a href="https://ieeexplore.ieee.org/abstract/document/9190822" class="paper-title">
            Deep Inhomogeneous Regularization for Transfer Learning
        </a>
        <div class="paper-authors">
            Wen Wang, <span class="me">Wei Zhai</span>, Yang Cao.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">ICIP 2020</span>
        </div>
        <div class="paper-desc">
            We propose a novel <b>Inhomogeneous Regularization (IR)</b> method for <b>transfer learning</b> that imposes decaying averaged deviation penalties to tackle <b>catastrophic forgetting</b> and negative transfer.
        </div>
        <div class="paper-links">
            <a href="https://ieeexplore.ieee.org/abstract/document/9190822">PDF</a>
            <a href="javascript:toggleblock('wang2020deep-bib')">BibTeX</a>
        </div>
        <div id="wang2020deep-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@inproceedings{wang2020deep,
    title={Deep inhomogeneous regularization for transfer learning},
    author={Wang, Wen and Zhai, Wei and Cao, Yang},
    booktitle={2020 IEEE International Conference on Image Processing (ICIP)},
    pages={221--225},
    year={2020},
    organization={IEEE}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/zhai2020deep.png" alt="DSR-Net">
    </div>
    <div class="paper-content">
        <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhai_Deep_Structure-Revealed_Network_for_Texture_Recognition_CVPR_2020_paper.pdf" class="paper-title">
            Deep Structure-Revealed Network for Texture Recognition
        </a>
        <div class="paper-authors">
            <span class="me">Wei Zhai</span>, Yang Cao, Zheng-Jun Zha, HaiYong Xie, Feng Wu.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">CVPR 2020</span>
            <span class="highlight-tag">(Oral)</span>
        </div>
        <div class="paper-desc">
            We propose <b>DSR-Net</b> for <b>texture recognition</b>, leveraging a <b>primitive capturing module</b> and <b>dependence learning module</b> to reveal spatial dependency and structural representations. 
        </div>
        <div class="paper-links">
            <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhai_Deep_Structure-Revealed_Network_for_Texture_Recognition_CVPR_2020_paper.pdf">PDF</a>
            <a href="javascript:toggleblock('zhai2020deep-bib')">BibTeX</a>
        </div>
        <div id="zhai2020deep-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@inproceedings{zhai2020deep,
    title={Deep structure-revealed network for texture recognition},
    author={Zhai, Wei and Cao, Yang and Zha, Zheng-Jun and Xie, HaiYong and Wu, Feng},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={11010--11019},
    year={2020}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/zhu2020one.png" alt="OS-TR">
    </div>
    <div class="paper-content">
        <a href="https://ieeexplore.ieee.org/abstract/document/9224176" class="paper-title">
            One-Shot Texture Retrieval Using Global Grouping Metric
        </a>
        <div class="paper-authors">
            Kai Zhu, Yang Cao, <span class="me">Wei Zhai</span>, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">IEEE T-MM 2020</span>
            <br><span style="color:#666; font-size:12px;">Journal version of "One-Shot Texture Retrieval with Global Context Metric" (IJCAI 2019)</span>
        </div>
        <div class="paper-desc">
            We propose an <b>OS-TR network</b> for <b>one-shot texture retrieval</b> that utilizes an <b>adaptive directionality-aware module</b> and a <b>grouping-attention mechanism</b> for robust generalization.
        </div>
        <div class="paper-links">
            <a href="https://ieeexplore.ieee.org/abstract/document/9224176">PDF</a>
            <a href="javascript:toggleblock('zhu2020one-bib')">BibTeX</a>
        </div>
        <div id="zhu2020one-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{zhu2020one,
    title={One-shot texture retrieval using global grouping metric},
    author={Zhu, Kai and Cao, Yang and Zhai, Wei and Zha, Zheng-Jun},
    journal={IEEE Transactions on Multimedia},
    volume={23},
    pages={3726--3737},
    year={2020},
    publisher={IEEE}
    }
</pre>
        </div>
    </div>
</div>

<div class="year-header">2019</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/zhai2019deep.png" alt="MAP-Net">
    </div>
    <div class="paper-content">
        <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhai_Deep_Multiple-Attribute-Perceived_Network_for_Real-World_Texture_Recognition_ICCV_2019_paper.pdf" class="paper-title">
            Deep Multiple-Attribute-Perceived Network for Real-World Texture Recognition
        </a>
        <div class="paper-authors">
            <span class="me">Wei Zhai</span>, Yang Cao, Jing Zhang, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">ICCV 2019</span>
        </div>
        <div class="paper-desc">
            We propose <b>MAP-Net</b> for texture recognition, which progressively learns <b>visual texture attributes</b> in a multi-branch architecture using <b>deformable pooling</b> and attribute transfer schemes.
        </div>
        <div class="paper-links">
            <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhai_Deep_Multiple-Attribute-Perceived_Network_for_Real-World_Texture_Recognition_ICCV_2019_paper.pdf">PDF</a>
            <a href="javascript:toggleblock('zhai2019deep-bib')">BibTeX</a>
        </div>
        <div id="zhai2019deep-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@inproceedings{zhai2019deep,
    title={Deep multiple-attribute-perceived network for real-world texture recognition},
    author={Zhai, Wei and Cao, Yang and Zhang, Jing and Zha, Zheng-Jun},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
    pages={3613--3622},
    year={2019}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/zhu2019one.png" alt="OS-TR">
    </div>
    <div class="paper-content">
        <a href="https://arxiv.org/pdf/1905.06656.pdf" class="paper-title">
            One-Shot Texture Retrieval with Global Context Metric
        </a>
        <div class="paper-authors">
            Kai Zhu, <span class="me">Wei Zhai</span>, Zheng-Jun Zha, Yang Cao.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">IJCAI 2019</span>
            <span class="highlight-tag">(Oral)</span>
        </div>
        <div class="paper-desc">
            We tackle <b>one-shot texture retrieval</b> with an OS-TR network that includes a <b>directionality-aware module</b> and a <b>self-gating mechanism</b> to exploit global context information.
        </div>
        <div class="paper-links">
            <a href="https://arxiv.org/pdf/1905.06656.pdf">PDF</a>
            <a href="javascript:toggleblock('zhu2019one-bib')">BibTeX</a>
        </div>
        <div id="zhu2019one-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{zhu2019one,
    title={One-shot texture retrieval with global context metric},
    author={Zhu, Kai and Zhai, Wei and Zha, Zheng-Jun and Cao, Yang},
    journal={arXiv preprint arXiv:1905.06656},
    year={2019}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/wu2019pixtextgan.png" alt="PixTextGAN">
    </div>
    <div class="paper-content">
        <a href="https://ietresearch.onlinelibrary.wiley.com/doi/pdfdirect/10.1049/iet-ipr.2018.6588" class="paper-title">
            PixTextGAN: Structure Aware Text Image Synthesis for License Plate Recognition
        </a>
        <div class="paper-authors">
            Shilian Wu, <span class="me">Wei Zhai</span>, Yang Cao.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">IET Image Processing (IET-IP 2019)</span>
        </div>
        <div class="paper-desc">
            We propose <b>PixTextGAN</b>, a controllable architecture for synthesizing <b>license plate images</b> with structure-aware loss, refraining from collecting vast amounts of labelled data. 
        </div>
        <div class="paper-links">
            <a href="https://ietresearch.onlinelibrary.wiley.com/doi/pdfdirect/10.1049/iet-ipr.2018.6588">PDF</a>
            <a href="javascript:toggleblock('wu2019pixtextgan-bib')">BibTeX</a>
        </div>
        <div id="wu2019pixtextgan-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{wu2019pixtextgan,
    title={PixTextGAN: structure aware text image synthesis for license plate recognition},
    author={Wu, Shilian and Zhai, Wei and Cao, Yang},
    journal={IET Image Processing},
    volume={13},
    number={14},
    pages={2744--2752},
    year={2019},
    publisher={Wiley Online Library}
    }
</pre>
        </div>
    </div>
</div>

<div class="year-header">2018</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/zhai2018generative.png" alt="Unsupervised Inspection">
    </div>
    <div class="paper-content">
        <a href="https://ieeexplore.ieee.org/abstract/document/8462364" class="paper-title">
            A Generative Adversarial Network Based Framework for Unsupervised Visual Surface Inspection
        </a>
        <div class="paper-authors">
            <span class="me">Wei Zhai</span>, Jiang Zhu, Yang Cao, Zengfu Wang.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">ICASSP 2018</span>
            <span class="highlight-tag">(Oral)</span>
        </div>
        <div class="paper-desc">
            We propose a <b>GAN-based framework</b> for <b>unsupervised visual surface inspection</b>, where the discriminator serves as a one-class classifier to detect <b>abnormal regions</b> using multi-scale fusion. 
        </div>
        <div class="paper-links">
            <a href="https://ieeexplore.ieee.org/abstract/document/8462364">PDF</a>
            <a href="javascript:toggleblock('zhai2018generative-bib')">BibTeX</a>
        </div>
        <div id="zhai2018generative-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@inproceedings{zhai2018generative,
    title={A generative adversarial network based framework for unsupervised visual surface inspection},
    author={Zhai, Wei and Zhu, Jiang and Cao, Yang and Wang, Zengfu},
    booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
    pages={1283--1287},
    year={2018},
    organization={IEEE}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/zhu2018co.png" alt="Depth Map SR">
    </div>
    <div class="paper-content">
        <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-73603-7_8.pdf" class="paper-title">
            Co-Occurrent Structural Edge Detection for Color-Guided Depth Map Super-Resolution
        </a>
        <div class="paper-authors">
            Jiang Zhu, <span class="me">Wei Zhai</span>, Yang Cao, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            In <span class="venue-name">MMM 2018</span>
            <span class="highlight-tag">(Oral)</span>
        </div>
        <div class="paper-desc">
            We propose a <b>CNN-based method</b> for <b>color-guided depth map super-resolution</b> that detects <b>co-occurrent structural edges</b> to effectively exploit structural correlations between depth and color images. 
        </div>
        <div class="paper-links">
            <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-73603-7_8.pdf">PDF</a>
            <a href="javascript:toggleblock('zhu2018co-bib')">BibTeX</a>
        </div>
        <div id="zhu2018co-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@inproceedings{zhu2018co,
    title={Co-occurrent structural edge detection for color-guided depth map super-resolution},
    author={Zhu, Jiang and Zhai, Wei and Cao, Yang and Zha, Zheng-Jun},
    booktitle={MultiMedia Modeling: 24th International Conference, MMM 2018, Bangkok, Thailand, February 5-7, 2018, Proceedings, Part I 24},
    pages={93--105},
    year={2018},
    organization={Springer}
    }
</pre>
        </div>
    </div>
</div>






<h2>Pre-prints </h2>





<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/han2025touch.png" alt="TOUCH">
    </div>
    <div class="paper-content">
        <a href="https://arxiv.org/abs/2510.14874" class="paper-title">
            TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactions
        </a>
        <div class="paper-authors">
            Guangyi Han, <span class="me">Wei Zhai</span>, Yuhang Yang, Yang Cao, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            Arxiv
        </div>
        <div class="paper-desc">
            We introduce <b>Free-Form HOI Generation</b> and <b>TOUCH</b>, a framework leveraging a multi-level diffusion model and <b>explicit contact modeling</b> to generate diverse, physically plausible hand-object interactions from text. 
        </div>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2510.14874">PDF</a>
            <a href="https://guangyid.github.io/hoi123touch/">Project</a>
            <a href="javascript:toggleblock('han2025touch-bib')">BibTeX</a>
        </div>
        <div id="han2025touch-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{
}
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/shao2025value.png" alt="VGPO">
    </div>
    <div class="paper-content">
        <a href="https://arxiv.org/abs/" class="paper-title">
            Value-Anchored Group Policy Optimization for Flow Models
        </a>
        <div class="paper-authors">
            Yawen Shao, Jie Xiao, Kai Zhu, Yu Liu, <span class="me">Wei Zhai</span>, Yuhang Yang, Yang Cao, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            Arxiv
        </div>
        <div class="paper-desc">
            We propose <b>Value-Anchored Group Policy Optimization (VGPO)</b> for flow matching-based image generation, redefining value estimation with <b>process-aware value estimates</b> to enable precise credit assignment and stable optimization. 
        </div>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/">PDF</a>
            <a href="https://xxxx.github.io/">Project</a>
            <a href="javascript:toggleblock('shao2025value-bib')">BibTeX</a>
        </div>
        <div id="shao2025value-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{
}
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/wu2025alitok.png" alt="AliTok">
    </div>
    <div class="paper-content">
        <a href="https://arxiv.org/abs/2506.05289" class="paper-title">
            AliTok: Towards Sequence Modeling Alignment between Tokenizer and Autoregressive Model
        </a>
        <div class="paper-authors">
            Pingyu Wu, Kai Zhu, Yu Liu, Longxiang Tang, Jian Yang, Yansong Peng, <span class="me">Wei Zhai</span>, Yang Cao, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            Arxiv
        </div>
        <div class="paper-desc">
            We introduce <b>AliTok</b>, an <b>Aligned Tokenizer</b> using a causal decoder to establish unidirectional dependencies, aligning token modeling with autoregressive models for superior <b>image generation</b> performance. 
        </div>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2506.05289">PDF</a>
            <a href="https://github.com/ali-vilab/alitok">Code</a>
            <a href="javascript:toggleblock('wu2025alitok-bib')">BibTeX</a>
        </div>
        <div id="wu2025alitok-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{wu2025alitok,
  title={AliTok: Towards Sequence Modeling Alignment between Tokenizer and Autoregressive Model},
  author={Wu, Pingyu and Zhu, Kai and Liu, Yu and Tang, Longxiang and Yang, Jian and Peng, Yansong and Zhai, Wei and Cao, Yang and Zha, Zheng-Jun},
  journal={arXiv preprint arXiv:2506.05289},
  year={2025}
}
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/yang2025videogen.png" alt="VideoGen-Eval">
    </div>
    <div class="paper-content">
        <a href="https://arxiv.org/abs/2503.23452" class="paper-title">
            VideoGen-Eval: Agent-based System for Video Generation Evaluation
        </a>
        <div class="paper-authors">
            Yuhang Yang, Shangkun Sun, Hongxiang Li, Ke Fan, Ailing Zeng, Feilin Han, <span class="me">Wei Zhai</span>, Wei Liu, Yang Cao, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            Arxiv
        </div>
        <div class="paper-desc">
            We propose <b>VideoGen-Eval</b>, an <b>agent-based dynamic evaluation system</b> for video generation that integrates content structuring and multimodal judgment, validated against human preferences. 
        </div>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2503.23452">PDF</a>
            <a href="https://github.com/AILab-CVC/VideoGen-Eval">Code</a>
            <a href="javascript:toggleblock('yang2025videogen-bib')">BibTeX</a>
        </div>
        <div id="yang2025videogen-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{yang2025videogen,
  title={VideoGen-Eval: Agent-based System for Video Generation Evaluation},
  author={Yang, Yuhang and Fan, Ke and Sun, Shangkun and Li, Hongxiang and Zeng, Ailing and Han, FeiLin and Zhai, Wei and Liu, Wei and Cao, Yang and Zha, Zheng-Jun},
  journal={arXiv preprint arXiv:2503.23452},
  year={2025}
}
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/fang2025vangogh.gif" alt="VanGogh">
    </div>
    <div class="paper-content">
        <a href="https://arxiv.org/abs/2501.09499" class="paper-title">
            VanGogh: A Unified Multimodal Diffusion-based Framework for Video Colorization
        </a>
        <div class="paper-authors">
            Zixun Fang, Zhiheng Liu, Kai Zhu, Yu Liu, Ka Leong Cheng, <span class="me">Wei Zhai</span>, Yang Cao, Zheng-Jun Zha
        </div>
        <div class="paper-venue">
            Arxiv
        </div>
        <div class="paper-desc">
            We introduce <b>VanGogh</b>, a <b>unified multimodal diffusion-based framework</b> for <b>video colorization</b> that employs a Dual Qformer and depth-guided generation to achieve superior temporal consistency and color fidelity. 
        </div>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2501.09499">PDF</a>
            <a href="https://becauseimbatman0.github.io/VanGogh">Code</a>
            <a href="javascript:toggleblock('fang2025vangogh-bib')">BibTeX</a>
        </div>
        <div id="fang2025vangogh-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@misc{fang2025vangoghunifiedmultimodaldiffusionbased,
    title={VanGogh: A Unified Multimodal Diffusion-based Framework for Video Colorization}, 
    author={Zixun Fang and Zhiheng Liu and Kai Zhu and Yu Liu and Ka Leong Cheng and Wei Zhai and Yang Cao and Zheng-Jun Zha},
    year={2025},
    eprint={2501.09499},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2501.09499}, 
}
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/chen2025event.png" alt="EDFilter">
    </div>
    <div class="paper-content">
        <a href="https://arxiv.org/abs/2504.07503" class="paper-title">
            Event Signal Filtering via Probability Flux Estimation
        </a>
        <div class="paper-authors">
            Jinze Chen, <span class="me">Wei Zhai</span>, Yang Cao, Bin Li, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            Arxiv
        </div>
        <div class="paper-desc">
            We introduce <b>EDFilter</b>, an event signal filtering framework that estimates <b>probability flux</b> from discrete events using <b>nonparametric kernel smoothing</b>, enhancing signal fidelity for downstream tasks. 
        </div>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2504.07503">PDF</a>
            <a href="javascript:toggleblock('chen2025event-bib')">BibTeX</a>
        </div>
        <div id="chen2025event-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{chen2025event,
title={Event Signal Filtering via Probability Flux Estimation},
author={Chen, Jinze and Zhai, Wei and Cao, Yang and Li, Bin and Zha, Zheng-Jun},
journal={arXiv preprint arXiv:2504.07503},
year={2025}
}
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/luo2024visual.png" alt="VCR-Net">
    </div>
    <div class="paper-content">
        <a href="https://arxiv.org/pdf/2410.11363" class="paper-title">
            Visual-Geometric Collaborative Guidance for Affordance Learning
        </a>
        <div class="paper-authors">
            Hongchen Luo, <span class="me">Wei Zhai</span>, Jiao Wang, Yang Cao, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            Arxiv
            <br><span style="color:#666; font-size:12px;">Journal version of "Leverage Interactive Affinity for Affordance Learning" (CVPR 2023)</span>
        </div>
        <div class="paper-desc">
            We propose a <b>visual-geometric collaborative guided affordance learning network</b> that leverages <b>interactive affinity</b> to transfer knowledge from human-object interactions to non-interactive objects. 
        </div>
        <div class="paper-links">
            <a href="https://arxiv.org/pdf/2410.11363">PDF</a>
            <a href="https://github.com/lhc1224/VCR-Net">Code</a>
            <a href="javascript:toggleblock('luo2024visual-bib')">BibTeX</a>
        </div>
        <div id="luo2024visual-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{luo2024visual,
    title={Visual-Geometric Collaborative Guidance for Affordance Learning},
    author={Luo, Hongchen and Zhai, Wei and Wang, Jiao and Cao, Yang and Zha, Zheng-Jun},
    journal={arXiv preprint arXiv:2410.11363},
    year={2024}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/liu2024grounding.png" alt="Ego-SAG">
    </div>
    <div class="paper-content">
        <a href="https://arxiv.org/pdf/2409.19650" class="paper-title">
            Grounding 3D Scene Affordance From Egocentric Interactions
        </a>
        <div class="paper-authors">
            Cuiyu Liu, <span class="me">Wei Zhai</span>, Yuhang Yang, Hongchen Luo, Sen Liang, Yang Cao, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            Arxiv
        </div>
        <div class="paper-desc">
            We introduce <b>Ego-SAG</b>, a framework for <b>grounding 3D scene affordance</b> from <b>egocentric interactions</b> using interaction intent guidance and a bidirectional query decoder mechanism. 
        </div>
        <div class="paper-links">
            <a href="https://arxiv.org/pdf/2409.19650">PDF</a>
            <a href="javascript:toggleblock('liu2024grounding-bib')">BibTeX</a>
        </div>
        <div id="liu2024grounding-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{liu2024grounding,
    title={Grounding 3D Scene Affordance From Egocentric Interactions},
    author={Liu, Cuiyu and Zhai, Wei and Yang, Yuhang and Luo, Hongchen and Liang, Sen and Cao, Yang and Zha, Zheng-Jun},
    journal={arXiv preprint arXiv:2409.19650},
    year={2024}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/fang2024vivid.gif" alt="ViViD">
    </div>
    <div class="paper-content">
        <a href="https://arxiv.org/abs/2405.11794" class="paper-title">
            ViViD: Video Virtual Try-on using Diffusion Models
        </a>
        <div class="paper-authors">
            Zixun Fang, <span class="me">Wei Zhai</span>, Aimin Su, Hongliang Song, Kai Zhu, Mao Wang, Yu Chen, Zhiheng Liu, Yang Cao, Zheng-Jun Zha.
        </div>
        <div class="paper-venue">
            Arxiv
        </div>
        <div class="paper-desc">
            We present <b>ViViD</b>, a framework using <b>diffusion models</b> for <b>video virtual try-on</b>, incorporating a Garment Encoder, Pose Encoder, and Temporal Modules to ensure spatial-temporal consistency. 
        </div>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2405.11794">PDF</a>
            <a href="https://github.com/alibaba-yuanjing-aigclab/ViViD">Code</a>
            <a href="javascript:toggleblock('fang2024vivid-bib')">BibTeX</a>
        </div>
        <div id="fang2024vivid-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{fang2024vivid,
    title={ViViD: Video Virtual Try-on using Diffusion Models},
    author={Fang, Zixun and Zhai, Wei and Su, Aimin and Song, Hongliang and Zhu, Kai and Wang, Mao and Chen, Yu and Liu, Zhiheng and Cao, Yang and Zha, Zheng-Jun},
    journal={arXiv preprint arXiv:2405.11794},
    year={2024}
    }
</pre>
        </div>
    </div>
</div>

<div class="paper-row">
    <div class="paper-img">
        <img src="./Wei Zhai_files/luo2024intention.png" alt="IDE">
    </div>
    <div class="paper-content">
        <a href="https://arxiv.org/abs/2403.09194" class="paper-title">
            Intention-driven Ego-to-Exo Video Generation
        </a>
        <div class="paper-authors">
            Hongchen Luo, Kai Zhu, <span class="me">Wei Zhai</span>, Yang Cao.
        </div>
        <div class="paper-venue">
            Arxiv
        </div>
        <div class="paper-desc">
            We propose <b>IDE</b>, an <b>Intention-Driven Ego-to-Exo video generation</b> framework that uses action intention and cross-view feature perception to generate consistent exocentric videos from egocentric inputs. 
        </div>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2403.09194">PDF</a>
            <a href="javascript:toggleblock('luo2024intention-bib')">BibTeX</a>
        </div>
        <div id="luo2024intention-bib" style="display:none; margin-top:10px;">
<pre style="background:#f8f9fa; padding:12px; border:1px solid #eee; border-radius:4px; font-size:12px; color:#555; font-family:Consolas, monospace;">
@article{luo2024intention,
    title={Intention-driven Ego-to-Exo Video Generation},
    author={Luo, Hongchen and Zhu, Kai and Zhai, Wei and Cao, Yang},
    journal={arXiv preprint arXiv:2403.09194},
    year={2024}
    }
</pre>
        </div>
    </div>
</div>



<h2>Professional Activities</h2>
<div class="info-section">
    <div class="service-container">
        
        <div class="service-col">
            <span class="service-header">Conference Reviewer</span>
            <ul class="service-list">
                <li>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</li>
                <li>IEEE International Conference on Computer Vision (ICCV)</li>
                <li>European Conference on Computer Vision (ECCV)</li>
                <li>Neural Information Processing Systems (NeurIPS)</li>
                <li>International Conference on Learning Representations (ICLR)</li>
                <li>International Conference on Machine Learning (ICML)</li>
                <li>AAAI Conference on Artificial Intelligence (AAAI)</li>
                <li>ACM Multimedia (ACM MM)</li>
                <li>International Joint Conferences on AI (IJCAI)</li>
            </ul>
        </div>

        <div class="service-col">
            <span class="service-header">Journal Reviewer</span>
            <ul class="service-list">
                <li>IEEE Trans. on Pattern Analysis and Machine Intelligence (T-PAMI)</li>
                <li>International Journal of Computer Vision (IJCV)</li>
                <li>IEEE Transactions on Image Processing (T-IP)</li>
                <li>IEEE Transactions on Neural Networks and Learning Systems (T-NNLS)</li>
                <li>IEEE Transactions on Multimedia (T-MM)</li>
                <li>IEEE Trans. on Circuits and Systems for Video Technology (T-CSVT)</li>
                <li>Pattern Recognition (PR)</li>
                <li>ACM Trans. on Multimedia Computing, Comm., and Appl. (ToMM)</li>
            </ul>
        </div>

    </div>
</div>

<h2>Awards and Honors</h2>
<div class="info-section">

    <div class="info-row">
        <div class="info-label">2025</div>
        <div class="info-content">
            <b>ACM MM MSMA Workshop Best Student Paper</b>
            <span class="award-badge badge-blue">Best Paper</span>
        </div>
    </div>

    <div class="info-row">
        <div class="info-label">2025</div>
        <div class="info-content">
            1st Place, Body Contact Estimation Challenge (RHOBIN2025 CVPR Workshop)
            <span class="award-badge badge-gold">Champion</span>
        </div>
    </div>

    <div class="info-row">
        <div class="info-label">2025</div>
        <div class="info-content">
            1st Place, Efficient Event-based Eye-Tracking Challenge (CVPR Workshop)
            <span class="award-badge badge-gold">Champion</span>
        </div>
    </div>

    <div class="info-row">
        <div class="info-label">2024</div>
        <div class="info-content">
            1st Place, Event-based Eye Tracking Task (AIS2024 CVPR Workshop)
            <span class="award-badge badge-gold">Champion</span>
        </div>
    </div>

    <div class="info-row">
        <div class="info-label">2024</div>
        <div class="info-content">
            2nd Place, 3D Contact Estimation Challenge (RHOBIN2024 CVPR Workshop)
            <span class="award-badge badge-red">Runner-up</span>
        </div>
    </div>

    <div class="info-row">
        <div class="info-label">2024</div>
        <div class="info-content">
            2nd Place, NTIRE 2024 Efficient Super-Resolution Challenge
            <span class="award-badge badge-red">Runner-up</span>
        </div>
    </div>

    <div class="info-row">
        <div class="info-label">2023</div>
        <div class="info-content">
            <b>AAAI Distinguished Paper Award</b>
            <span class="award-badge badge-blue">Distinguished</span>
        </div>
    </div>

    <div class="info-row">
        <div class="info-label">2021</div>
        <div class="info-content">
            Outstanding Internship at JD Explore Academy
        </div>
    </div>

    <div class="info-row">
        <div class="info-label">2019</div>
        <div class="info-content">
            National Scholarship (University of Science and Technology of China)
        </div>
    </div>

    <div class="info-row">
        <div class="info-label">2017</div>
        <div class="info-content">
            Outstanding Graduate of Southwest Jiaotong University
        </div>
    </div>

    <div class="info-row">
        <div class="info-label">2016</div>
        <div class="info-content">
            National Scholarship (Southwest Jiaotong University)
        </div>
    </div>

</div>


<h2>Teaching</h2>
<div class="info-section">

    <div class="info-row">
        <div class="info-label">Autumn 2024</div>
        <div class="info-content">
            Computer Vision, USTC
        </div>
    </div>

    <div class="info-row">
        <div class="info-label">Autumn 2025</div>
        <div class="info-content">
            Computer Vision, USTC
        </div>
    </div>

</div>


<h2>Teaching Assistants</h2>
<div class="info-section">

    <div class="info-row">
        <div class="info-label">Autumn 2020</div>
        <div class="info-content">
            Computer Vision, USTC
        </div>
    </div>

    <div class="info-row">
        <div class="info-label">Autumn 2019</div>
        <div class="info-content">
            Image Processing, USTC
        </div>
    </div>

</div>



<script type="text/javascript">
function toggleNewsHistory() {
    var hiddenDiv = document.getElementById("news-hidden");
    var btn = document.getElementById("news-btn-text");
    
    if (hiddenDiv.style.display === "none" || hiddenDiv.style.display === "") {
        hiddenDiv.style.display = "block"; // 展开
        btn.innerHTML = "Collaspe History &#9650;"; // 按钮变文字
    } else {
        hiddenDiv.style.display = "none"; // 收起
        btn.innerHTML = "View History News... &#9660;"; // 按钮变文字
    }
}
</script>

</body></html>
