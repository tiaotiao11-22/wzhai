<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/">

<link rel="icon" src="./Wei Zhai_files/ustc_icon.png">
<link href="./Wei Zhai_files/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="./Wei Zhai_files/main.css" type="text/css">
<link rel="stylesheet" href="./Wei Zhai_files/nomenu.css" type="text/css">
<!--- <title>Wei Zhai</title> --->
<title>Wei Zhai</title>
<!-- MathJax -->
<script src="./Wei Zhai_files/latest.js.下载" async="">
</script>
<script type="text/x-mathjax-config;executed=true">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
<script type="text/javascript" src="./Wei Zhai_files/hidebib.js.下载"></script>
<script type="text/javascript" async="" src="./Wei Zhai_files/MathJax.js.下载"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-chartest {display: block; visibility: hidden; position: absolute; top: 0; line-height: normal; font-size: 500%}
.mjx-chartest .mjx-char {display: inline}
.mjx-chartest .mjx-box {padding-top: 1000px}
.MJXc-processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MJXc-processed {display: none}
.mjx-test {font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow: hidden; height: 1px}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
#MathJax_CHTML_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.mjx-chtml .mjx-noError {line-height: 1.2; vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
.MJXc-TeX-unknown-R {font-family: STIXGeneral,'Cambria Math','Arial Unicode MS',serif; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: STIXGeneral,'Cambria Math','Arial Unicode MS',serif; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: STIXGeneral,'Cambria Math','Arial Unicode MS',serif; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: STIXGeneral,'Cambria Math','Arial Unicode MS',serif; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></head>
<body><div id="MathJax_Message" style="display: none;"></div>
<div id="main-container">
<div id="layout-content"> <!-- nomenu  -->
<div id="toptitle">
<h1>Wei Zhai (翟伟)</h1>
</div>
<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/1.jpg" alt="" width="200px"></div>
<div id="text-container">

<p> I'm currently an Associate Researcher at the University of Science and Technology of China (USTC). I obtained my PhD degree from USTC in 2022, where I was advised by Professor <a href="https://faculty.ustc.edu.cn/chazhengjun/zh_CN/index.htm">Zheng-Jun Zha</a> and Associate Professor <a href="https://staff.iaticetc.cn:1234/">Yang Cao</a>.<br>
</p>

<p>
<b>Research</b>: 
I work on computer vision, embodied intelligence and machine learning. I am currently focusing on three aspects: 1) Build efficient computational framework for embodied intelligence by drawing on brain mechanisms. 2) Develop egocentric perception, which involves understanding egocentric scenarios, analyzing present interactions, and anticipating future activity. 3) Endow embodied agents working in complex real-world scenes with generalizable 2D/3D vision and interaction skills.
</p>

<p>
<a href="mailto:wzhai056@ustc.edu.cn" target="“blank”">EMail</a> /
<a href="https://faculty.ustc.edu.cn/zhaiwei/zh_CN/index.htm" target="“blank”">School Homepage</a> / 
<a href="https://scholar.google.ca/citations?user=UI5_qZcAAAAJ&hl=en" target="“blank”">Scholar</a> / 
<a href="https://xxxx" target="“blank”">Lab</a>
</p>

</div>
</div>
<h2>News</h2>

<p>&#9658; (02/2025) <b><font color=#FF0000>Five </font></b><font color=#000000>papers are accepted by </font><b><font color=#FF0000>CVPR 2025 (<font color=#0000FF>One Highlight Paper</font>) ~</font></b></i></p>
<p>&#9658; (09/2024) <b><font color=#FF0000>One </font></b><font color=#000000>paper is accepted by </font><b><font color=#FF0000>NeurIPS 2024 ~</font></b></i></p>
<p>&#9658; (07/2024) <b><font color=#FF0000>One </font></b><font color=#000000>paper is accepted by </font><b><font color=#FF0000>ACM MM 2024 ~</font></b></i></p>
<p>&#9658; (07/2024) <b><font color=#FF0000>One </font></b><font color=#000000>paper is accepted by </font><b><font color=#FF0000>T-IP ~</font></b></i></p>
<p>&#9658; (07/2024) <b><font color=#FF0000>One </font></b><font color=#000000>paper is accepted by </font><b><font color=#FF0000>ECCV 2024 ~</font></b></i></p>
<p>&#9658; (06/2024) <font color=#000000>Our team wins the </font><b><font color=#0000FF>2nd Place </font></b><font color=#000000>of </font><b><font color=#FF0000>3D Contact Estimation Challenge (RHOBIN2024 CVPR) ~</font></b></i></p>
<p>&#9658; (04/2024) <b><font color=#FF0000>One </font></b><font color=#000000>paper is accepted by </font><b><font color=#FF0000>Optics Express ~</font></b></i></p>
<p>&#9658; (04/2024) <b><font color=#FF0000>One </font></b><font color=#000000>paper is accepted by </font><b><font color=#FF0000>T-AI ~</font></b></i></p>
<p>&#9658; (03/2024) <font color=#000000>Our team wins the </font><b><font color=#0000FF>2nd Place </font></b><font color=#000000>of </font><b><font color=#FF0000>Efficient Super-Resolution Challenge (NTIRE2024 CVPR) ~</font></b></i></p>
<p>&#9658; (03/2024) <font color=#000000>Our team wins the </font><b><font color=#0000FF>1st Place </font></b><font color=#000000>of </font><b><font color=#FF0000>Event-based Eye Tracking Task (AIS2024 CVPR) ~</font></b></i></p>
<p>&#9658; (02/2024) <b><font color=#FF0000>One </font></b><font color=#000000>papers are accepted by </font><b><font color=#FF0000>CVPR 2024 ~</font></b></i></p>
<p>&#9658; (12/2023) <b><font color=#FF0000>One </font></b><font color=#000000>paper is accepted by </font><b><font color=#FF0000>AAAI 2024 ~</font></b></i></p>
<p>&#9658; (11/2023) <b><font color=#FF0000>One </font></b><font color=#000000>paper is accepted by </font><b><font color=#FF0000>IJCV ~</font></b></i></p>
<p>&#9658; (10/2023) <b><font color=#FF0000>One </font></b><font color=#000000>paper is accepted by </font><b><font color=#FF0000>T-PAMI ~</font></b></i></p>
<p>&#9658; (09/2023) <b><font color=#FF0000>One </font></b><font color=#000000>paper is accepted by </font><b><font color=#FF0000>IJCV ~</font></b></i></p>
<p>&#9658; (07/2023) <b><font color=#FF0000>One </font></b><font color=#000000>papers are accepted by </font><b><font color=#FF0000>T-NNLS ~</font></b></i></p>
<p>&#9658; (07/2023) <b><font color=#FF0000>Two </font></b><font color=#000000>papers are accepted by </font><b><font color=#FF0000>ICCV 2023 ~</font></b></i></p>
<p>&#9658; (03/2023) <b><font color=#FF0000>Two </font></b><font color=#000000>papers are accepted by </font><b><font color=#FF0000>CVPR 2023 ~</font></b></i></p>
<p>&#9658; (01/2023) <b><font color=#FF0000>One </font></b><font color=#000000>paper is accepted by </font><b><font color=#FF0000>AAAI 2023 (<font color=#0000FF>Distinguished Paper</font>) ~</font></b></i></p>
<p>&#9658; (09/2022) <b><font color=#FF0000>One </font></b><font color=#000000>paper is accepted by </font><b><font color=#FF0000>NeurIPS 2022 ~</font></b></i></p>
<p>&#9658; (08/2022) <b><font color=#FF0000>One </font></b><font color=#000000>paper is accepted by </font><b><font color=#FF0000>T-AI ~</font></b></i></p>
<p>&#9658; (06/2022) <b><font color=#FF0000>One </font></b><font color=#000000>paper is accepted by </font><b><font color=#FF0000>IJCV ~</font></b></i></p>
<p>&#9658; (Before 06/2022) <b>......</b></i></p>


<h2>Experience</h2>

<p><b>University of <a href="http://www.ustc.edu.cn/">Science and Technology of China (USTC)</a></b></p>
<ul>
<li><p><i>Jul 2024 - Now</i> &emsp; Associate Researcher at <a href="https://auto.ustc.edu.cn/">Department of Automation</a></p>
</li>
</ul>
<p><b>University of <a href="http://www.ustc.edu.cn/">Science and Technology of China (USTC)</a></b></p>
<ul>
<li><p><i>Jul 2022 - Jun 2024</i> &emsp; Postdoctoral Researcher in <a href="https://auto.ustc.edu.cn/">Department of Automation</a> (working with Professor <a href="https://faculty.ustc.edu.cn/chazhengjun/zh_CN/index.htm">Zheng-Jun Zha</a> and Associate Professor <a href="https://staff.iaticetc.cn:1234/">Yang Cao</a>)</p>
</li>
</ul>
<p><b>University of <a href="http://www.ustc.edu.cn/">Science and Technology of China (USTC)</a></b></p>
<ul>
<li><p><i>Sep 2017 - Jun 2022</i> &emsp; Ph.D in <a href="http://cybersec.ustc.edu.cn//">School of Cyberspace Security</a> (working with Professor <a href="https://faculty.ustc.edu.cn/chazhengjun/zh_CN/index.htm">Zheng-Jun Zha</a> and Associate Professor <a href="https://staff.iaticetc.cn:1234/">Yang Cao</a>)</p>
</li>
</ul>
<p><b><a href="http://cybersec.ustc.edu.cn//">JD</a> Explore Academy</a></b></p>
<ul>
<li><p><i>Dec 2020 - Sep 2021</i> &emsp; Research Intern in JD Explore Academy</a> (working with Professor <a href="https://scholar.google.ca/citations?user=RwlJNLcAAAAJ&hl=en">Dacheng Tao</a> and <a href="https://scholar.google.ca/citations?user=9jH5v74AAAAJ&hl=en">Jing Zhang</a>)</p>
</li>
</ul>
<p><b><a href="https://www.swjtu.edu.cn/">Southwest Jiaotong</a> University</a></b></p>
<ul>
<li><p><i>Sep 2013 - Jun 2017</i> &emsp; B.S. in <a href="https://www.swjtu.edu.cn/">Computer Science</a></p>
</li>
</ul>


<h2>Publications</h2>

<p><b>2025</b></p>

<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/lu2024benchmarking.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://arxiv.org/abs/2412.08614" target="“blank”">Benchmarking Large Vision-Language Models via Directed Scene Graph for Comprehensive Image Captioning</a></b>
<br>Fan Lu, Wei Wu, Kecheng Zheng*, Shuailei Ma, Biao Gong, Jiawei Liu, <b>Wei Zhai*</b>, Yang Cao, Yujun Shen, Zheng-Jun Zha.
<br><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR 2025</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;lu2024benchmarking-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;lu2024benchmarking-bib&#39;)">bibtex</a> / <a href="https://github.com/LuFan31/CompreCap" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="lu2024benchmarking-abs" style="display: none;">
Generating detailed captions comprehending text-rich visual content in images has received growing attention for Large Vision-Language Models (LVLMs). However, few studies have developed benchmarks specifically tailored for detailed captions to measure their accuracy and comprehensiveness. In this paper, we introduce a detailed caption benchmark, termed as CompreCap, to evaluate the visual context from a directed scene graph view. Concretely, we first manually segment the image into semantically meaningful regions (i.e., semantic segmentation mask) according to common-object vocabulary, while also distinguishing attributes of objects within all those regions. Then directional relation labels of these objects are annotated to compose a directed scene graph that can well encode rich compositional information of the image. Based on our directed scene graph, we develop a pipeline to assess the generated detailed captions from LVLMs on multiple levels, including the object-level coverage, the accuracy of attribute descriptions, the score of key relationships, etc. Experimental results on the CompreCap dataset confirm that our evaluation method aligns closely with human evaluation scores across LVLMs.</i>
</p>
<p align="justify">
</p><pre id="lu2024benchmarking-bib" style="display: none;">
@article{lu2024benchmarking,
    title={Benchmarking Large Vision-Language Models via Directed Scene Graph for Comprehensive Image Captioning},
    author={Lu, Fan and Wu, Wei and Zheng, Kecheng and Ma, Shuailei and Gong, Biao and Liu, Jiawei and Zhai, Wei and Cao, Yang and Shen, Yujun and Zha, Zheng-Jun},
    journal={arXiv preprint arXiv:2412.08614},
    year={2024}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("lu2024benchmarking-bib");hideblock("lu2024benchmarking-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/shao2024great.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://arxiv.org/abs/2411.19626" target="“blank”">GREAT: Geometry-Intention Collaborative Inference for Open-Vocabulary 3D Object Affordance Grounding</a></b>
<br>Yawen Shao, <b>Wei Zhai*</b>, Yuhang Yang, Hongchen Luo, Yang Cao, Zheng-Jun Zha.
<br><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR 2025</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;shao2024great-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;shao2024great-bib&#39;)">bibtex</a> / <a href="https://yawen-shao.github.io/GREAT/" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="shao2024great-abs" style="display: none;">
Open-Vocabulary 3D object affordance grounding aims to anticipate ``action possibilities'' regions on 3D objects with arbitrary instructions, which is crucial for robots to generically perceive real scenarios and respond to operational changes. Existing methods focus on combining images or languages that depict interactions with 3D geometries to introduce external interaction priors. However, they are still vulnerable to a limited semantic space by failing to leverage implied invariant geometries and potential interaction intentions. Normally, humans address complex tasks through multi-step reasoning and respond to diverse situations by leveraging associative and analogical thinking. In light of this, we propose GREAT (GeometRy-intEntion collAboraTive inference) for Open-Vocabulary 3D Object Affordance Grounding, a novel framework that mines the object invariant geometry attributes and performs analogically reason in potential interaction scenarios to form affordance knowledge, fully combining the knowledge with both geometries and visual contents to ground 3D object affordance. Besides, we introduce the Point Image Affordance Dataset v2 (PIADv2), the largest 3D object affordance dataset at present to support the task. Extensive experiments demonstrate the effectiveness and superiority of GREAT. Code and dataset are available at project.</i>
</p>
<p align="justify">
</p><pre id="shao2024great-bib" style="display: none;">
@article{shao2024great,
title={GREAT: Geometry-Intention Collaborative Inference for Open-Vocabulary 3D Object Affordance Grounding},
author={Shao, Yawen and Zhai, Wei and Yang, Yuhang and Luo, Hongchen and Cao, Yang and Zha, Zheng-Jun},
journal={arXiv preprint arXiv:2411.19626},
year={2024}
}
</pre>
<p></p>
<script language="JavaScript">hideblock("shao2024great-bib");hideblock("shao2024great-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/wu2024improved.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://arxiv.org/abs/2411.06449" target="“blank”">Improved Video VAE for Latent Video Diffusion Model</a></b>
<br>Pingyu Wu, Kai Zhu*, Yu Liu, Liming Zhao, <b>Wei Zhai*</b>, Yang Cao, Zheng-Jun Zha.
<br><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR 2025</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;wu2024improved-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;wu2024improved-bib&#39;)">bibtex</a> / <a href="https://wpy1999.github.io/IV-VAE/" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="wu2024improved-abs" style="display: none;">
Variational Autoencoder (VAE) aims to compress pixel data into low-dimensional latent space, playing an important role in OpenAI's Sora and other latent video diffusion generation models. While most of existing video VAEs inflate a pretrained image VAE into the 3D causal structure for temporal-spatial compression, this paper presents two astonishing findings: (1) The initialization from a well-trained image VAE with the same latent dimensions suppresses the improvement of subsequent temporal compression capabilities. (2) The adoption of causal reasoning leads to unequal information interactions and unbalanced performance between frames. To alleviate these problems, we propose a keyframe-based temporal compression (KTC) architecture and a group causal convolution (GCConv) module to further improve video VAE (IV-VAE). Specifically, the KTC architecture divides the latent space into two branches, in which one half completely inherits the compression prior of keyframes from a lower-dimension image VAE while the other half involves temporal compression through 3D group causal convolution, reducing temporal-spatial conflicts and accelerating the convergence speed of video VAE. The GCConv in above 3D half uses standard convolution within each frame group to ensure inter-frame equivalence, and employs causal logical padding between groups to maintain flexibility in processing variable frame video. Extensive experiments on five benchmarks demonstrate the SOTA video reconstruction and generation capabilities of the proposed IV-VAE (https://wpy1999.github.io/IV-VAE/).</i>
</p>
<p align="justify">
</p><pre id="wu2024improved-bib" style="display: none;">
@article{wu2024improved,
title={Improved Video VAE for Latent Video Diffusion Model},
author={Wu, Pingyu and Zhu, Kai and Liu, Yu and Zhao, Liming and Zhai, Wei and Cao, Yang and Zha, Zheng-Jun},
journal={arXiv preprint arXiv:2411.06449},
year={2024}
}
</pre>
<p></p>
<script language="JavaScript">hideblock("wu2024improved-bib");hideblock("wu2024improved-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/yang2024mmar.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://arxiv.org/pdf/2410.10798?" target="“blank”">MMAR: Towards Lossless Multi-Modal Auto-Regressive Prababilistic Modeling</a></b>
<br>Jian Yang, Dacheng Yin, Yizhou Zhou, Fengyun Rao, <b>Wei Zhai</b>, Yang Cao, Zheng-Jun Zha.
<br><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR 2025</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;yang2024mmar-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;yang2024mmar-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="yang2024mmar-abs" style="display: none;">
Recent advancements in multi-modal large language models have propelled the development of joint probabilistic models capable of both image understanding and generation. However, we have identified that recent methods inevitably suffer from loss of image information during understanding task, due to either image discretization or diffusion denoising steps. To address this issue, we propose a novel Multi-Modal Auto-Regressive (MMAR) probabilistic modeling framework. Unlike discretization line of method, MMAR takes in continuous-valued image tokens to avoid information loss. Differing from diffusion-based approaches, we disentangle the diffusion process from auto-regressive backbone model by employing a light-weight diffusion head on top each auto-regressed image patch embedding. In this way, when the model transits from image generation to understanding through text generation, the backbone model’s hidden representation of the image is not limited to the last denoising step. To successfully train our method, we also propose a theoretically proven technique that addresses the numerical stability issue and a training strategy that balances the generation and understanding task goals. Through extensive evaluations on 18 image understanding benchmarks, MMAR demonstrates much more superior performance than other joint multi-modal models, matching the method that employs pretrained CLIP vision encoder, meanwhile being able to generate high quality images at the same time. We also showed that our method is scalable with larger data and model size.
</i>
</p>
<p align="justify">
</p><pre id="yang2024mmar-bib" style="display: none;">
@article{yang2024mmar,
    title={MMAR: Towards Lossless Multi-Modal Auto-Regressive Prababilistic Modeling},
    author={Yang, Jian and Yin, Dacheng and Zhou, Yizhou and Rao, Fengyun and Zhai, Wei and Cao, Yang and Zha, Zheng-Jun},
    journal={arXiv preprint arXiv:2410.10798},
    year={2024}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("yang2024mmar-bib");hideblock("yang2024mmar-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/wang2025efficient.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://arxiv.org/abs/" target="“blank”">Efficient Test-time Adaptive Object Detection via Sensitivity-Guided Pruning</a></b>
<br>Kunyu Wang, Xueyang Fu, Xin Lu, Chengjie Ge, Chengzhi Cao, <b>Wei Zhai</b>, Zheng-Jun Zha.
<br><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR 2025, <i><font color=#FF0000>Highlight</font></i></b>).</i>
<br>
<a href="javascript:toggleblock(&#39;wang2025efficient-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;wang2025efficient-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="wang2025efficient-abs" style="display: none;">
Continual test-time adaptive object detection (CTTA-OD) aims to online adapt a source pre-trained detector to ever-changing environments during inference under continuous domain shifts. Most existing CTTA-OD methods prioritize effectiveness while overlooking computational efficiency, which is crucial for resource-constrained scenarios. In this paper, we propose an efficient CTTA-OD method via pruning. Our motivation stems from the observation that not all learned source features are beneficial; certain domain-sensitive feature channels can adversely affect target domain performance. Inspired by this, we introduce a sensitivity-guided channel pruning strategy that quantifies each channel based on its sensitivity to domain discrepancies at both image and instance levels. We apply weighted sparsity regularization to selectively suppress and prune these sensitive channels, focusing adaptation efforts on invariant ones. Additionally, we introduce a stochastic channel reactivation mechanism to restore pruned channels, enabling recovery of potentially useful features and mitigating the risks of early pruning. Extensive experiments on three benchmarks show that our method achieves superior adaptation performance while reducing computational overhead by 12% in FLOPs compared to the recent SOTA method.</i>
</p>
<p align="justify">
</p><pre id="wang2025efficient-bib" style="display: none;">

</pre>
<p></p>
<script language="JavaScript">hideblock("wang2025efficient-bib");hideblock("wang2025efficient-abs");</script>
<p></p>
</div></div>


<p><b>2024</b></p>

<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/yang2024egochoir.gif" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://openreview.net/pdf?id=ea4oxkiMP7" target="“blank”">EgoChoir: Capturing 3D Human-Object Interaction Regions from Egocentric Views</a></b>
<br>Yuhang Yang, <b>Wei Zhai*</b>, Chengfeng Wang, Chengjun Yu, Yang Cao, Zheng-Jun Zha.
<br><i>Neural Information Processing Systems (<b>NeurIPS 2024</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;yang2024egochoir-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;yang2024egochoir-bib&#39;)">bibtex</a> / <a href="https://yyvhang.github.io/EgoChoir/" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="yang2024egochoir-abs" style="display: none;">
Understanding egocentric human-object interaction (HOI) is a fundamental aspect of human-centric perception, facilitating applications like AR/VR and embodied AI. For the egocentric HOI, in addition to perceiving semantics e.g., ''what'' interaction is occurring, capturing ''where'' the interaction specifically manifests in 3D space is also crucial, which links the perception and operation. Existing methods primarily leverage observations of HOI to capture interaction regions from an exocentric view. However, incomplete observations of interacting parties in the egocentric view introduce ambiguity between visual observations and interaction contents, impairing their efficacy. From the egocentric view, humans integrate the visual cortex, cerebellum, and brain to internalize their intentions and interaction concepts of objects, allowing for the pre-formulation of interactions and making behaviors even when interaction regions are out of sight. In light of this, we propose harmonizing the visual appearance, head motion, and 3D object to excavate the object interaction concept and subject intention, jointly inferring 3D human contact and object affordance from egocentric videos. To achieve this, we present EgoChoir, which links object structures with interaction contexts inherent in appearance and head motion to reveal object affordance, further utilizing it to model human contact. Additionally, a gradient modulation is employed to adopt appropriate clues for capturing interaction regions across various egocentric scenarios. Moreover, 3D contact and affordance are annotated for egocentric videos collected from Ego-Exo4D and GIMO to support the task. Extensive experiments on them demonstrate the effectiveness and superiority of EgoChoir. Code and data will be open.
</i>
</p>
<p align="justify">
</p><pre id="yang2024egochoir-bib" style="display: none;">
@article{yang2024egochoir,
    title={EgoChoir: Capturing 3D Human-Object Interaction Regions from Egocentric Views},
    author={Yang, Yuhang and Zhai, Wei and Wang, Chengfeng and Yu, Chengjun and Cao, Yang and Zha, Zheng-Jun},
    journal={arXiv preprint arXiv:2405.13659},
    year={2024}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("yang2024egochoir-bib");hideblock("yang2024egochoir-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/dong2024unidense.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://dl.acm.org/doi/pdf/10.1145/3664647.3680831" target="“blank”">UniDense: Unleashing Diffusion Models with Meta-Routers for Universal Few-Shot Dense Prediction</a></b>
<br>Lintao Dong, <b>Wei Zhai*</b>, Zheng-Jun Zha.
<br><i>ACM Multimedia (<b>ACM MM 2024</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;dong2024unidense-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;dong2024unidense-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="dong2024unidense-abs" style="display: none;">
Universal few-shot dense prediction requires a versatile model capable of learning any dense prediction task from limited labeled images, which necessitates the model to possess efficient adaptation abilities.
Prevailing few-shot learning methods rely on efficient fine-tuning of model weights for few-shot adaptation, which carries the risk of disrupting the pre-trained knowledge and lacks the capability to extract task-specific knowledge contained in the pre-trained model.
To overcome these limitations, our paper approaches universal few-shot dense prediction from a novel perspective. Unlike conventional fine-tuning techniques that use all model parameters and modify a specific set of weights for few-shot adaptation, our method focuses on selecting task-relevant computation pathways of the pre-trained model while keeping the model weights frozen. Building upon this idea, we introduce a novel framework UniDense for universal few-shot dense prediction. First, we construct a versatile MoE (Mixture of Experts) architecture for dense prediction based on the Stable Diffusion model. We then utilize episodes-based meta-learning to train a set of routers for this MoE model, called Meta-Routers, which act as hyper-networks responsible for selecting computation blocks relevant to each task. We demonstrate that fine-tuning these meta-routers enables efficient few-shot adaptation of the entire model. 
Moreover, for each few-shot task, we leverage support samples to extract a task embedding, which serves as a conditioning factor for meta-routers. This strategy allows meta-routers to dynamically adapt themselves for different few-shot task, leading to improved adaptation performance.
Experiments on a challenging variant of Taskonomy dataset with 10 dense prediction tasks demonstrate the superiority of our approach.
</i>
</p>
<p align="justify">
</p><pre id="dong2024unidense-bib" style="display: none;">

</pre>
<p></p>
<script language="JavaScript">hideblock("dong2024unidense-bib");hideblock("dong2024unidense-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/wan2023event.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://ieeexplore.ieee.org/abstract/document/10612786/" target="“blank”">Event-based Optical Flow via Transforming into Motion-dependent View</a></b>
<br>Zengyu Wan, Yang Wang, <b>Wei Zhai*</b>, Ganchao Tan, Yang Cao, Zheng-Jun Zha*.
<br><i>IEEE Transactions on Image Processing (<b>T-IP</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;wan2023event-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;wan2023event-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="wan2023event-abs" style="display: none;">
Event cameras respond to temporal dynamics, helping to resolve ambiguities in spatio-temporal changes for optical flow estimation. However, the unique spatio-temporal event distribution challenges the feature extraction, and the direct construction of motion representation through the orthogonal view is less than ideal due to the entanglement of appearance and motion. This paper proposes to transform the orthogonal view into a motion-dependent one for enhancing event-based motion representation and presents a Motion View-based Network (MV-Net) for practical optical flow estimation. Specifically, this motion-dependent view transformation is achieved through the Event View Transformation Module, which captures the relationship between the steepest temporal changes and motion direction, incorporating these temporal cues into the view transformation process for feature gathering. This module includes two phases: extracting the temporal evolution clues by central difference operation in the extraction phase and capturing the motion pattern by evolution-guided deformable convolution in the perception phase. Besides, the MV-Net constructs an eccentric downsampling process to avoid response weakening from the sparsity of events in the downsampling stage. The whole network is trained end-to-end in a self-supervised manner, and the evaluations conducted on four challenging datasets reveal the superior performance of the proposed model compared to state-of-the-art (SOTA) methods.
</i>
</p>
<p align="justify">
</p><pre id="wan2023event-bib" style="display: none;">
@article{wan2024event,
    title={Event-based Optical Flow via Transforming into Motion-dependent View},
    author={Wan, Zengyu and Wang, Yang and Wei, Zhai and Tan, Ganchao and Cao, Yang and Zha, Zheng-Jun},
    journal={IEEE Transactions on Image Processing},
    year={2024},
    publisher={IEEE}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("wan2023event-bib");hideblock("wan2023event-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/zhang2023bidirectional.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07631.pdf" target="“blank”">Bidirectional Progressive Transformer for Interaction Intention Anticipation</a></b>
<br>Zichen Zhang, Hongchen Luo, <b>Wei Zhai*</b>, Yang Cao, Yu Kang.
<br><i>European Conference on Computer Vision (<b>ECCV 2024</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;zhang2023bidirectional-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;zhang2023bidirectional-bib&#39;)">bibtex</a> / <a href="https://arxiv.org/pdf/2405.05552" target=&ldquo;blank&rdquo;><font color=#BB0000>arxiv</font></a></p>
<p>
</p><p align="justify">
<i id="zhang2023bidirectional-abs" style="display: none;">
Interaction intention anticipation aims to jointly predict future hand trajectories and interaction hotspots. Existing research often treated trajectory forecasting and interaction hotspots prediction as separate tasks or solely considered the impact of trajectories on interaction hotspots, which led to the accumulation of prediction errors over time. However, a deeper inherent connection exists between hand trajectories and interaction hotspots, which allows for continuous mutual correction between them. Building upon this relationship, a novel Bidirectional prOgressive Transformer (BOT), which introduces a Bidirectional Progressive mechanism into the anticipation of interaction intention is established. Initially, BOT maximizes the utilization of spatial information from the last observation frame through the Spatial-Temporal Reconstruction Module, mitigating conflicts arising from changes of view in first-person videos. Subsequently, based on two independent prediction branches, a Bidirectional Progressive Enhancement Module is introduced to mutually improve the prediction of hand trajectories and interaction hotspots over time to minimize error accumulation. Finally, acknowledging the intrinsic randomness in human natural behavior, we employ a Trajectory Stochastic Unit and a C-VAE to introduce appropriate uncertainty to trajectories and interaction hotspots, respectively. Our method achieves state-of-the-art results on three benchmark datasets Epic-Kitchens-100, EGO4D, and EGTEA Gaze+, demonstrating superior in complex scenarios.
</i>
</p>
<p align="justify">
</p><pre id="zhang2023bidirectional-bib" style="display: none;">
@article{zhang2024bidirectional,
    title={Bidirectional Progressive Transformer for Interaction Intention Anticipation},
    author={Zhang, Zichen and Luo, Hongchen and Zhai, Wei and Cao, Yang and Kang, Yu},
    journal={arXiv preprint arXiv:2405.05552},
    year={2024}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("zhang2023bidirectional-bib");hideblock("zhang2023bidirectional-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/wu2024event.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://opg.optica.org/oe/fulltext.cfm?uri=oe-32-11-18527&id=549921" target="“blank”">Event-based Asynchronous HDR Imaging by Temporal Incident Light Modulation</a></b>
<br>Yuliang Wu, Ganchao Tan, Jinze Chen, <b>Wei Zhai*</b>, Yang Cao, Zheng-Jun Zha.
<br><i>Optics Express (<b>OE</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;wu2024event-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;wu2024event-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="wu2024event-abs" style="display: none;">
Dynamic range (DR) is a pivotal characteristic of imaging systems. Current frame-based cameras struggle to achieve high dynamic range imaging due to the conflict between globally uniform exposure and spatially variant scene illumination. In this paper, we propose AsynHDR, a pixel-asynchronous HDR imaging system, based on key insights into the challenges in HDR imaging and the unique event-generating mechanism of dynamic vision sensors (DVS). Our proposed AsynHDR system integrates the DVS with a set of LCD panels. The LCD panels modulate the irradiance incident upon the DVS by altering their transparency, thereby triggering the pixel-independent event streams. The HDR image is subsequently decoded from the event streams through our temporal-weighted algorithm. Experiments under the standard test platform and several challenging scenes have verified the feasibility of the system in HDR imaging tasks.
</i>
</p>
<p align="justify">
</p><pre id="wu2024event-bib" style="display: none;">
@article{wu2024event,
    title={Event-based asynchronous HDR imaging by temporal incident light modulation},
    author={Wu, Yuliang and Tan, Ganchao and Chen, Jinze and Zhai, Wei and Cao, Yang and Zha, Zheng-Jun},
    journal={Optics Express},
    volume={32},
    number={11},
    pages={18527--18538},
    year={2024},
    publisher={Optica Publishing Group}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("wu2024event-bib");hideblock("wu2024event-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/deng2023priorited.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://ieeexplore.ieee.org/abstract/document/10494062" target="“blank”">Prioritized Local Matching Network for	Cross-Category Few-Shot Anomaly Detection</a></b>
<br>Huilin Deng, Hongchen Luo, <b>Wei Zhai</b>, Yang Cao, Yanming Guo, Yu Kang.
<br><i>IEEE Artificial Intelligence (<b>T-AI</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;deng2023prioritized-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;deng2023prioritized-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="deng2023prioritized-abs" style="display: none;">
In response to the rapid evolution of products in industrial inspection, this paper introduces the Cross-category Few-shot Anomaly Detection (C-FSAD) task, aimed at efficiently detecting anomalies in new object categories with minimal normal samples. However, the diversity of defects and significant visual distinctions among various objects hinder the identification of anomalous regions. To tackle this, we adopt a pairwise comparison between query and normal samples, establishing an intimate correlation through fine-grained correspondence. Specifically, we propose the Prioritized Local Matching Network (PLMNet), emphasizing local analysis of correlation, which includes three primary components: 1) Local Perception Network refines the initial matches through bidirectional local analysis; 2) Step Aggregation strategy employs multiple stages of local convolutional pooling to aggregate local insights; 3) Defect-sensitive Weight Learner adaptively enhances channels informative for defect structures, ensuring more discriminative representations of encoded context. Our PLMNet deepens the interpretation of correlations, from geometric cues to semantics, efficiently extracting discrepancies in feature space. Extensive experiments on two standard industrial anomaly detection benchmarks demonstrate our state-of-the-art performance in both detection and localization, with margins of 9.8% and 5.4% respectively.
</i>
</p>
<p align="justify">
</p><pre id="deng2023prioritized-bib" style="display: none;">
@article{deng2024prioritized,
    title={Prioritized Local Matching Network for Cross-Category Few-Shot Anomaly Detection},
    author={Deng, Huilin and Luo, Hongchen and Zhai, Wei and Cao, Yang and Kang, Yu},
    journal={IEEE Transactions on Artificial Intelligence},
    year={2024},
    publisher={IEEE}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("deng2023prioritized-bib");hideblock("deng2023prioritized-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/yang2023lemon.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_LEMON_Learning_3D_Human-Object_Interaction_Relation_from_2D_Images_CVPR_2024_paper.pdf" target="“blank”">LEMON: Learning 3D Human-Object Interaction Relation from 2D Images</a></b>
<br>Yuhang Yang, <b>Wei Zhai*</b>, Hongchen Luo, Yang Cao, Zheng-Jun Zha.
<br><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR 2024</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;yang2023lemon-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;yang2023lemon-bib&#39;)">bibtex</a> / <a href="https://arxiv.org/abs/2312.08963" target=&ldquo;blank&rdquo;><font color=#BB0000>arxiv</font></a> / <a href="https://yyvhang.github.io/LEMON/" target=&ldquo;blank&rdquo;><font color=#BB0000>website</font></a></p>
<p>
</p><p align="justify">
<i id="yang2023lemon-abs" style="display: none;">
Learning 3D human-object interaction relation is pivotal to embodied AI and interaction modeling. Most existing methods approach the goal by learning to predict isolated interaction elements eg human contact object affordance and human-object spatial relation primarily from the perspective of either the human or the object. Which underexploit certain correlations between the interaction counterparts (human and object) and struggle to address the uncertainty in interactions. Actually objects' functionalities potentially affect humans' interaction intentions which reveals what the interaction is. Meanwhile the interacting humans and objects exhibit matching geometric structures which presents how to interact. In light of this we propose harnessing these inherent correlations between interaction counterparts to mitigate the uncertainty and jointly anticipate the above interaction elements in 3D space. To achieve this we present LEMON (LEarning 3D huMan-Object iNteraction relation) a unified model that mines interaction intentions of the counterparts and employs curvatures to guide the extraction of geometric correlations combining them to anticipate the interaction elements. Besides the 3D Interaction Relation dataset (3DIR) is collected to serve as the test bed for training and evaluation. Extensive experiments demonstrate the superiority of LEMON over methods estimating each element in isolation.
</i>
</p>
<p align="justify">
</p><pre id="yang2023lemon-bib" style="display: none;">
@inproceedings{yang2024lemon,
    title={LEMON: Learning 3D Human-Object Interaction Relation from 2D Images},
    author={Yang, Yuhang and Zhai, Wei and Luo, Hongchen and Cao, Yang and Zha, Zheng-Jun},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={16284--16295},
    year={2024}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("yang2023lemon-bib");hideblock("yang2023lemon-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/wang2024mambapupil.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://openaccess.thecvf.com/content/CVPR2024W/AI4Streaming/papers/Wang_MambaPupil_Bidirectional_Selective_Recurrent_Model_for_Event-based_Eye_Tracking_CVPRW_2024_paper.pdf" target="“blank”">Mambapupil: Bidirectional Selective Recurrent Model for Event-based Eye Tracking</a></b>
<br>Zhong Wang, Zengyu Wan, Han Han, Bohao Liao, Yuliang Wu, <b>Wei Zhai*</b>, Yang Cao, Zheng-Jun Zha.
<br><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR 2024</b>), Workshop.</i>
<br><b>Event-based Eye Tracking-AIS2024 CVPR Workshop, 1st Place.</b>
<br>
<a href="javascript:toggleblock(&#39;wang2024mambapupil-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;wang2024mambapupil-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="wang2024mambapupil-abs" style="display: none;">
Event-based eye tracking has shown great promise with the high temporal resolution and low redundancy provided by the event camera. However the diversity and abruptness of eye movement patterns including blinking fixating saccades and smooth pursuit pose significant challenges for eye localization. To achieve a stable event-based eye-tracking system this paper proposes a bidirectional long-term sequence modeling and time-varying state selection mechanism to fully utilize contextual temporal information in response to the variability of eye movements. Specifically the MambaPupil network is proposed which consists of the multi-layer convolutional encoder to extract features from the event representations a bidirectional Gated Recurrent Unit (GRU) and a Linear Time-Varying State Space Module (LTV-SSM) to selectively capture contextual correlation from the forward and backward temporal relationship. Furthermore the Bina-rep is utilized as a compact event representation and the tailor-made data augmentation called as Event-Cutout is proposed to enhance the model's robustness by applying spatial random masking to the event image. The evaluation of the ThreeET-plus benchmark shows that the MambaPupil realizes stable and accurate eye tracking under various complex conditions and achieves state-of-the-art performance.
</i>
</p>
<p align="justify">
</p><pre id="wang2024mambapupil-bib" style="display: none;">
@inproceedings{wang2024mambapupil,
    title={Mambapupil: Bidirectional selective recurrent model for event-based eye tracking},
    author={Wang, Zhong and Wan, Zengyu and Han, Han and Liao, Bohao and Wu, Yuliang and Zhai, Wei and Cao, Yang and Zha, Zheng-jun},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={5762--5770},
    year={2024}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("wang2024mambapupil-bib");hideblock("wang2024mambapupil-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/liang2023hypercorrelation.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://ojs.aaai.org/index.php/AAAI/article/download/28117/28238" target="“blank”">Hypercorrelation Evolution for Video Class-Incremental Learning</a></b>
<br>Sen Liang, Kai Zhu*, Zhiheng Liu, <b>Wei Zhai*</b>, Yang Cao.
<br><i>AAAI Conference on Artificial Intelligence (<b>AAAI 2024</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;liang2023hypercorrelation-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;liang2023hypercorrelation-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="liang2023hypercorrelation-abs" style="display: none;">
Video class-incremental learning aims to recognize new actions while restricting the catastrophic forgetting of old ones, whose representative samples can only be saved in limited memory. Semantically variable subactions are susceptible to class confusion due to data imbalance. While existing methods address the problem by estimating and distilling the spatio-temporal knowledge, we further explores that the refinement of hierarchical correlations is crucial for the alignment of spatio-temporal features. To enhance the adaptability on evolved actions, we proposes a hierarchical aggregation strategy, in which hierarchical matching matrices are combined and jointly optimized to selectively store and retrieve relevant features from previous tasks. Meanwhile, a correlation refinement mechanism is presented to reinforce the bias on informative exemplars according to online hypercorrelation distribution. Experimental results demonstrate the effectiveness of the proposed method on three standard video class-incremental learning benchmarks, outperforming state-of-the-art methods. Code is available at: https://github.com/Lsen991031/HCE.
</i>
</p>
<p align="justify">
</p><pre id="liang2023hypercorrelation-bib" style="display: none;">
@inproceedings{liang2024hypercorrelation,
    title={Hypercorrelation Evolution for Video Class-Incremental Learning},
    author={Liang, Sen and Zhu, Kai and Zhai, Wei and Liu, Zhiheng and Cao, Yang},
    booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
    volume={38},
    number={4},
    pages={3315--3323},
    year={2024}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("liang2023hypercorrelation-bib");hideblock("liang2023hypercorrelation-abs");</script>
<p></p>
</div></div>


<p><b>2023</b></p>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/luo2022grounded.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://link.springer.com/article/10.1007/s11263-023-01962-z" target="“blank”">Grounded Affordance from Exocentric View</a></b>
<br>Hongchen Luo, <b>Wei Zhai</b>, Jing Zhang, Yang Cao, Dacheng Tao.
<br><i>International Journal of Computer Vision (<b>IJCV</b>).</i>
<br><i><font color=#666666>Journal version of "<b>Learning Affordance Grounding from Exocentric Images</b>" (CVPR 2022)</font></i>
<br>
<a href="javascript:toggleblock(&#39;luo2022grounded-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;luo2022grounded-bib&#39;)">bibtex</a> / <a href="https://arxiv.org/abs/2208.13196" target=&ldquo;blank&rdquo;><font color=#BB0000>arxiv</font></a> / <a href="https://github.com/lhc1224/Cross-View-AG" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="luo2022grounded-abs" style="display: none;">
Affordance grounding aims to locate objects' "action possibilities" regions, which is an essential step toward embodied intelligence. Due to the diversity of interactive affordance, the uniqueness of different individuals leads to diverse interactions, which makes it difficult to establish an explicit link between object parts and affordance labels. Human has the ability that transforms the various exocentric interactions into invariant egocentric affordance to counter the impact of interactive diversity. To empower an agent with such ability, this paper proposes a task of affordance grounding from exocentric view, i.e., given exocentric human-object interaction and egocentric object images, learning the affordance knowledge of the object and transferring it to the egocentric image using only the affordance label as supervision. However, there is some "interaction bias" between personas, mainly regarding different regions and different views. To this end, we devise a cross-view affordance knowledge transfer framework that extracts affordance-specific features from exocentric interactions and transfers them to the egocentric view. Specifically, the perception of affordance regions is enhanced by preserving affordance co-relations. In addition, an affordance grounding dataset named AGD20K is constructed by collecting and labeling over 20K images from [Math Processing Error] affordance categories. Experimental results demonstrate that our method outperforms the representative models regarding objective metrics and visual quality. 
</i>
</p>
<p align="justify">
</p><pre id="luo2022grounded-bib" style="display: none;">
@article{luo2024grounded,
    title={Grounded affordance from exocentric view},
    author={Luo, Hongchen and Zhai, Wei and Zhang, Jing and Cao, Yang and Tao, Dacheng},
    journal={International Journal of Computer Vision},
    volume={132},
    number={6},
    pages={1945--1969},
    year={2024},
    publisher={Springer}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("luo2022grounded-bib");hideblock("luo2022grounded-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/zhai2021on.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://ieeexplore.ieee.org/abstract/document/10286884/" target="“blank”">On Exploring Multiplicity of Primitives and Attributes for Texture Recognition in the Wild</a></b>
<br><b>Wei Zhai</b>, Yang Cao, Jing Zhang, Haiyong Xie, Dacheng Tao, Zheng-Jun Zha.
<br><i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>T-PAMI</b>).</i>
<br><i><font color=#666666>Journal version of "<b>Deep Multiple-Attribute-Perceived Network for Real-World Texture Recognition</b>" (ICCV 2019) and "<b>Deep Structure-Revealed Network for Texture Recognition</b>" (CVPR 2020).</font></i>
<br>
<a href="javascript:toggleblock(&#39;zhai2021on-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;zhai2021on-bib&#39;)">bibtex</a> / <a href="https://github.com/tiaotiao11-22/MPAP" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="zhai2021on-abs" style="display: none;">
Texture recognition is a challenging visual task since its multiple primitives or attributes can be perceived from the texture image under different spatial contexts. Existing approaches predominantly built upon CNN incorporate rich local descriptors with orderless aggregation to capture invariance to the spatial layout. However, these methods ignore the inherent structure relation organized by primitives and the semantic concept described by attributes, which are critical cues for texture representation. In this paper, we propose a novel Multiple Primitives and Attributes Perception network (MPAP) that extracts features by modeling the relation of bottom-up structure and top-down attribute in a multi-branch unified framework. A bottom-up process is first proposed to capture the inherent relation of various primitive structures by leveraging structure dependency and spatial order information. Then, a top-down process is introduced to model the latent relation of multiple attributes by transferring attribute-related features between adjacent branches. Moreover, an augmentation module is devised to bridge the gap between high-level attributes and low-level structure features. MPAP can learn representation through jointing bottom-up and top-down processes in a mutually reinforced manner. Experimental results on six challenging texture datasets demonstrate the superiority of MPAP over state-of-the-art methods in terms of accuracy, robustness, and efficiency.
</i>
</p>
<p align="justify">
</p><pre id="zhai2021on-bib" style="display: none;">
@article{zhai2023exploring,
    title={On exploring multiplicity of primitives and attributes for texture recognition in the wild},
    author={Zhai, Wei and Cao, Yang and Zhang, Jing and Xie, Haiyong and Tao, Dacheng and Zha, Zheng-Jun},
    journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
    year={2023},
    publisher={IEEE}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("zhai2021on-bib");hideblock("zhai2021on-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/zhai2022background.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://link.springer.com/article/10.1007/s11263-023-01919-2" target="“blank”">Background Activation Suppression for Weakly Supervised Object Localization and Semantic Segmentation</a></b>
<br><b>Wei Zhai</b>, Pingyu Wu, Kai Zhu, Yang Cao, Feng Wu, Zheng-Jun Zha.
<br><i>International Journal of Computer Vision (<b>IJCV</b>).</i>
<br><i><font color=#666666>Journal version of "<b>Background Activation Suppression for Weakly Supervised Object Localization</b>" (CVPR 2022)</font></i>
<br>
<a href="javascript:toggleblock(&#39;zhai2022background-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;zhai2022background-bib&#39;)">bibtex</a> / <a href="https://arxiv.org/abs/2309.12943" target=&ldquo;blank&rdquo;><font color=#BB0000>arxiv</font></a> / <a href="https://github.com/wpy1999/BAS-Extension" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="zhai2022background-abs" style="display: none;">
Weakly supervised object localization and semantic segmentation aim to localize objects using only image-level labels. Recently, a new paradigm has emerged by generating a foreground prediction map (FPM) to achieve pixel-level localization. While existing FPM-based methods use cross-entropy to evaluate the foreground prediction map and to guide the learning of the generator, this paper presents two astonishing experimental observations on the object localization learning process: For a trained network, as the foreground mask expands, 1) the cross-entropy converges to zero when the foreground mask covers only part of the object region. 2) The activation value continuously increases until the foreground mask expands to the object boundary. Therefore, to achieve a more effective localization performance, we argue for the usage of activation value to learn more object regions. In this paper, we propose a Background Activation Suppression (BAS) method. Specifically, an Activation Map Constraint (AMC) module is designed to facilitate the learning of generator by suppressing the background activation value. Meanwhile, by using foreground region guidance and area constraint, BAS can learn the whole region of the object. In the inference phase, we consider the prediction maps of different categories together to obtain the final localization results. Extensive experiments show that BAS achieves significant and consistent improvement over the baseline methods on the CUB-200-2011 and ILSVRC datasets. In addition, our method also achieves state-of-the-art weakly supervised semantic segmentation performance on the PASCAL VOC 2012 and MS COCO 2014 datasets.
</i>
</p>
<p align="justify">
</p><pre id="zhai2022background-bib" style="display: none;">
@article{zhai2024background,
    title={Background activation suppression for weakly supervised object localization and semantic segmentation},
    author={Zhai, Wei and Wu, Pingyu and Zhu, Kai and Cao, Yang and Wu, Feng and Zha, Zheng-Jun},
    journal={International Journal of Computer Vision},
    volume={132},
    number={3},
    pages={750--775},
    year={2024},
    publisher={Springer}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("zhai2022background-bib");hideblock("zhai2022background-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/luo2021learning.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://ieeexplore.ieee.org/abstract/document/10246333/" target="“blank”">Learning Visual Affordance Grounding from Demonstration Videos</a></b>
<br>Hongchen Luo, <b>Wei Zhai</b>, Jing Zhang, Yang Cao, Dacheng Tao.
<br><i>IEEE Transactions on Neural Networks and Learning Systems (<b>T-NNLS</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;luo2021learning-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;luo2021learning-bib&#39;)">bibtex</a> / <a href="https://arxiv.org/abs/2108.05675" target=&ldquo;blank&rdquo;><font color=#BB0000>arxiv</font></a> / <a href="https://github.com/lhc1224/HAG-Net" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="luo2021learning-abs" style="display: none;">
Visual affordance grounding aims to segment all possible interaction regions between people and objects from an image/video, which benefits many applications, such as robot grasping and action recognition. Prevailing methods predominantly depend on the appearance feature of the objects to segment each region of the image, which encounters the following two problems: 1) there are multiple possible regions in an object that people interact with and 2) there are multiple possible human interactions in the same object region. To address these problems, we propose a hand-aided affordance grounding network (HAG-Net) that leverages the aided clues provided by the position and action of the hand in demonstration videos to eliminate the multiple possibilities and better locate the interaction regions in the object. Specifically, HAG-Net adopts a dual-branch structure to process the demonstration video and object image data. For the video branch, we introduce hand-aided attention to enhance the region around the hand in each video frame and then use the long short-term memory (LSTM) network to aggregate the action features. For the object branch, we introduce a semantic enhancement module (SEM) to make the network focus on different parts of the object according to the action classes and utilize a distillation loss to align the output features of the object branch with that of the video branch and transfer the knowledge in the video branch to the object branch. Quantitative and qualitative evaluations on two challenging datasets show that our method has achieved state-of-the-art results for affordance grounding.
</i>
</p>
<p align="justify">
</p><pre id="luo2021learning-bib" style="display: none;">
@article{luo2023learning,
    title={Learning visual affordance grounding from demonstration videos},
    author={Luo, Hongchen and Zhai, Wei and Zhang, Jing and Cao, Yang and Tao, Dacheng},
    journal={IEEE Transactions on Neural Networks and Learning Systems},
    year={2023},
    publisher={IEEE}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("luo2021learning-bib");hideblock("luo2021learning-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/wu2023spatial.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Spatial-Aware_Token_for_Weakly_Supervised_Object_Localization_ICCV_2023_paper.pdf" target="“blank”">Spatial-Aware Token for Weakly Supervised Object Localization</a></b>
<br>Pingyu Wu, <b>Wei Zhai*</b>, Yang Cao, Jiebo Luo and Zheng-Jun Zha.
<br><i>IEEE/CVF International Conference on Computer Vision (<b>ICCV 2023</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;wu2023spatial-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;wu2023spatial-bib&#39;)">bibtex</a> / <a href="https://arxiv.org/abs/2303.10438" target=&ldquo;blank&rdquo;><font color=#BB0000>arxiv</font></a> / <a href="https://github.com/wpy1999/SAT" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="wu2023spatial-abs" style="display: none;">
Weakly supervised object localization (WSOL) is a challenging task aiming to localize objects with only image-level supervision. Recent works apply visual transformer to WSOL and achieve significant success by exploiting the long-range feature dependency in self-attention mechanism. However, existing transformer-based methods synthesize the classification feature maps as the localization map, which leads to optimization conflicts between classification and localization tasks. To address this problem, we propose to learn a task-specific spatial-aware token (SAT) to condition localization in a weakly supervised manner. Specifically, a spatial token is first introduced in the input space to aggregate representations for localization task. Then a spatial aware attention module is constructed, which allows spatial token to generate foreground probabilities of different patches by querying and to extract localization knowledge from the classification task. Besides, for the problem of sparse and unbalanced pixel-level supervision obtained from the image-level label, two spatial constraints, including batch area loss and normalization loss, are designed to compensate and enhance this supervision. Experiments show that the proposed SAT achieves state-of-the-art performance on both CUB-200 and ImageNet, with 98.45% and 73.13% GT-known Loc, respectively. Even under the extreme setting of using only 1 image per class from ImageNet for training, SAT already exceeds the SOTA method by 2.1% GT-known Loc.
</i>
</p>
<p align="justify">
</p><pre id="wu2023spatial-bib" style="display: none;">
@inproceedings{wu2023spatial,
    title={Spatial-aware token for weakly supervised object localization},
    author={Wu, Pingyu and Zhai, Wei and Cao, Yang and Luo, Jiebo and Zha, Zheng-Jun},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
    pages={1844--1854},
    year={2023}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("wu2023spatial-bib");hideblock("wu2023spatial-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/yang2023grounding.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Grounding_3D_Object_Affordance_from_2D_Interactions_in_Images_ICCV_2023_paper.pdf" target="“blank”">Grounding 3D Object Affordance from 2D Interactions in Images</a></b>
<br>Yuhang Yang, <b>Wei Zhai*</b>, Hongchen Luo, Yang Cao, Jiebo Luo and Zheng-Jun Zha.
<br><i>IEEE/CVF International Conference on Computer Vision (<b>ICCV 2023</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;yang2023grounding-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;yang2023grounding-bib&#39;)">bibtex</a> / <a href="https://arxiv.org/abs/2303.10438" target=&ldquo;blank&rdquo;><font color=#BB0000>arxiv</font></a> / <a href="https://github.com/yyvhang/IAGNet" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="yang2023grounding-abs" style="display: none;">
Grounding 3D object affordance seeks to locate objects'" action possibilities" regions in the 3D space, which serves as a link between perception and operation for embodied agents. Existing studies primarily focus on connecting visual affordances with geometry structures, eg, relying on annotations to declare interactive regions of interest on the object and establishing a mapping between the regions and affordances. However, the essence of learning object affordance is to understand how to use it, and the manner that detaches interactions is limited in generalization. Normally, humans possess the ability to perceive object affordances in the physical world through demonstration images or videos. Motivated by this, we introduce a novel task setting: grounding 3D object affordance from 2D interactions in images, which faces the challenge of anticipating affordance through interactions of different sources. To address this problem, we devise a novel Interaction-driven 3D Affordance Grounding Network (IAG), which aligns the region feature of objects from different sources and models the interactive contexts for 3D object affordance grounding. Besides, we collect a Point-Image Affordance Dataset (PIAD) to support the proposed task. Comprehensive experiments on PIAD demonstrate the reliability of the proposed task and the superiority of our method.
</i>
</p>
<p align="justify">
</p><pre id="yang2023grounding-bib" style="display: none;">
@inproceedings{yang2023grounding,
    title={Grounding 3d object affordance from 2d interactions in images},
    author={Yang, Yuhang and Zhai, Wei and Luo, Hongchen and Cao, Yang and Luo, Jiebo and Zha, Zheng-Jun},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
    pages={10905--10915},
    year={2023}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("yang2023grounding-bib");hideblock("yang2023grounding-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/wang2023robustness.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://just.ustc.edu.cn/en/article/doi/10.52396/JUSTC-2022-0165" target="“blank”">Robustness Benchmark for Unsupervised Anomaly Detection Models</a></b>
<br>Pei Wang, <b>Wei Zhai</b>, and Yang Cao.
<br><i>Journal of University of Science and Technology of China (JUSTC).</i>
<br>
<a href="javascript:toggleblock(&#39;wang2023robustness-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;wang2023robustness-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="wang2023robustness-abs" style="display: none;">
Due to the complexity and diversity of production environments, it is essential to understand the robustness of unsupervised anomaly detection models to common corruptions. To explore this issue systematically, we propose a dataset named MVTec-C to evaluate the robustness of unsupervised anomaly detection models. Based on this dataset, we explore the robustness of approaches in five paradigms, namely, reconstruction-based, representation similarity-based, normalizing flow-based, self-supervised representation learning-based, and knowledge distillation-based paradigms. Furthermore, we explore the impact of different modules within two optimal methods on robustness and accuracy. This includes the multi-scale features, the neighborhood size, and the sampling ratio in the PatchCore method, as well as the multi-scale features, the MMF module, the OCE module, and the multi-scale distillation in the Reverse Distillation method. Finally, we propose a feature alignment module (FAM) to reduce the feature drift caused by corruptions and combine PatchCore and the FAM to obtain a model with both high performance and high accuracy. We hope this work will serve as an evaluation method and provide experience in building robust anomaly detection models in the future.
</i>
</p>
<p align="justify">
</p><pre id="wang2023robustness-bib" style="display: none;">
@article{wang2024robustness,
    title={Robustness benchmark for unsupervised anomaly detection models},
    author={Wang, Pei and Zhai, Wei and Cao, Yang},
    journal={JUSTC},
    volume={54},
    number={1},
    pages={0103--1},
    year={2024},
    publisher={JUSTC}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("wang2023robustness-bib");hideblock("wang2023robustness-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/luo2023leverage.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Leverage_Interactive_Affinity_for_Affordance_Learning_CVPR_2023_paper.pdf" target="“blank”">Leverage Interactive Affinity for Affordance Learning</a></b>
<br>Hongchen Luo#, <b>Wei Zhai#</b>, Jing Zhang, Yang Cao, and Dacheng Tao.
<br><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR 2023</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;luo2023leverage-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;luo2023leverage-bib&#39;)">bibtex</a> / <a href="https://github.com/lhc1224/PIAL-Net" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="luo2023leverage-abs" style="display: none;">
Perceiving potential" action possibilities"(ie, affordance) regions of images and learning interactive functionalities of objects from human demonstration is a challenging task due to the diversity of human-object interactions. Prevailing affordance learning algorithms often adopt the label assignment paradigm and presume that there is a unique relationship between functional region and affordance label, yielding poor performance when adapting to unseen environments with large appearance variations. In this paper, we propose to leverage interactive affinity for affordance learning, ie, extracting interactive affinity from human-object interaction and transferring it to non-interactive objects. Interactive affinity, which represents the contacts between different parts of the human body and local regions of the target object, can provide inherent cues of interconnectivity between humans and objects, thereby reducing the ambiguity of the perceived action possibilities. Specifically, we propose a pose-aided interactive affinity learning framework that exploits human pose to guide the network to learn the interactive affinity from human-object interactions. Particularly, a keypoint heuristic perception (KHP) scheme is devised to exploit the keypoint association of human pose to alleviate the uncertainties due to interaction diversities and contact occlusions. Besides, a contact-driven affordance learning (CAL) dataset is constructed by collecting and labeling over 5,000 images. Experimental results demonstrate that our method outperforms the representative models regarding objective metrics and visual quality.
</i>
</p>
<p align="justify">
</p><pre id="luo2023leverage-bib" style="display: none;">
@inproceedings{luo2023leverage,
    title={Leverage interactive affinity for affordance learning},
    author={Luo, Hongchen and Zhai, Wei and Zhang, Jing and Cao, Yang and Tao, Dacheng},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={6809--6819},
    year={2023}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("luo2023leverage-bib");hideblock("luo2023leverage-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/lu2023uncertainty.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Uncertainty-Aware_Optimal_Transport_for_Semantically_Coherent_Out-of-Distribution_Detection_CVPR_2023_paper.pdf" target="“blank”">Uncertainty-Aware Optimal Transport for Semantically Coherent Out-of-Distribution Detection</a></b>
<br>Fan Lu, Kai Zhu, <b>Wei Zhai</b>, Kecheng Zheng, and Yang Cao.
<br><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR 2023</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;lu2023uncertainty-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;lu2023uncertainty-bib&#39;)">bibtex</a> / <a href="https://github.com/LuFan31/ET-OOD" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="lu2023uncertainty-abs" style="display: none;">
Semantically coherent out-of-distribution (SCOOD) detection aims to discern outliers from the intended data distribution with access to unlabeled extra set. The coexistence of in-distribution and out-of-distribution samples will exacerbate the model overfitting when no distinction is made. To address this problem, we propose a novel uncertainty-aware optimal transport scheme. Our scheme consists of an energy-based transport (ET) mechanism that estimates the fluctuating cost of uncertainty to promote the assignment of semantic-agnostic representation, and an inter-cluster extension strategy that enhances the discrimination of semantic property among different clusters by widening the corresponding margin distance. Furthermore, a T-energy score is presented to mitigate the magnitude gap between the parallel transport and classifier branches. Extensive experiments on two standard SCOOD benchmarks demonstrate the above-par OOD detection performance, outperforming the state-of-the-art methods by a margin of 27.69% and 34.4% on FPR@ 95, respectively.
</i>
</p>
<p align="justify">
</p><pre id="lu2023uncertainty-bib" style="display: none;">
@inproceedings{lu2023uncertainty,
    title={Uncertainty-Aware Optimal Transport for Semantically Coherent Out-of-Distribution Detection},
    author={Lu, Fan and Zhu, Kai and Zhai, Wei and Zheng, Kecheng and Cao, Yang},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={3282--3291},
    year={2023}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("lu2023uncertainty-bib");hideblock("lu2023uncertainty-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/dong2023exploring.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://ojs.aaai.org/index.php/AAAI/article/download/25128/24900" target="“blank”">Exploring Tuning Characteristics of Ventral Stream's Neurons for Few-Shot Image Classification</a></b>
<br>Lintao Dong, <b>Wei Zhai</b>, Zheng-Jun Zha.
<br><i>AAAI Conference on Artificial Intelligence (<b>AAAI 2023, <i><font color=#FF0000>Oral</font></i>, <i><font color=#FF0000>Distinguished Paper</font></i></b>).</i>
<br>
<a href="javascript:toggleblock(&#39;dong2023exploring-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;dong2023exploring-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="dong2023exploring-abs" style="display: none;">
Human has the remarkable ability of learning novel objects by browsing extremely few examples, which may be attributed to the generic and robust feature extracted in the ventral stream of our brain for representing visual objects. In this sense, the tuning characteristics of ventral stream's neurons can be useful prior knowledge to improve few-shot classification. Specifically, we computationally model two groups of neurons found in ventral stream which are respectively sensitive to shape cues and color cues. Then we propose the hierarchical feature regularization method with these neuron models to regularize the backbone of a few-shot model, thus making it produce more generic and robust features for few-shot classification. In addition, to simulate the tuning characteristic that neuron firing at a higher rate in response to foreground stimulus elements compared to background elements, which we call belongingness, we design a foreground segmentation algorithm based on the observation that the foreground object usually does not appear at the edge of the picture, then multiply the foreground mask with the backbone of few-shot model. Our method is model-agnostic and can be applied to few-shot models with different backbones, training paradigms and classifiers.
</i>
</p>
<p align="justify">
</p><pre id="dong2023exploring-bib" style="display: none;">
@inproceedings{dong2023exploring,
    title={Exploring Tuning Characteristics of Ventral Stream’s Neurons for Few-Shot Image Classification},
    author={Dong, Lintao and Zhai, Wei and Zha, Zheng-Jun},
    booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
    volume={37},
    number={1},
    pages={534--542},
    year={2023}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("dong2023exploring-bib");hideblock("dong2023exploring-abs");</script>
<p></p>
</div></div>


<p><b>2022</b></p>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/zhai2022exploring.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/6cc31b44d88dce8380d36e81485cd07f-Paper-Conference.pdf" target="“blank”">Exploring Figure-Ground Assignment Mechanism in Perceptual Organization</a></b>
<br><b>Wei Zhai</b>, Yang Cao, jing Zhang, Zheng-Jun Zha.
<br><i>Neural Information Processing Systems (<b>NeurIPS 2022</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;zhai2022exploring-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;zhai2022exploring-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="zhai2022exploring-abs" style="display: none;">
Perceptual organization is a challenging visual task that aims to perceive and group the individual visual element so that it is easy to understand the meaning of the scene as a whole. Most recent methods building upon advanced Convolutional Neural Network (CNN) come from learning discriminative representation and modeling context hierarchically. However, when the visual appearance difference between foreground and background is obscure, the performance of existing methods degrades significantly due to the visual ambiguity in the discrimination process. In this paper, we argue that the figure-ground assignment mechanism, which conforms to human vision cognitive theory, can be explored to empower CNN to achieve a robust perceptual organization despite visual ambiguity. Specifically, we present a novel Figure-Ground-Aided (FGA) module to learn the configural statistics of the visual scene and leverage it for the reduction of visual ambiguity. Particularly, we demonstrate the benefit of using stronger supervisory signals by teaching (FGA) module to perceive configural cues, \ie, convexity and lower region, that human deem important for the perceptual organization. Furthermore, an Interactive Enhancement Module (IEM) is devised to leverage such configural priors to assist representation learning, thereby achieving robust perception organization with complex visual ambiguities. In addition, a well-founded visual segregation test is designed to validate the capability of the proposed FGA mechanism explicitly. Comprehensive evaluation results demonstrate our proposed FGA mechanism can effectively enhance the capability of perception organization on various baseline models. Nevertheless, the model augmented via our proposed FGA mechanism also outperforms state-of-the-art approaches on four challenging real-world applications.
</i>
</p>
<p align="justify">
</p><pre id="zhai2022exploring-bib" style="display: none;">
@article{zhai2022exploring,
    title={Exploring figure-ground assignment mechanism in perceptual organization},
    author={Zhai, Wei and Cao, Yang and Zhang, Jing and Zha, Zheng-Jun},
    journal={Advances in Neural Information Processing Systems},
    volume={35},
    pages={17030--17042},
    year={2022}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("zhai2022exploring-bib");hideblock("zhai2022exploring-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/lu2022phrase.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/6cc31b44d88dce8380d36e81485cd07f-Paper-Conference.pdf" target="“blank”">Phrase-Based Affordance Detection via Cyclic Bilateral Interaction</a></b>
<br>Liangsheng Lu#, <b>Wei Zhai#</b>, Hongchen Luo, Kang Yu, Yang Cao.
<br><i>IEEE Artificial Intelligence (<b>T-AI</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;lu2022phrase-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;lu2022phrase-bib&#39;)">bibtex</a> / <a href="https://arxiv.org/abs/2202.12076" target=&ldquo;blank&rdquo;><font color=#BB0000>arxiv</font></a> / <a href="https://github.com/lulsheng/CBCE-Net" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="lu2022phrase-abs" style="display: none;">
Affordance detection, which refers to perceiving objects with potential action possibilities in images, is a challenging task since the possible affordance depends on the person's purpose in real-world application scenarios. The existing works mainly extract the inherent human–object dependencies from image/video to accommodate affordance properties that change dynamically. In this article, we explore to perceive affordances from a vision-language perspective, and consider the challenging phrase-based affordance detection task, i.e., given a set of phrases describing the potential actions, all the object regions in a scene with the same affordance should be detected. To this end, we propose a cyclic bilateral c onsistency enhancement network (CBCE-Net) to align language and vision features in a progressive manner. Specifically, the presented CBCE-Net consists of a mutual guided vision-language module that updates the common features of vision and language in a progressive manner, and a cyclic interaction module that facilitates the perception of possible interaction with objects in a cyclic manner. In addition, we extend the public purpose-driven affordance dataset (PAD) by annotating affordance categories with short phrases. The extensive contrastive experimental results demonstrate the superior performance of our method over nine typical methods from four relevant fields in terms of both objective metrics and visual quality.
</i>
</p>
<p align="justify">
</p><pre id="lu2022phrase-bib" style="display: none;">
@article{lu2022phrase,
    title={Phrase-based affordance detection via cyclic bilateral interaction},
    author={Lu, Liangsheng and Zhai, Wei and Luo, Hongchen and Kang, Yu and Cao, Yang},
    journal={IEEE Transactions on Artificial Intelligence},
    year={2022},
    publisher={IEEE}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("lu2022phrase-bib");hideblock("lu2022phrase-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/zhai2022one.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://link.springer.com/article/10.1007/s11263-022-01642-4" target="“blank”">One-Shot Affordance Detection in the Wild</a></b>
<br><b>Wei Zhai#</b>, Hongchen Luo#, Jing Zhang, Yang Cao, Dacheng Tao.
<br><i>International Journal of Computer Vision (<b>IJCV</b>).</i>
<br><i><font color=#666666>Journal version of "<b>One-Shot Affordance Detection</b>" (IJCAI 2021)</font></i>
<br>
<a href="javascript:toggleblock(&#39;zhai2022one-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;zhai2022one-bib&#39;)">bibtex</a> / <a href="https://arxiv.org/pdf/2108.03658.pdf" target=&ldquo;blank&rdquo;><font color=#BB0000>arxiv</font></a> / <a href="https://github.com/lhc1224/OSAD_Net" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="zhai2022one-abs" style="display: none;">
Affordance detection refers to identifying the potential action possibilities of objects in an image, which is a crucial ability for robot perception and manipulation. To empower robots with this ability in unseen scenarios, we first study the challenging one-shot affordance detection problem in this paper, i.e., given a support image that depicts the action purpose, all objects in a scene with the common affordance should be detected. To this end, we devise a One-Shot Affordance Detection Network (OSAD-Net) that firstly estimates the human action purpose and then transfers it to help detect the common affordance from all candidate images. Through collaboration learning, OSAD-Net can capture the common characteristics between objects having the same underlying affordance and learn a good adaptation capability for perceiving unseen affordances. Besides, we build a large-scale purpose-driven affordance dataset v2 (PADv2) by collecting and labeling 30k images from 39 affordance and 103 object categories. With complex scenes and rich annotations, our PADv2 dataset can be used as a test bed to benchmark affordance detection methods and may also facilitate downstream vision tasks, such as scene understanding, action recognition, and robot manipulation. Specifically, we conducted comprehensive experiments on PADv2 dataset by including 11 advanced models from several related research fields. Experimental results demonstrate the superiority of our model over previous representative ones in terms of both objective metrics and visual quality. The benchmark suite is available at https://github.com/lhc1224/OSAD_Net.
</i>
</p>
<p align="justify">
</p><pre id="zhai2022one-bib" style="display: none;">
@article{zhai2022one,
    title={One-shot object affordance detection in the wild},
    author={Zhai, Wei and Luo, Hongchen and Zhang, Jing and Cao, Yang and Tao, Dacheng},
    journal={International Journal of Computer Vision},
    volume={130},
    number={10},
    pages={2472--2500},
    year={2022},
    publisher={Springer}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("zhai2022one-bib");hideblock("zhai2022one-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/zhai2022deep.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://ieeexplore.ieee.org/abstract/document/9815160" target="“blank”">Deep Texton-Coherence Network for Camouflaged Object Detection</a></b>
<br><b>Wei Zhai</b>, Yang Cao, Haiyong Xie, Zheng-Jun Zha.
<br><i>IEEE Transactions on Multimedia (<b>T-MM</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;zhai2022deep-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;zhai2022deep-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="zhai2022deep-abs" style="display: none;">
Camouflaged object detection is a challenging visual task since the appearance and morphology of foreground objects and background regions are highly similar in nature. Recent CNN-based studies gradually integrated the high-level semantic information and the low-level local features of images through hierarchical and progressive structures to achieve camouflaged object detection. However, these methods ignore the spatial statistical properties of the local context, which is a critical cue for distinguishing and describing camouflaged objects. To address this problem, we propose a novel Deep Texton-Coherence Network (DTC-Net) that leverages the spatial organization of textons in the foreground and background regions as discriminative cues for camouflaged object detection. Specifically, a Local Bilinear module (LB) is devised to obtain the robust representation of texton to trivial details and illumination changes, by replacing the classic first-order linearization operations with bilinear second-order statistical operations in the convolution process. Next, these texton representations are associated with a Spatial Coherence Organization module (SCO) to capture irregular spatial coherence via a deformable convolutional strategy, and then the descriptions of the textons extracted by the LB module are used as weights to suppress features that are spatially adjacent but have different representations. Finally, the texton-coherence representation is integrated with the original features at different levels to achieve camouflaged object detection. Evaluation on the three most challenging camouflaged object detection datasets demonstrats the superiority of the proposed model when compared to the state-of-the-art methods. Furthermore, our ablation studies and performance analyses demonstrate the effectiveness of the texton-coherence module.
</i>
</p>
<p align="justify">
</p><pre id="zhai2022deep-bib" style="display: none;">
@article{zhai2022deep,
    title={Deep texton-coherence network for camouflaged object detection},
    author={Zhai, Wei and Cao, Yang and Xie, HaiYong and Zha, Zheng-Jun},
    journal={IEEE Transactions on Multimedia},
    year={2022},
    publisher={IEEE}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("zhai2022deep-bib");hideblock("zhai2022deep-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/li2022location.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://ieeexplore.ieee.org/abstract/document/9817812" target="“blank”">Location-Free Camouflage Generation Network</a></b>
<br>Yangyang Li#, <b>Wei Zhai#</b>, Yang Cao, Zheng-Jun Zha.
<br><i>IEEE Transactions on Multimedia (<b>T-MM</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;li2022location-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;li2022location-bib&#39;)">bibtex</a> / <a href="https://arxiv.org/abs/2203.09845" target=&ldquo;blank&rdquo;><font color=#BB0000>arxiv</font></a> / <a href="https://github.com/Tale17/LCG-Net" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="li2022location-abs" style="display: none;">
Camouflage is a common visual phenomenon, which refers to hiding the foreground objects into the background images, making them briefly invisible to the human eye. Previous work has typically been implemented by an iterative optimization process. However, these methods struggle in 1) efficiently generating camouflage images using foreground and background with flexible structure; 2) camouflaging foreground objects to regions with multiple appearances ( e.g. the junction of the vegetation and the mountains), which limit their practical application. To address these problems, this paper proposes a novel L ocation-free C amouflage G eneration Net work ( LCG-Net ) that fuse high-level features of foreground and background image, and generate result by one inference. Specifically, a Position-aligned Structure Fusion (PSF) module is devised to guide structure feature fusion based on the point-to-point structure similarity of foreground and background, and introduce local appearance features point-by-point. To retain the necessary identifiable features, a new immerse loss is adopted under our pipeline, while a background patch appearance loss is utilized to ensure that the hidden objects look continuous and natural at regions with multiple appearances. Experiments show that our method has results as satisfactory as state-of-the-art in the single-appearance regions and are less likely to be completely invisible, but far exceed the quality of the state-of-the-art in the multi-appearance regions. Moreover, our method is hundreds of times faster than previous methods. Benefitting from the unique advantages of our method, we provide some downstream applications for camouflage generation, which show its potential.
</i>
</p>
<p align="justify">
</p><pre id="li2022location-bib" style="display: none;">
@article{li2022location,
    title={Location-free camouflage generation network},
    author={Li, Yangyang and Zhai, Wei and Cao, Yang and Zha, Zheng-jun},
    journal={IEEE Transactions on Multimedia},
    year={2022},
    publisher={IEEE}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("li2022location-bib");hideblock("li2022location-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/luo2022learning.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Luo_Learning_Affordance_Grounding_From_Exocentric_Images_CVPR_2022_paper.pdf" target="“blank”">Learning Affordance Grounding from Exocentric Images</a></b>
<br>Hongchen Luo#, <b>Wei Zhai#</b>, Jing Zhang, Yang Cao, and Dacheng Tao.
<br><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR 2022</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;luo2022learning-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;luo2022learning-bib&#39;)">bibtex</a> / <a href="https://arxiv.org/pdf/2203.09905.pdf" target=&ldquo;blank&rdquo;><font color=#BB0000>arxiv</font></a> / <a href="https://github.com/lhc1224/Cross-View-AG" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="luo2022learning-abs" style="display: none;">
Affordance grounding, a task to ground (i.e., localize) action possibility region in objects, which faces the challenge of establishing an explicit link with object parts due to the diversity of interactive affordance. Human has the ability that transform the various exocentric interactions to invariant egocentric affordance so as to counter the impact of interactive diversity. To empower an agent with such ability, this paper proposes a task of affordance grounding from exocentric view, i.e., given exocentric human-object interaction and egocentric object images, learning the affordance knowledge of the object and transferring it to the egocentric image using only the affordance label as supervision. To this end, we devise a cross-view knowledge transfer framework that extracts affordance-specific features from exocentric interactions and enhances the perception of affordance regions by preserving affordance correlation. Specifically, an Affordance Invariance Mining module is devised to extract specific clues by minimizing the intra-class differences originated from interaction habits in exocentric images. Besides, an Affordance Co-relation Preserving strategy is presented to perceive and localize affordance by aligning the co-relation matrix of predicted results between the two views. Particularly, an affordance grounding dataset named AGD20K is constructed by collecting and labeling over 20K images from 36 affordance categories. Experimental results demonstrate that our method outperforms the representative models in terms of objective metrics and visual quality. Code: github.com/lhc1224/Cross-View-AG.
</i>
</p>
<p align="justify">
</p><pre id="luo2022learning-bib" style="display: none;">
@inproceedings{luo2022learning,
    title={Learning affordance grounding from exocentric images},
    author={Luo, Hongchen and Zhai, Wei and Zhang, Jing and Cao, Yang and Tao, Dacheng},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={2252--2261},
    year={2022}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("luo2022learning-bib");hideblock("luo2022learning-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/wu2022background.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Background_Activation_Suppression_for_Weakly_Supervised_Object_Localization_CVPR_2022_paper.pdf" target="“blank”">Background Activation Suppression for Weakly Supervised Object Localization</a></b>
<br>Pingyu Wu#, <b>Wei Zhai#</b>, Yang Cao.
<br><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR 2022</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;wu2022background-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;wu2022background-bib&#39;)">bibtex</a> / <a href="https://arxiv.org/pdf/2112.00580.pdf" target=&ldquo;blank&rdquo;><font color=#BB0000>arxiv</font></a> / <a href="https://github.com/wpy1999/BAS" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="wu2022background-abs" style="display: none;">
Weakly supervised object localization (WSOL) aims to localize objects using only image-level labels. Recently a new paradigm has emerged by generating a foreground prediction map (FPM) to achieve localization task. Existing FPM-based methods use cross-entropy (CE) to evaluate the foreground prediction map and to guide the learning of generator. We argue for using activation value to achieve more efficient learning. It is based on the experimental observation that, for a trained network, CE converges to zero when the foreground mask covers only part of the object region. While activation value increases until the mask expands to the object boundary, which indicates that more object areas can be learned by using activation value. In this paper, we propose a Background Activation Suppression (BAS) method. Specifically, an Activation Map Constraint module (AMC) is designed to facilitate the learning of generator by suppressing the background activation value. Meanwhile, by using the foreground region guidance and the area constraint, BAS can learn the whole region of the object. In the inference phase, we consider the prediction maps of different categories together to obtain the final localization results. Extensive experiments show that BAS achieves significant and consistent improvement over the baseline methods on the CUB-200-2011 and ILSVRC datasets. Code and models are available at github.com/wpy1999IBAS.
</i>
</p>
<p align="justify">
</p><pre id="wu2022background-bib" style="display: none;">
@inproceedings{wu2022background,
    title={Background activation suppression for weakly supervised object localization},
    author={Wu, Pingyu and Zhai, Wei and Cao, Yang},
    booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    pages={14228--14237},
    year={2022},
    organization={IEEE}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("wu2022background-bib");hideblock("wu2022background-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/zhu2022self.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_Self-Sustaining_Representation_Expansion_for_Non-Exemplar_Class-Incremental_Learning_CVPR_2022_paper.pdf" target="“blank”">Self-Sustaining Representation Expansion for Non-Exemplar Class-Incremental Learnings</a></b>
<br>Kai Zhu, <b>Wei Zhai</b>, Yang Cao, Jiebo Luo, Zheng-Jun Zha.
<br><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR 2022</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;zhu2022self-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;zhu2022self-bib&#39;)">bibtex</a> / <a href="https://arxiv.org/pdf/2203.06359.pdf" target=&ldquo;blank&rdquo;><font color=#BB0000>arxiv</font></a> / <a href="https://github.com/zhukaii/SSRE" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="zhu2022self-abs" style="display: none;">
Non-exemplar class-incremental learning is to recognize both the old and new classes when old class samples cannot be saved. It is a challenging task since representation optimization and feature retention can only be achieved under supervision from new classes. To address this problem, we propose a novel self-sustaining representation expansion scheme. Our scheme consists of a structure reorganization strategy that fuses main-branch expansion and side-branch updating to maintain the old features, and a main-branch distillation scheme to transfer the invariant knowledge. Furthermore, a prototype selection mechanism is proposed to enhance the discrimination between the old and new classes by selectively incorporating new samples into the distillation process. Extensive experiments on three benchmarks demonstrate significant incremental performance, outperforming the state-of-the-art methods by a margin of 3%, 3% and 6%, respectively.
</i>
</p>
<p align="justify">
</p><pre id="zhu2022self-bib" style="display: none;">
@inproceedings{zhu2022self,
    title={Self-sustaining representation expansion for non-exemplar class-incremental learning},
    author={Zhu, Kai and Zhai, Wei and Cao, Yang and Luo, Jiebo and Zha, Zheng-Jun},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={9296--9305},
    year={2022}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("zhu2022self-bib");hideblock("zhu2022self-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/wang2022robust.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://ieeexplore.ieee.org/abstract/document/9697984" target="“blank”">Robust Object Detection via Adversarial Novel Style Exploration</a></b>
<br>Wen Wang, Jing Zhang, <b>Wei Zhai</b>, Yang Cao, Dacheng Tao.
<br><i>IEEE Transactions on Image Processing (<b>T-IP</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;wang2022robust-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;wang2022robust-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="wang2022robust-abs" style="display: none;">
Deep object detection models trained on clean images may not generalize well on degraded images due to the well-known domain shift issue. This hinders their application in real-life scenarios such as video surveillance and autonomous driving. Though domain adaptation methods can adapt the detection model from a labeled source domain to an unlabeled target domain, they struggle in dealing with open and compound degradation types. In this paper, we attempt to address this problem in the context of object detection by proposing a robust object Detector via Adversarial Novel Style Exploration (DANSE). Technically, DANSE first disentangles images into domain-irrelevant content representation and domain-specific style representation under an adversarial learning framework. Then, it explores the style space to discover diverse novel degradation styles that are complementary to those of the target domain images by leveraging a novelty regularizer and a diversity regularizer. The clean source domain images are transferred into these discovered styles by using a content-preserving regularizer to ensure realism. These transferred source domain images are combined with the target domain images and used to train a robust degradation-agnostic object detection model via adversarial domain adaptation. Experiments on both synthetic and real benchmark scenarios confirm the superiority of DANSE over state-of-the-art methods.
</i>
</p>
<p align="justify">
</p><pre id="wang2022robust-bib" style="display: none;">
@article{wang2022robust,
    title={Robust object detection via adversarial novel style exploration},
    author={Wang, Wen and Zhang, Jing and Zhai, Wei and Cao, Yang and Tao, Dacheng},
    journal={IEEE Transactions on Image Processing},
    volume={31},
    pages={1949--1962},
    year={2022},
    publisher={IEEE}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("wang2022robust-bib");hideblock("wang2022robust-abs");</script>
<p></p>
</div></div>


<p><b>2021</b></p>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/luo2021one.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://www.ijcai.org/proceedings/2021/0124.pdf" target="“blank”">Robust Object Detection via Adversarial Novel Style Exploration</a></b>
<br>Hongchen Luo, <b>Wei Zhai</b>, Jing Zhang, Yang Cao, Dacheng Tao.
<br><i>International Joint Conferences on Artificial Intelligence Organization (<b>IJCAI 2021, <i><font color=#FF0000>Oral</font></i></b>).</i>
<br>
<a href="javascript:toggleblock(&#39;luo2021one-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;luo2021one-bib&#39;)">bibtex</a> / <a href="https://arxiv.org/pdf/2106.14747.pdf" target=&ldquo;blank&rdquo;><font color=#BB0000>arxiv</font></a> / <a href="https://github.com/lhc1224/OSAD_Net" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="luo2021one-abs" style="display: none;">
Affordance detection refers to identifying the potential action possibilities of objects in an image, which is an important ability for robot perception and manipulation. To empower robots with this ability in unseen scenarios, we consider the challenging one-shot affordance detection problem in this paper, i.e., given a support image that depicts the action purpose, all objects in a scene with the common affordance should be detected. To this end, we devise a One-Shot Affordance Detection (OS-AD) network that firstly estimates the purpose and then transfers it to help detect the common affordance from all candidate images. Through collaboration learning, OS-AD can capture the common characteristics between objects having the same underlying affordance and learn a good adaptation capability for perceiving unseen affordances. Besides, we build a Purpose-driven Affordance Dataset (PAD) by collecting and labeling 4k images from 31 affordance and 72 object categories. Experimental results demonstrate the superiority of our model over previous representative ones in terms of both objective metrics and visual quality. The benchmark suite is at ProjectPage.
</i>
</p>
<p align="justify">
</p><pre id="luo2021one-bib" style="display: none;">
@article{luo2021one,
    title={One-shot affordance detection},
    author={Luo, Hongchen and Zhai, Wei and Zhang, Jing and Cao, Yang and Tao, Dacheng},
    journal={arXiv preprint arXiv:2106.14747},
    year={2021}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("luo2021one-bib");hideblock("luo2021one-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/li2021tri.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://ietresearch.onlinelibrary.wiley.com/doi/pdfdirect/10.1049/cvi2.12017" target="“blank”">A Tri-Attention Enhanced Graph Convolutional Network for Skeleton-Based Action Recognition</a></b>
<br>Xingming Li, <b>Wei Zhai</b>, Yang Cao.
<br><i>IET Computer Vision (IET-CV 2021).</i>
<br>
<a href="javascript:toggleblock(&#39;li2021tri-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;li2021tri-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="li2021tri-abs" style="display: none;">
Skeleton-based action recognition has recently attracted a lot of research interests due to its advantage in computational efficiency. Some recent work building upon Graph Convolutional Networks (GCNs) has shown promising performance in this task by modelling intrinsic spatial correlations between skeleton joints. However, these methods only consider local properties of action sequences in the spatial-temporal domain, and consequently, are limited in distinguishing complex actions with similar local movements. To address this problem, a novel tri-attention module (TAM) is proposed to guide GCNs to perceive significant variations across local movements. Specifically, the devised TAM is implemented in three steps: i) A dimension permuting unit is proposed to characterise skeleton action sequences in three different domains: body poses, joint trajectories, and evolving projections. ii) A global statistical modelling unit is introduced to aggregate the first-order and second-order properties of global contexts to perceive the significant movement variations of each domain. iii) A fusion unit is presented to integrate the features of these three domains together and leverage as orientation for graph convolution at each layer. Through these three steps, significant-variation frames, joints, and channels can be enhanced. We conduct extensive experiments on two large-scale benchmark datasets, NTU RGB-D and Kinetics-Skeleton. Experimental results demonstrate that the proposed TAM can be easily plugged into existing GCNs and achieve comparable performance with the state-of-the-art methods.
</i>
</p>
<p align="justify">
</p><pre id="li2021tri-bib" style="display: none;">
@article{li2021tri,
    title={A tri-attention enhanced graph convolutional network for skeleton-based action recognition},
    author={Li, Xingming and Zhai, Wei and Cao, Yang},
    journal={IET Computer Vision},
    volume={15},
    number={2},
    pages={110--121},
    year={2021},
    publisher={Wiley Online Library}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("li2021tri-bib");hideblock("li2021tri-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/zhu2021self.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhu_Self-Promoted_Prototype_Refinement_for_Few-Shot_Class-Incremental_Learning_CVPR_2021_paper.pdf" target="“blank”">Self-Promoted Prototype Refinement for Few-Shot Class-Incremental Learning</a></b>
<br>Kai Zhu, Yang Cao, <b>Wei Zhai</b>, Jie Cheng, Zheng-Jun Zha.
<br><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR 2021</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;zhu2021self-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;zhu2021self-bib&#39;)">bibtex</a> / <a href="https://arxiv.org/abs/2107.08918" target=&ldquo;blank&rdquo;><font color=#BB0000>arxiv</font></a> / <a href="https://github.com/zhukaii/SPPR" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="zhu2021self-abs" style="display: none;">
Few-shot class-incremental learning is to recognize the new classes given few samples and not forget the old classes. It is a challenging task since representation optimization and prototype reorganization can only be achieved under little supervision. To address this problem, we propose a novel incremental prototype learning scheme. Our scheme consists of a random episode selection strategy that adapts the feature representation to various generated incremental episodes to enhance the corresponding extensibility, and a self-promoted prototype refinement mechanism which strengthens the expression ability of the new classes by explicitly considering the dependencies among different classes. Particularly, a dynamic relation projection module is proposed to calculate the relation matrix in a shared embedding space and leverage it as the factor for bootstrapping the update of prototypes. Extensive experiments on three benchmark datasets demonstrate the above-par incremental performance, outperforming state-of-the-art methods by a margin of 13%, 17% and 11%, respectively.
</i>
</p>
<p align="justify">
</p><pre id="zhu2021self-bib" style="display: none;">
@inproceedings{zhu2021self,
    title={Self-promoted prototype refinement for few-shot class-incremental learning},
    author={Zhu, Kai and Cao, Yang and Zhai, Wei and Cheng, Jie and Zha, Zheng-Jun},
    booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
    pages={6801--6810},
    year={2021}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("zhu2021self-bib");hideblock("zhu2021self-abs");</script>
<p></p>
</div></div>


<p><b>2020</b></p>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/zhu2020self.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://www.ijcai.org/proceedings/2020/0142.pdf" target="“blank”">Self-Supervised Tuning for Few-Shot Segmentation</a></b>
<br>Kai Zhu, <b>Wei Zhai</b>, Yang Cao.
<br><i>International Joint Conferences on Artificial Intelligence Organization (<b>IJCAI 2020, <i><font color=#FF0000>Oral</font></i></b>).</i>
<br>
<a href="javascript:toggleblock(&#39;zhu2020self-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;zhu2020self-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="zhu2020self-abs" style="display: none;">
Few-shot segmentation aims at assigning a category label to each image pixel with few annotated samples. It is a challenging task since the dense prediction can only be achieved under the guidance of latent features defined by sparse annotations. Existing meta-learning method tends to fail in generating category-specifically discriminative descriptor when the visual features extracted from support images are marginalized in embedding space. To address this issue, this paper presents an adaptive tuning framework, in which the distribution of latent features across different episodes is dynamically adjusted based on a self-segmentation scheme, augmenting category-specific descriptors for label prediction. Specifically, a novel self-supervised inner-loop is firstly devised as the base learner to extract the underlying semantic features from the support image. Then, gradient maps are calculated by back-propagating self-supervised loss through the obtained features, and leveraged as guidance for augmenting the corresponding elements in embedding space. Finally, with the ability to continuously learn from different episodes, an optimization-based meta-learner is adopted as outer loop of our proposed framework to gradually refine the segmentation results. Extensive experiments on benchmark PASCAL-5i and COCO-20i datasets demonstrate the superiority of our proposed method over state-of-the-art.
</i>
</p>
<p align="justify">
</p><pre id="zhu2020self-bib" style="display: none;">
@inproceedings{zhu2021self,
    title={Self-supervised tuning for few-shot segmentation},
    author={Zhu, Kai and Zhai, Wei and Cao, Yang},
    booktitle={Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence},
    pages={1019--1025},
    year={2021}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("zhu2020self-bib");hideblock("zhu2020self-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/wang2020deep.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://ieeexplore.ieee.org/abstract/document/9190822" target="“blank”">Deep Inhomogeneous Regularization for Transfer Learning</a></b>
<br>Wen Wang, <b>Wei Zhai</b>, Yang Cao.
<br><i>IEEE International Conference on Image Processing (ICIP 2020).</i>
<br>
<a href="javascript:toggleblock(&#39;wang2020deep-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;wang2020deep-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="wang2020deep-abs" style="display: none;">
Fine-tuning is an effective transfer learning method to achieve ideal performance on target task with limited training data. Some recent works regularize parameters of deep neural networks for better knowledge transfer. However, these methods enforce homogeneous penalties for all parameters, resulting in catastrophic forgetting or negative transfer. To address this problem, we propose a novel Inhomogeneous Regularization (IR) method that imposes a strong regularization on parameters of transferable convolutional filters to tackle catastrophic forgetting and alleviate the regularization on parameters of less transferable filters to tackle negative transfer. Moreover, we use the decaying averaged deviation of parameters from the start point (pre-trained parameters) to accurately measure the transferability of each filter. Evaluation on the three challenging benchmarks datasets has demonstrated the superiority of the proposed model against state-of-the-art methods.
</i>
</p>
<p align="justify">
</p><pre id="wang2020deep-bib" style="display: none;">
@inproceedings{wang2020deep,
    title={Deep inhomogeneous regularization for transfer learning},
    author={Wang, Wen and Zhai, Wei and Cao, Yang},
    
    booktitle={2020 IEEE International Conference on Image Processing (ICIP)},
    pages={221--225},
    year={2020},
    organization={IEEE}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("wang2020deep-bib");hideblock("wang2020deep-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/zhai2020deep.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhai_Deep_Structure-Revealed_Network_for_Texture_Recognition_CVPR_2020_paper.pdf" target="“blank”">Deep Structure-Revealed Network for Texture Recognition</a></b>
<br><b>Wei Zhai</b>, Yang Cao, Zheng-Jun Zha, HaiYong Xie, Feng Wu.
<br><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR 2020, <i><font color=#FF0000>Oral</font></i></b>).</i>
<br>
<a href="javascript:toggleblock(&#39;zhai2020deep-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;zhai2020deep-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="zhai2020deep-abs" style="display: none;">
Texture recognition is a challenging visual task since various primitives along with their arrangements can be recognized from a same texture image when perceiving with different contexts. Some recent work building on CNNs exploits orderless aggregating to provide invariance to spatial arrangements. However, these methods ignore the inherent structural property of textures, which is a critical cue for distinguishing and describing texture images in the wild. To address this problem, we propose a novel Deep Structure-Revealed Network (DSR-Net) that leverages spatial dependency among the captured primitives as structural representation for texture recognition. Specifically, a primitive capturing module (PCM) is devised to generate multiple primitives from eight directional spatial contexts, in which deep features are firstly extracted under the constrains of direction map and then encoded based on the similarities of neighborhood. Next, these primitives are associated with a dependence learning module (DLM) to generate structural representation, in which a two-way collaborative relationship strategy is introduced to perceive the spatial dependencies among multiple primitives. At last, the structure-revealed texture representations are integrated with spatial ordered information to achieve real-world texture recognition. Evaluation on the five most challenging texture recognition datasets has demonstrated the superiority of the proposed model against state-of-the-art methods. The structure-revealed performances of DSR-Net are further verified on some extensive experiments, including fine-grained classification and semantic segmentation.
</i>
</p>
<p align="justify">
</p><pre id="zhai2020deep-bib" style="display: none;">
@inproceedings{zhai2020deep,
    title={Deep structure-revealed network for texture recognition},
    author={Zhai, Wei and Cao, Yang and Zha, Zheng-Jun and Xie, HaiYong and Wu, Feng},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={11010--11019},
    year={2020}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("zhai2020deep-bib");hideblock("zhai2020deep-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/zhu2020one.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://ieeexplore.ieee.org/abstract/document/9224176" target="“blank”">One-Shot Texture Retrieval Using Global Grouping Metric</a></b>
<br>Kai Zhu, Yang Cao, <b>Wei Zhai</b>, Zheng-Jun Zha.
<br><i>IEEE Transactions on Multimedia (<b>T-MM 2020</b>).</i>
<br><i><font color=#666666>Journal version of "<b>One-Shot Texture Retrieval with Global Context Metric</b>" (IJCAI 2019)</font></i>
<br>
<a href="javascript:toggleblock(&#39;zhu2020one-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;zhu2020one-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="zhu2020one-abs" style="display: none;">
Texture retrieval is widely used in the fields of fashion and e-commerce. This paper presents the problem of one-shot texture retrieval: given an example of a new reference texture, we aim to detect and segment all pixels of the same texture category within an arbitrary image. To address this problem, an OS-TR network is proposed to encode both reference and query images into a texture representation space, and a better comparison is made based on the global grouping information. Because the learned texture representation should be invariant to the spatial layout while preserving the rough semantic concepts, we introduce an adaptive directionality-aware module to finely discriminate the orderless texture details. To make full use of the global context information given only a few examples, we incorporate a grouping-attention mechanism into the relation network, resulting in the per-channel modulation of the local relation features. Extensive experiments on two benchmark datasets (i.e., the DTD and ADE20K dataset) and real scenarios demonstrate that our proposed method can achieve above-par segmentation performance and robust generalization across domains.
</i>
</p>
<p align="justify">
</p><pre id="zhu2020one-bib" style="display: none;">
@article{zhu2020one,
    title={One-shot texture retrieval using global grouping metric},
    author={Zhu, Kai and Cao, Yang and Zhai, Wei and Zha, Zheng-Jun},
    journal={IEEE Transactions on Multimedia},
    volume={23},
    pages={3726--3737},
    year={2020},
    publisher={IEEE}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("zhu2020one-bib");hideblock("zhu2020one-abs");</script>
<p></p>
</div></div>


<p><b>2019</b></p>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/zhai2019deep.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhai_Deep_Multiple-Attribute-Perceived_Network_for_Real-World_Texture_Recognition_ICCV_2019_paper.pdf" target="“blank”">Deep Multiple-Attribute-Perceived Network for Real-World Texture Recognition</a></b>
<br><b>Wei Zhai</b>, Yang Cao, Jing Zhang, Zheng-Jun Zha.
<br><i>IEEE/CVF International Conference on Computer Vision (<b>ICCV 2019</b>).</i>
<br>
<a href="javascript:toggleblock(&#39;zhai2019deep-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;zhai2019deep-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="zhai2019deep-abs" style="display: none;">
Texture recognition is a challenging visual task as multiple perceptual attributes may be perceived from the same texture image when combined with different spatial context. Some recent works building upon Convolutional Neural Network (CNN) incorporate feature encoding with orderless aggregating to provide invariance to spatial layouts. However, these existing methods ignore visual texture attributes, which are important cues for describing the real-world texture images, resulting in incomplete description and inaccurate recognition. To address this problem, we propose a novel deep Multiple-Attribute-Perceived Network (MAP-Net) by progressively learning visual texture attributes in a mutually reinforced manner. Specifically, a multi-branch network architecture is devised, in which cascaded global contexts are learned by introducing similarity constraint at each branch, and leveraged as guidance of spatial feature encoding at next branch through an attribute transfer scheme. To enhance the modeling capability of spatial transformation, a deformable pooling strategy is introduced to augment the spatial sampling with adaptive offsets to the global context, leading to perceive new visual attributes. An attribute fusion module is then introduced to jointly utilize the perceived visual attributes and the abstracted semantic concepts at each branch. Experimental results on the five most challenging texture recognition datasets have demonstrated the superiority of the proposed model against the state-of-the-arts.
</i>
</p>
<p align="justify">
</p><pre id="zhai2019deep-bib" style="display: none;">
@inproceedings{zhai2019deep,
    title={Deep multiple-attribute-perceived network for real-world texture recognition},
    author={Zhai, Wei and Cao, Yang and Zhang, Jing and Zha, Zheng-Jun},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
    pages={3613--3622},
    year={2019}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("zhai2019deep-bib");hideblock("zhai2019deep-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/zhu2019one.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://arxiv.org/pdf/1905.06656.pdf" target="“blank”">One-Shot Texture Retrieval with Global Context Metric</a></b>
<br>Kai Zhu, <b>Wei Zhai</b>, Zheng-Jun Zha, Yang Cao.
<br><i>International Joint Conferences on Artificial Intelligence Organization (<b>IJCAI 2019, <i><font color=#FF0000>Oral</font></i></b>).</i>
<br>
<a href="javascript:toggleblock(&#39;zhu2019one-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;zhu2019one-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="zhu2019one-abs" style="display: none;">
In this paper, we tackle one-shot texture retrieval: given an example of a new reference texture, detect and segment all the pixels of the same texture category within an arbitrary image. To address this problem, we present an OS-TR network to encode both reference and query image, leading to achieve texture segmentation towards the reference category. Unlike the existing texture encoding methods that integrate CNN with orderless pooling, we propose a directionality-aware module to capture the texture variations at each direction, resulting in spatially invariant representation. To segment new categories given only few examples, we incorporate a self-gating mechanism into relation network to exploit global context information for adjusting per-channel modulation weights of local relation features. Extensive experiments on benchmark texture datasets and real scenarios demonstrate the above-par segmentation performance and robust generalization across domains of our proposed method.
</i>
</p>
<p align="justify">
</p><pre id="zhu2019one-bib" style="display: none;">
@article{zhu2019one,
    title={One-shot texture retrieval with global context metric},
    author={Zhu, Kai and Zhai, Wei and Zha, Zheng-Jun and Cao, Yang},
    journal={arXiv preprint arXiv:1905.06656},
    year={2019}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("zhu2019one-bib");hideblock("zhu2019one-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/wu2019pixtextgan.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://ietresearch.onlinelibrary.wiley.com/doi/pdfdirect/10.1049/iet-ipr.2018.6588" target="“blank”">PixTextGAN: Structure Aware Text Image Synthesis for License Plate Recognition</a></b>
<br>Shilian Wu, <b>Wei Zhai</b>, Yang Cao.
<br><i>IET Image Processing (IET-IP 2019).</i>
<br>
<a href="javascript:toggleblock(&#39;wu2019pixtextgan-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;wu2019pixtextgan-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="wu2019pixtextgan-abs" style="display: none;">
Rapid progress on text image recognition has been achieved with the development of deep-learning techniques. However, it is still a great challenge to achieve a comprehensive license plate recognition in the real scenes, since there are no publicly available large diverse datasets for the training of deep learning models. This paper aims at synthesising of license plate images with generative adversarial networks (GAN), refraining from collecting a vast amount of labelled data. The authors thus propose a novel PixTextGAN that leverages a controllable architecture that generates specific character structures for different text regions to generate synthetic license plate images with reasonable text details. Specifically, a comprehensive structure-aware loss function is presented to preserve the key characteristic of each character region and thus to achieve appearance adaption for better recognition. Qualitative and quantitative experiments demonstrate the superiority of authors’ proposed method in text image synthetisation over state-of-the-art GANs. Further experimental results of license plate recognition on ReId and CCPD dataset demonstrate that using the synthesised images by PixTextGAN can greatly improve the recognition accuracy.
</i>
</p>
<p align="justify">
</p><pre id="wu2019pixtextgan-bib" style="display: none;">
@article{wu2019pixtextgan,
    title={PixTextGAN: structure aware text image synthesis for license plate recognition},
    author={Wu, Shilian and Zhai, Wei and Cao, Yang},
    journal={IET Image Processing},
    volume={13},
    number={14},
    pages={2744--2752},
    year={2019},
    publisher={Wiley Online Library}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("wu2019pixtextgan-bib");hideblock("wu2019pixtextgan-abs");</script>
<p></p>
</div></div>


<p><b>2018</b></p>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/zhai2018generative.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://ieeexplore.ieee.org/abstract/document/8462364" target="“blank”">A Generative Adversarial Network Based Framework for Unsupervised Visual Surface Inspection</a></b>
<br><b>Wei Zhai</b>, Jiang Zhu, Yang Cao, Zengfu Wang.
<br><i>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2018, <b><i><font color=#FF0000>Oral</font></i></b>).</i>
<br>
<a href="javascript:toggleblock(&#39;zhai2018generative-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;zhai2018generative-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="zhai2018generative-abs" style="display: none;">
Visual surface inspection is a challenging task due to the highly inconsistent appearance of the target surfaces and the abnormal regions. Most of the state-of-the-art methods are highly dependent on the labelled training samples, which are difficult to collect in practical industrial applications. To address this problem, we propose a generative adversarial network based framework for unsupervised surface inspection. The generative adversarial network is trained to generate the fake images analogous to the normal surface images. It implies that a well-trained GAN indeed learns a good representation of the normal surface images in a latent feature space. And consequently, the discriminator of GAN can serve as a naturally one-class classifier. We use the first three conventional layer of the discriminator as the feature extractor, whose response is sensitive to the abnormal regions. Particularly, a multi-scale fusion strategy is adopted to fuse the responses of the three convolution layers and thus improve the segmentation performance of abnormal detection. Various experimental results demonstrate the effectiveness of our proposed method.
</i>
</p>
<p align="justify">
</p><pre id="zhai2018generative-bib" style="display: none;">
@inproceedings{zhai2018generative,
    title={A generative adversarial network based framework for unsupervised visual surface inspection},
    author={Zhai, Wei and Zhu, Jiang and Cao, Yang and Wang, Zengfu},
    booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
    pages={1283--1287},
    year={2018},
    organization={IEEE}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("zhai2018generative-bib");hideblock("zhai2018generative-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/zhu2018co.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-73603-7_8.pdf" target="“blank”">Co-Occurrent Structural Edge Detection for Color-Guided Depth Map Super-Resolution</a></b>
<br>Jiang Zhu, <b>Wei Zhai</b>, Yang Cao, Zheng-Jun Zha.
<br><i>International Conference on Multimedia Modeling (MMM 2018, <b><i><font color=#FF0000>Oral</font></i></b>).</i>
<br>
<a href="javascript:toggleblock(&#39;zhu2018co-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;zhu2018co-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="zhu2018co-abs" style="display: none;">
Although RGBD cameras can provide depth information in real scenes, the captured depth map is often of low resolution and insufficient quality compared to the color image. Typically, most of the existing methods work by assuming that the edges in depth map and its corresponding color image are more likely to occur simultaneously. However, when the color image is rich in detail, the high-frequency information which is non-existent in the depth map will be introduced into the depth map. In this paper, we propose a CNN-based method to detect the co-occurrent structural edge for color-guided depth map super-resolution. Firstly, we design an edge detection convolutional neural network (CNN) to obtain the co-occurrent structural edge in depth map and its corresponding color image. Then we pack the obtained co-occurrent structural edges and the interpolated low-resolution depth maps into another customized CNN for depth map super-resolution. The presented scheme can effectively interpret and exploit the structural correlation between the depth map and the color image. Additionally, recursive learning is adopted to reduce the parameters of the customized CNN for depth map super-resolution and avoid overfitting. Experimental results demonstrate the effectiveness and reliability of our proposed approach by comparing with the state-of-the-art methods.
</i>
</p>
<p align="justify">
</p><pre id="zhu2018co-bib" style="display: none;">
@inproceedings{zhu2018co,
    title={Co-occurrent structural edge detection for color-guided depth map super-resolution},
    author={Zhu, Jiang and Zhai, Wei and Cao, Yang and Zha, Zheng-Jun},
    booktitle={MultiMedia Modeling: 24th International Conference, MMM 2018, Bangkok, Thailand, February 5-7, 2018, Proceedings, Part I 24},
    pages={93--105},
    year={2018},
    organization={Springer}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("zhu2018co-bib");hideblock("zhu2018co-abs");</script>
<p></p>
</div></div>


<h2>Pre-prints </h2>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/yang2025sigman.gif" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://arxiv.org/abs/2504.06982" target="“blank”">SIGMAN: Scaling 3D Human Gaussian Generation with Millions of Assets</a></b>
<br>Yuhang Yang, Fengqi Liu, Yixing Lu, Qin Zhao, Pingyu Wu, <b>Wei Zhai</b>, Ran Yi, Yang Cao, Lizhuang Ma, Zheng-Jun Zha, Junting Dong.
<br><i>Arxiv.</i>
<br>
<a href="javascript:toggleblock(&#39;yang2025sigman-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;yang2025sigman-bib&#39;)">bibtex</a> / <a href="https://yyvhang.github.io/SIGMAN_3D/" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="yang2025sigman-abs" style="display: none;">
3D human digitization has long been a highly pursued yet challenging task. Existing methods aim to generate high-quality 3D digital humans from single or multiple views, but remain primarily constrained by current paradigms and the scarcity of 3D human assets. Specifically, recent approaches fall into several paradigms: optimization-based and feed-forward (both single-view regression and multi-view generation with reconstruction). However, they are limited by slow speed, low quality, cascade reasoning, and ambiguity in mapping low-dimensional planes to high-dimensional space due to occlusion and invisibility, respectively. Furthermore, existing 3D human assets remain small-scale, insufficient for large-scale training. To address these challenges, we propose a latent space generation paradigm for 3D human digitization, which involves compressing multi-view images into Gaussians via a UV-structured VAE, along with DiT-based conditional generation, we transform the ill-posed low-to-high-dimensional mapping problem into a learnable distribution shift, which also supports end-to-end inference. In addition, we employ the multi-view optimization approach combined with synthetic data to construct the HGS-1M dataset, which contains 1 million 3D Gaussian assets to support the large-scale training. Experimental results demonstrate that our paradigm, powered by large-scale training, produces high-quality 3D human Gaussians with intricate textures, facial details, and loose clothing deformation. All training code, models, and the dataset will be open-sourced.</i>
</p>
<p align="justify">
</p><pre id="yang2025sigman-bib" style="display: none;">

</pre>
<p></p>
<script language="JavaScript">hideblock("yang2025sigman-bib");hideblock("yang2025sigman-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/yang2025videogen.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://arxiv.org/abs/2503.23452" target="“blank”">VideoGen-Eval: Agent-based System for Video Generation Evaluation</a></b>
<br>Yuhang Yang, Shangkun Sun, Hongxiang Li, Ke Fan, Ailing Zeng, Feilin Han, <b>Wei Zhai</b>, Wei Liu, Yang Cao, Zheng-Jun Zha.
<br><i>Arxiv.</i>
<br>
<a href="javascript:toggleblock(&#39;yang2025videogen-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;yang2025videogen-bib&#39;)">bibtex</a> / <a href="https://github.com/AILab-CVC/VideoGen-Eval" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="yang2025videogen-abs" style="display: none;">
The rapid advancement of video generation has rendered existing evaluation systems inadequate for assessing state-of-the-art models, primarily due to simplistic prompts in current benchmarks, evaluation operators struggling with Out-of-Distribution (OOD) cases, and misalignment between computed metrics and human preferences. To bridge the gap, we propose VideoGen-Eval, an agent-based dynamic evaluation system that integrates content structuring, multimodal content judgment, and dimensional evaluation enhancement through fusion tools. Additionally, we introduce a video generation benchmark comprising 700 structured, content-rich prompts. Through evaluations of over 5,500 video outputs generated by 8 cutting-edge models, our agent system demonstrates strong alignment with human preferences across objective dimensions (e.g., background and appearance). By leveraging temporally dense evaluation tools, it also captures human preferences in motion-related aspects. Extensive experiments validate that our benchmark offers rich evaluation content and the proposed agent system reliably completes evaluations while aligning with human judgments. All the generated assets and agent code will be open-sourced.</i>
</p>
<p align="justify">
</p><pre id="yang2025videogen-bib" style="display: none;">

</pre>
<p></p>
<script language="JavaScript">hideblock("yang2025videogen-bib");hideblock("yang2025videogen-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/fang2025vangogh.gif" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://arxiv.org/abs/2501.09499" target="“blank”">VanGogh: A Unified Multimodal Diffusion-based Framework for Video Colorization</a></b>
<br>Zixun Fang, Zhiheng Liu, Kai Zhu, Yu Liu, Ka Leong Cheng, <b>Wei Zhai</b>, Yang Cao, Zheng-Jun Zha
<br><i>Arxiv.</i>
<br>
<a href="javascript:toggleblock(&#39;fang2025vangogh-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;fang2025vangogh-bib&#39;)">bibtex</a> / <a href="https://becauseimbatman0.github.io/VanGogh" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="fang2025vangogh-abs" style="display: none;">
Video colorization aims to transform grayscale videos into vivid color representations while maintaining temporal consistency and structural integrity. Existing video colorization methods often suffer from color bleeding and lack comprehensive control, particularly under complex motion or diverse semantic cues. To this end, we introduce VanGogh, a unified multimodal diffusion-based framework for video colorization. VanGogh tackles these challenges using a Dual Qformer to align and fuse features from multiple modalities, complemented by a depth-guided generation process and an optical flow loss, which help reduce color overflow. Additionally, a color injection strategy and luma channel replacement are implemented to improve generalization and mitigate flickering artifacts. Thanks to this design, users can exercise both global and local control over the generation process, resulting in higher-quality colorized videos. Extensive qualitative and quantitative evaluations, and user studies, demonstrate that VanGogh achieves superior temporal consistency and color fidelity.</i>
</p>
<p align="justify">
</p><pre id="fang2025vangogh-bib" style="display: none;">
@misc{fang2025vangoghunifiedmultimodaldiffusionbased,
    title={VanGogh: A Unified Multimodal Diffusion-based Framework for Video Colorization}, 
    author={Zixun Fang and Zhiheng Liu and Kai Zhu and Yu Liu and Ka Leong Cheng and Wei Zhai and Yang Cao and Zheng-Jun Zha},
    year={2025},
    eprint={2501.09499},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2501.09499}, 
}
</pre>
<p></p>
<script language="JavaScript">hideblock("fang2025vangogh-bib");hideblock("fang2025vangogh-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/yu2025hero.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://arxiv.org/abs/2503.08270" target="“blank”">HERO: Human Reaction Generation from Videos</a></b>
<br>Chengjun Yu, <b>Wei Zhai</b>, Yuhang Yang, Yang Cao, Zheng-Jun Zha.
<br><i>Arxiv.</i>
<br>
<a href="javascript:toggleblock(&#39;yu2025hero-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;yu2025hero-bib&#39;)">bibtex</a> / <a href="https://jackyu6.github.io/HERO/" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="yu2025hero-abs" style="display: none;">
Human reaction generation represents a significant research domain for interactive AI, as humans constantly interact with their surroundings. Previous works focus mainly on synthesizing the reactive motion given a human motion sequence. This paradigm limits interaction categories to human-human interactions and ignores emotions that may influence reaction generation. In this work, we propose to generate 3D human reactions from RGB videos, which involves a wider range of interaction categories and naturally provides information about expressions that may reflect the subject's emotions. To cope with this task, we present HERO, a simple yet powerful framework for Human rEaction geneRation from videOs. HERO considers both global and frame-level local representations of the video to extract the interaction intention, and then uses the extracted interaction intention to guide the synthesis of the reaction. Besides, local visual representations are continuously injected into the model to maximize the exploitation of the dynamic properties inherent in videos. Furthermore, the ViMo dataset containing paired Video-Motion data is collected to support the task. In addition to human-human interactions, these video-motion pairs also cover animal-human interactions and scene-human interactions. Extensive experiments demonstrate the superiority of our methodology. The code and dataset will be publicly available at this https URL.</i>
</p>
<p align="justify">
</p><pre id="yu2025hero-bib" style="display: none;">
@inproceedings{yu2025hero,
    title={HERO: Human Reaction Generation from Videos},
    author={Yu, Chengjun and Zhai, Wei and Yang, Yuhang and Cao, Yang and Zha, Zheng-Jun},
    journal={arXiv preprint arXiv:2503.08270},
    year={2025}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("yu2025hero-bib");hideblock("yu2025hero-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/huang2024leverage.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://arxiv.org/abs/2411.16082" target="“blank”">Learning Object Affordance Ranking with Task Context</a></b>
<br>Haojie Huang, Hongchen Luo, <b>Wei Zhai</b>, Yang Cao, Zheng-Jun Zha.
<br><i>Arxiv.</i>
<br>
<a href="javascript:toggleblock(&#39;huang2024leverage-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;huang2024leverage-bib&#39;)">bibtex</a> / <a href="https://wpy1999.github.io/IV-VAE/" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="huang2024leverage-abs" style="display: none;">
Understanding the affordance of various objects in relation to task requirements is fundamental for an agent to perform daily activities. Most previous studies leverage task input to locate relevant objects and treat them equally, overlooking the internal priority difference among objects with the same affordance within the task context, which may lead to inappropriate decisions. To enable agents to develop a more fine-grained understanding of the objects required to perform tasks, we propose to learn object affordance ranking with task context, i.e., given image of a complex scene and the textual description of the affordance and task context, revealing task-object relationships and clarifying the priority rank of detected objects. To this end, we propose a novel Context-embed Group Ranking Framework with task relation mining and graph group update to deeply integrate task context and perform global relative relationship transmission. Due to lack of such data, we construct the first large-scale task-oriented affordance ranking dataset with 25 common tasks, over 50k images and more than 661k objects. Experimental results demonstrate the feasibility of the task context based affordance learning paradigm and the superiority of our model over state-of-the-art models in the fields of saliency ranking and multimodal object detection. The source code and dataset will open to the public.</i>
</p>
<p align="justify">
</p><pre id="huang2024leverage-bib" style="display: none;">
@article{huang2024leverage,
title={Leverage Task Context for Object Affordance Ranking},
author={Huang, Haojie and Luo, Hongchen and Zhai, Wei and Cao, Yang and Zha, Zheng-Jun},
journal={arXiv preprint arXiv:2411.16082},
year={2024}
}
</pre>
<p></p>
<script language="JavaScript">hideblock("huang2024leverage-bib");hideblock("huang2024leverage-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/chen2025event.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://arxiv.org/abs/2504.07503" target="“blank”">Event Signal Filtering via Probability Flux Estimation</a></b>
<br>Jinze Chen, <b>Wei Zhai</b>, Yang Cao, Bin Li, Zheng-Jun Zha.
<br><i>Arxiv.</i>
<br>
<a href="javascript:toggleblock(&#39;chen2025event-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;chen2025event-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="chen2025event-abs" style="display: none;">
Events offer a novel paradigm for capturing scene dynamics via asynchronous sensing, but their inherent randomness often leads to degraded signal quality. Event signal filtering is thus essential for enhancing fidelity by reducing this internal randomness and ensuring consistent outputs across diverse acquisition conditions. Unlike traditional time series that rely on fixed temporal sampling to capture steady-state behaviors, events encode transient dynamics through polarity and event intervals, making signal modeling significantly more complex. To address this, the theoretical foundation of event generation is revisited through the lens of diffusion processes. The state and process information within events is modeled as continuous probability flux at threshold boundaries of the underlying irradiance diffusion. Building on this insight, a generative, online filtering framework called Event Density Flow Filter (EDFilter) is introduced. EDFilter estimates event correlation by reconstructing the continuous probability flux from discrete events using nonparametric kernel smoothing, and then resamples filtered events from this flux. To optimize fidelity over time, spatial and temporal kernels are employed in a time-varying optimization framework. A fast recursive solver with O(1) complexity is proposed, leveraging state-space models and lookup tables for efficient likelihood computation. Furthermore, a new real-world benchmark Rotary Event Dataset (RED) is released, offering microsecond-level ground truth irradiance for full-reference event filtering evaluation. Extensive experiments validate EDFilter's performance across tasks like event filtering, super-resolution, and direct event-based blob tracking. Significant gains in downstream applications such as SLAM and video reconstruction underscore its robustness and effectiveness.</i>
</p>
<p align="justify">
</p><pre id="chen2025event-bib" style="display: none;">
@article{chen2025event,
title={Event Signal Filtering via Probability Flux Estimation},
author={Chen, Jinze and Zhai, Wei and Cao, Yang and Li, Bin and Zha, Zheng-Jun},
journal={arXiv preprint arXiv:2504.07503},
year={2025}
}
</pre>
<p></p>
<script language="JavaScript">hideblock("chen2025event-bib");hideblock("chen2025event-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/han2024event.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://arxiv.org/abs/2412.01300" target="“blank”">MATE: Motion-Augmented Temporal Consistency for Event-based Point Tracking</a></b>
<br>Han Han, <b>Wei Zhai</b>, Yang Cao, Bin Li, Zheng-Jun Zha.
<br><i>Arxiv.</i>
<br>
<a href="javascript:toggleblock(&#39;han2024event-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;han2024event-bib&#39;)">bibtex</a> / <a href="https://hh-xiaohu.github.io/Ev-TAP/" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="han2024event-abs" style="display: none;">
Tracking Any Point (TAP) plays a crucial role in motion analysis. Video-based approaches rely on iterative local matching for tracking, but they assume linear motion during the blind time between frames, which leads to target point loss under large displacements or nonlinear motion. The high temporal resolution and motion blur-free characteristics of event cameras provide continuous, fine-grained motion information, capturing subtle variations with microsecond precision. This paper presents an event-based framework for tracking any point, which tackles the challenges posed by spatial sparsity and motion sensitivity in events through two tailored modules. Specifically, to resolve ambiguities caused by event sparsity, a motion-guidance module incorporates kinematic features into the local matching process. Additionally, a variable motion aware module is integrated to ensure temporally consistent responses that are insensitive to varying velocities, thereby enhancing matching precision. To validate the effectiveness of the approach, an event dataset for tracking any point is constructed by simulation, and is applied in experiments together with two real-world datasets. The experimental results show that the proposed method outperforms existing SOTA methods. Moreover, it achieves 150\% faster processing with competitive model parameters.</i>
</p>
<p align="justify">
</p><pre id="han2024event-bib" style="display: none;">
@article{han2024event,
title={Event-Based Tracking Any Point with Motion-Augmented Temporal Consistency},
author={Han, Han and Zhai, Wei and Cao, Yang and Li, Bin and Zha, Zheng-jun},
journal={arXiv preprint arXiv:2412.01300},
year={2024}
}
</pre>
<p></p>
<script language="JavaScript">hideblock("han2024event-bib");hideblock("han2024event-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/wan2025emotive.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://arxiv.org/abs/2503.11371" target="“blank”">EMoTive: Event-guided Trajectory Modeling for 3D Motion Estimation</a></b>
<br>Zengyu Wan, <b>Wei Zhai</b>, Yang Cao, Zheng-Jun Zha.
<br><i>Arxiv.</i>
<br>
<a href="javascript:toggleblock(&#39;wan2025emotive-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;wan2025emotive-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="wan2025emotive-abs" style="display: none;">
Visual 3D motion estimation aims to infer the motion of 2D pixels in 3D space based on visual cues. The key challenge arises from depth variation induced spatio-temporal motion inconsistencies, disrupting the assumptions of local spatial or temporal motion smoothness in previous motion estimation frameworks. In contrast, event cameras offer new possibilities for 3D motion estimation through continuous adaptive pixel-level responses to scene changes. This paper presents EMoTive, a novel event-based framework that models spatio-temporal trajectories via event-guided non-uniform parametric curves, effectively characterizing locally heterogeneous spatio-temporal motion. Specifically, we first introduce Event Kymograph - an event projection method that leverages a continuous temporal projection kernel and decouples spatial observations to encode fine-grained temporal evolution explicitly. For motion representation, we introduce a density-aware adaptation mechanism to fuse spatial and temporal features under event guidance, coupled with a non-uniform rational curve parameterization framework to adaptively model heterogeneous trajectories. The final 3D motion estimation is achieved through multi-temporal sampling of parametric trajectories, yielding optical flow and depth motion fields. To facilitate evaluation, we introduce CarlaEvent3D, a multi-dynamic synthetic dataset for comprehensive validation. Extensive experiments on both this dataset and a real-world benchmark demonstrate the effectiveness of the proposed method.</i>
</p>
<p align="justify">
</p><pre id="wan2025emotive-bib" style="display: none;">

</pre>
<p></p>
<script language="JavaScript">hideblock("wan2025emotive-bib");hideblock("wan2025emotive-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/liao2024ef.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://arxiv.org/pdf/2410.15392" target="“blank”">EF-3DGS: Event-Aided Free-Trajectory 3D Gaussian Splatting</a></b>
<br>Bohao Liao, <b>Wei Zhai</b>, Zengyu Wan, Zhixin Cheng, Wenfei Yang, Yang Cao, Tianzhu Zhang, Zheng-Jun Zha.
<br><i>Arxiv.</i>
<br>
<a href="javascript:toggleblock(&#39;liao2024ef-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;liao2024ef-bib&#39;)">bibtex</a> / <a href="https://lbh666.github.io/ef-3dgs/" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="liao2024ef-abs" style="display: none;">
Scene reconstruction from casually captured videos has wide real-world applications. Despite recent progress, existing methods relying on traditional cameras tend to fail in high-speed scenarios due to insufficient observations and inaccurate pose estimation. Event cameras, inspired by biological vision, record pixel-wise intensity changes asynchronously with high temporal resolution and low latency, providing valuable scene and motion information in blind inter-frame intervals. In this paper, we introduce the event cameras to aid scene construction from a casually captured video for the first time, and propose Event-Aided Free-Trajectory 3DGS, called \textbf{EF-3DGS}, which seamlessly integrates the advantages of event cameras into 3DGS through three key components. First, we leverage the Event Generation Model (EGM) to fuse events and frames, enabling continuous supervision between discrete frames. Second, we extract motion information through Contrast Maximization (CMax) of warped events, which calibrates camera poses and provides gradient-domain constraints for 3DGS.Third, to address the absence of color information in events, we combine photometric bundle adjustment (PBA) with a Fixed-GS training strategy that separates structure and color optimization, effectively ensuring color consistency across different views. We evaluate our method on the public Tanks and Temples benchmark and a newly collected real-world dataset, RealEv-DAVIS. Our method achieves up to 3dB higher PSNR and 40% lower Absolute Trajectory Error (ATE) compared to state-of-the-art methods under challenging high-speed scenarios.
</i>
</p>
<p align="justify">
</p><pre id="liao2024ef-bib" style="display: none;">
@article{liao2024ef,
title={EF-3DGS: Event-Aided Free-Trajectory 3D Gaussian Splatting},
author={Liao, Bohao and Zhai, Wei and Wan, Zengyu and Zhang, Tianzhu and Cao, Yang and Zha, Zheng-Jun},
journal={arXiv preprint arXiv:2410.15392},
year={2024}
}
</pre>
<p></p>
<script language="JavaScript">hideblock("liao2024ef-bib");hideblock("liao2024ef-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/luo2024visual.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://arxiv.org/pdf/2410.11363" target="“blank”">Visual-Geometric Collaborative Guidance for Affordance Learning</a></b>
<br>Hongchen Luo, <b>Wei Zhai</b>, Jiao Wang, Yang Cao, Zheng-Jun Zha.
<br><i>Arxiv.</i>
<br><i><font color=#666666>Journal version of "<b>Leverage Interactive Affinity for Affordance Learning</b>" (CVPR 2023)</font></i>
<br>
<a href="javascript:toggleblock(&#39;luo2024visual-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;luo2024visual-bib&#39;)">bibtex</a> / <a href="https://github.com/lhc1224/VCR-Net" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="luo2024visual-abs" style="display: none;">
Perceiving potential ``action possibilities'' (\ie, affordance) regions of images and learning interactive functionalities of objects from human demonstration is a challenging task due to the diversity of human-object interactions. Prevailing affordance learning algorithms often adopt the label assignment paradigm and presume that there is a unique relationship between functional region and affordance label, yielding poor performance when adapting to unseen environments with large appearance variations. In this paper, we propose to leverage interactive affinity for affordance learning, \ie extracting interactive affinity from human-object interaction and transferring it to non-interactive objects. Interactive affinity, which represents the contacts between different parts of the human body and local regions of the target object, can provide inherent cues of interconnectivity between humans and objects, thereby reducing the ambiguity of the perceived action possibilities. To this end, we propose a visual-geometric collaborative guided affordance learning network that incorporates visual and geometric cues to excavate interactive affinity from human-object interactions jointly. Besides, a contact-driven affordance learning (CAL) dataset is constructed by collecting and labeling over 55,047 images. Experimental results demonstrate that our method outperforms the representative models regarding objective metrics and visual quality.
</i>
</p>
<p align="justify">
</p><pre id="luo2024visual-bib" style="display: none;">
@article{luo2024visual,
    title={Visual-Geometric Collaborative Guidance for Affordance Learning},
    author={Luo, Hongchen and Zhai, Wei and Wang, Jiao and Cao, Yang and Zha, Zheng-Jun},
    journal={arXiv preprint arXiv:2410.11363},
    year={2024}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("luo2024visual-bib");hideblock("luo2024visual-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/deng2024vmad.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://arxiv.org/pdf/2409.20146" target="“blank”">VMAD: Visual-enhanced Multimodal Large Language Model for Zero-Shot Anomaly Detection</a></b>
<br>Huilin Deng, Hongchen Luo, <b>Wei Zhai</b>, Yang Cao, Yu Kang.
<br><i>Arxiv.</i>
<br>
<a href="javascript:toggleblock(&#39;deng2024vmad-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;deng2024vmad-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="deng2024vmad-abs" style="display: none;">
Zero-shot anomaly detection (ZSAD) recognizes and localizes anomalies in previously unseen objects by establishing feature mapping between textual prompts and inspection images, demonstrating excellent research value in flexible industrial manufacturing. However, existing ZSAD methods are limited by closed-world settings, struggling to unseen defects with predefined prompts. Recently, adapting Multimodal Large Language Models (MLLMs) for Industrial Anomaly Detection (IAD) presents a viable solution. Unlike fixed-prompt methods, MLLMs exhibit a generative paradigm with open-ended text interpretation, enabling more adaptive anomaly analysis. However, this adaption faces inherent challenges as anomalies often manifest in finegrained regions and exhibit minimal visual discrepancies from normal samples. To address these challenges, we propose a novel framework VMAD (Visual-enhanced MLLM Anomaly Detection) that enhances MLLM with visual-based IAD knowledge and finegrained perception, simultaneously providing precise detection and comprehensive analysis of anomalies. Specifically, we design a Defect-Sensitive Structure Learning scheme that transfers patchsimilarities cues from visual branch to our MLLM for improved anomaly discrimination. Besides, we introduce a novel visual projector, Locality-enhanced Token Compression, which mines multi-level features in local contexts to enhance fine-grained detection. Furthermore, we introduce the Real Industrial Anomaly Detection (RIAD), a comprehensive IAD dataset with detailed anomaly descriptions and analyses, offering a valuable resource for MLLM-based IAD development. Extensive experiments on zero-shot benchmarks, including MVTec-AD, Visa, WFDD, and RIAD datasets, demonstrate our superior performance over stateof-the-art methods.
</i>
</p>
<p align="justify">
</p><pre id="deng2024vmad-bib" style="display: none;">
@article{deng2024vmad,
    title={VMAD: Visual-enhanced Multimodal Large Language Model for Zero-Shot Anomaly Detection},
    author={Deng, Huilin and Luo, Hongchen and Zhai, Wei and Cao, Yang and Kang, Yu},
    journal={arXiv preprint arXiv:2409.20146},
    year={2024}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("deng2024vmad-bib");hideblock("deng2024vmad-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/liu2024grounding.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://arxiv.org/pdf/2409.19650" target="“blank”">Grounding 3D Scene Affordance From Egocentric Interactions</a></b>
<br>Cuiyu Liu, <b>Wei Zhai</b>, Yuhang Yang, Hongchen Luo, Sen Liang, Yang Cao, Zheng-Jun Zha.
<br><i>Arxiv.</i>
<br>
<a href="javascript:toggleblock(&#39;liu2024grounding-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;liu2024grounding-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="liu2024grounding-abs" style="display: none;">
Grounding 3D scene affordance aims to locate interactive regions in 3D environments, which is crucial for embodied agents to interact intelligently with their surroundings. Most existing approaches achieve this by mapping semantics to 3D instances based on static geometric structure and visual appearance. This passive strategy limits the agent’s ability to actively perceive and engage with the environment, making it reliant on predefined semantic instructions. In contrast, humans develop complex interaction skills by observing and imitating how others interact with their surroundings. To empower the model with such abilities, we introduce a novel task: grounding 3D scene affordance from egocentric interactions, where the goal is to identify the corresponding affordance regions in a 3D scene based on an egocentric video of an interaction. This task faces the challenges of spatial complexity and alignment complexity across multiple sources. To address these challenges, we propose the Egocentric Interaction-driven 3D Scene Affordance Grounding (Ego-SAG) framework, which utilizes interaction intent to guide the model in focusing on interaction-relevant sub-regions and aligns affordance features from different sources through a bidirectional query decoder mechanism. Furthermore, we introduce the Egocentric Video-3D Scene Affordance Dataset (VSAD), covering a wide range of common interaction types and diverse 3D environments to support this task. Extensive experiments on VSAD validate both the feasibility of the proposed task and the effectiveness of our approach.
</i>
</p>
<p align="justify">
</p><pre id="liu2024grounding-bib" style="display: none;">
@article{liu2024grounding,
    title={Grounding 3D Scene Affordance From Egocentric Interactions},
    author={Liu, Cuiyu and Zhai, Wei and Yang, Yuhang and Luo, Hongchen and Liang, Sen and Cao, Yang and Zha, Zheng-Jun},
    journal={arXiv preprint arXiv:2409.19650},
    year={2024}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("liu2024grounding-bib");hideblock("liu2024grounding-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/zhang2024pear.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://arxiv.org/pdf/2407.21510" target="“blank”">PEAR: Phrase-Based Hand-Object Interaction Anticipation</a></b>
<br>Zichen Zhang, Hongchen Luo, <b>Wei Zhai</b>, Yang Cao, Yu Kang.
<br><i>Arxiv.</i>
<br>
<a href="javascript:toggleblock(&#39;zhang2024pear-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;zhang2024pear-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="zhang2024pear-abs" style="display: none;">
First-person hand-object interaction anticipation aims to predict the interaction process over a forthcoming period based on current scenes and prompts. This capability is crucial for embodied intelligence and human-robot collaboration. The complete interaction process involves both pre-contact interaction intention (i.e., hand motion trends and interaction hotspots) and post-contact interaction manipulation (i.e., manipulation trajectories and hand poses with contact). Existing research typically anticipates only interaction intention while neglecting manipulation, resulting in incomplete predictions and an increased likelihood of intention errors due to the lack of manipulation constraints. To address this, we propose a novel model, PEAR (Phrase-Based Hand-Object Interaction Anticipation), which jointly anticipates interaction intention and manipulation. To handle uncertainties in the interaction process, we employ a twofold approach. Firstly, we perform cross-alignment of verbs, nouns, and images to reduce the diversity of hand movement patterns and object functional attributes, thereby mitigating intention uncertainty. Secondly, we establish bidirectional constraints between intention and manipulation using dynamic integration and residual connections, ensuring consistency among elements and thus overcoming manipulation uncertainty. To rigorously evaluate the performance of the proposed model, we collect a new task-relevant dataset, EGO-HOIP, with comprehensive annotations. Extensive experimental results demonstrate the superiority of our method.
</i>
</p>
<p align="justify">
</p><pre id="zhang2024pear-bib" style="display: none;">
@article{zhang2024pear,
    title={PEAR: Phrase-Based Hand-Object Interaction Anticipation},
    author={Zhang, Zichen and Luo, Hongchen and Zhai, Wei and Cao, Yang and Kang, Yu},
    journal={arXiv preprint arXiv:2407.21510},
    year={2024}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("zhang2024pear-bib");hideblock("zhang2024pear-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/fang2024vivid.gif" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://arxiv.org/abs/2405.11794" target="“blank”">ViViD: Video Virtual Try-on using Diffusion Models</a></b>
<br>Zixun Fang, <b>Wei Zhai</b>, Aimin Su, Hongliang Song, Kai Zhu, Mao Wang, Yu Chen, Zhiheng Liu, Yang Cao, Zheng-Jun Zha.
<br><i>Arxiv.</i>
<br>
<a href="javascript:toggleblock(&#39;fang2024vivid-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;fang2024vivid-bib&#39;)">bibtex</a> / <a href="https://github.com/alibaba-yuanjing-aigclab/ViViD" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="fang2024vivid-abs" style="display: none;">
Video virtual try-on aims to transfer a clothing item onto the video of a target person. Directly applying the technique of image-based try-on to the video domain in a frame-wise manner will cause temporal-inconsistent outcomes while previous video-based try-on solutions can only generate low visual quality and blurring results. In this work, we present ViViD, a novel framework employing powerful diffusion models to tackle the task of video virtual try-on. Specifically, we design the Garment Encoder to extract fine-grained clothing semantic features, guiding the model to capture garment details and inject them into the target video through the proposed attention feature fusion mechanism. To ensure spatial-temporal consistency, we introduce a lightweight Pose Encoder to encode pose signals, enabling the model to learn the interactions between clothing and human posture and insert hierarchical Temporal Modules into the text-to-image stable diffusion model for more coherent and lifelike video synthesis. Furthermore, we collect a new dataset, which is the largest, with the most diverse types of garments and the highest resolution for the task of video virtual try-on to date. Extensive experiments demonstrate that our approach is able to yield satisfactory video try-on results. The dataset, codes, and weights will be publicly available.
</i>
</p>
<p align="justify">
</p><pre id="fang2024vivid-bib" style="display: none;">
@article{fang2024vivid,
    title={ViViD: Video Virtual Try-on using Diffusion Models},
    author={Fang, Zixun and Zhai, Wei and Su, Aimin and Song, Hongliang and Zhu, Kai and Wang, Mao and Chen, Yu and Liu, Zhiheng and Cao, Yang and Zha, Zheng-Jun},
    journal={arXiv preprint arXiv:2405.11794},
    year={2024}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("fang2024vivid-bib");hideblock("fang2024vivid-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/luo2024intention.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://arxiv.org/abs/2403.09194" target="“blank”">Intention-driven Ego-to-Exo Video Generation</a></b>
<br>Hongchen Luo, Kai Zhu, <b>Wei Zhai</b>, Yang Cao.
<br><i>Arxiv.</i>
<br>
<a href="javascript:toggleblock(&#39;han2023action-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;han2023action-bib&#39;)">bibtex</a></p>
<p>
</p><p align="justify">
<i id="han2023action-abs" style="display: none;">
Ego-to-exo video generation refers to generating the corresponding exocentric video according to the egocentric video, providing valuable applications in AR/VR and embodied AI. Benefiting from advancements in diffusion model techniques, notable progress has been achieved in video generation. However, existing methods build upon the spatiotemporal consistency assumptions between adjacent frames, which cannot be satisfied in the ego-to-exo scenarios due to drastic changes in views. To this end, this paper proposes an Intention-Driven Ego-to-exo video generation framework (IDE) that leverages action intention consisting of human movement and action description as view-independent representation to guide video generation, preserving the consistency of content and motion. Specifically, the egocentric head trajectory is first estimated through multi-view stereo matching. Then, cross-view feature perception module is introduced to establish correspondences between exo- and ego- views, guiding the trajectory transformation module to infer human full-body movement from the head trajectory. Meanwhile, we present an action description unit that maps the action semantics into the feature space consistent with the exocentric image. Finally, the inferred human movement and high-level action descriptions jointly guide the generation of exocentric motion and interaction content (i.e., corresponding optical flow and occlusion maps) in the backward process of the diffusion model, ultimately warping them into the corresponding exocentric video. We conduct extensive experiments on the relevant dataset with diverse exo-ego video pairs, and our IDE outperforms state-of-the-art models in both subjective and objective assessments, demonstrating its efficacy in ego-to-exo video generation.
</i>
</p>
<p align="justify">
</p><pre id="han2023action-bib" style="display: none;">
@article{luo2024intention,
    title={Intention-driven Ego-to-Exo Video Generation},
    author={Luo, Hongchen and Zhu, Kai and Zhai, Wei and Cao, Yang},
    journal={arXiv preprint arXiv:2403.09194},
    year={2024}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("han2023action-bib");hideblock("han2023action-abs");</script>
<p></p>
</div></div>


<div id="text-img-container"><div id="img-container">
<img src="./Wei Zhai_files/lu2023likelihood.png" alt="" width="200px"></div>
<div id="text-container"><p><b><a href="https://arxiv.org/abs/2312.01732" target="“blank”">Likelihood-Aware Semantic Alignment for Full-Spectrum Out-of-Distribution Detection</a></b>
<br>Fan Lu, Kai Zhu, Kecheng Zheng, <b>Wei Zhai</b>, Yang Cao.
<br><i>Arxiv.</i>
<br>
<a href="javascript:toggleblock(&#39;lu2023likelihood-abs&#39;)">abstract</a> / <a href="javascript:toggleblock(&#39;lu2023likelihood-bib&#39;)">bibtex</a> / <a href="https://github.com/LuFan31/LSA" target=&ldquo;blank&rdquo;><font color=#BB0000>code</font></a></p>
<p>
</p><p align="justify">
<i id="lu2023likelihood-abs" style="display: none;">
Full-spectrum out-of-distribution (F-OOD) detection aims to accurately recognize in-distribution (ID) samples while encountering semantic and covariate shifts simultaneously. However, existing out-of-distribution (OOD) detectors tend to overfit the covariance information and ignore intrinsic semantic correlation, inadequate for adapting to complex domain transformations. To address this issue, we propose a Likelihood-Aware Semantic Alignment (LSA) framework to promote the image-text correspondence into semantically high-likelihood regions. LSA consists of an offline Gaussian sampling strategy which efficiently samples semantic-relevant visual embeddings from the class-conditional Gaussian distribution, and a bidirectional prompt customization mechanism that adjusts both ID-related and negative context for discriminative ID/OOD boundary. Extensive experiments demonstrate the remarkable OOD detection performance of our proposed LSA especially on the intractable Near-OOD setting, surpassing existing methods by a margin of 15.26% and 18.88% on two F-OOD benchmarks, respectively.
</i>
</p>
<p align="justify">
</p><pre id="lu2023likelihood-bib" style="display: none;">
@article{lu2023likelihood,
    title={Likelihood-Aware Semantic Alignment for Full-Spectrum Out-of-Distribution Detection},
    author={Lu, Fan and Zhu, Kai and Zheng, Kecheng and Zhai, Wei and Cao, Yang},
    journal={arXiv preprint arXiv:2312.01732},
    year={2023}
    }
</pre>
<p></p>
<script language="JavaScript">hideblock("lu2023likelihood-bib");hideblock("lu2023likelihood-abs");</script>
<p></p>
</div></div>


<h2>Professional Activities</h2>

<p><b>Conference Reviewer:</b></p>
<ul>
<li><p>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</p></li>
<li><p>IEEE International Conference on Computer Vision (ICCV)</p></li>
<li><p>European Conference on Computer Vision (ECCV)</p></li>
<li><p>Neural Information Processing Systems (NeurIPS)</p></li>
<li><p>International Conference on Learning Representations (ICLR)</p></li>
<li><p>International Conference on Machine Learning (ICML)</p></li>
<li><p>AAAI Conference on Artificial Intelligence (AAAI)</p></li>
<li><p>ACM Multimedia (ACM MM)</p></li>
<li><p>International Joint Conferences on Artificial Intelligence Organization (IJCAI)

</p></li>
</ul>

<p><b>Journal Reviewer:</b></p>
<ul>
<li><p>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</p></li>
<li><p>International Journal of Computer Vision (IJCV)</p></li>
<li><p>IEEE Transactions on Image Processing (T-IP)</p></li>
<li><p>IEEE Transactions on Neural Networks and Learning Systems (T-NNLS)</p></li>
<li><p>IEEE Transactions on Circuits and Systems for Video Technology (T-CSVT)</p></li>
<li><p>IEEE Transactions on Multimedia (T-MM)</p></li>
<li><p>Pattern Recognition (PR)</p></li>
<li><p>ACM Transactions on Multimedia Computing, Communications, and Applications (ToMM)</p></li>
</ul>

<h2>Awards and Honors</h2>

<ul>
<li><p>3D Contact Estimation Challenge-RHOBIN2024 CVPR Workshop, 2nd Place, 2024</p>
</li>
<li><p>Event-based Eye Tracking-AIS2024 CVPR Workshop, 1st Place, 2024</p>
</li>
<li><p>NTIRE 2024 Efficient Super-Resolution Challenge, 2nd Place, 2024</p>
</li>
<li><p>AAAI Distinguished Paper, 2023</p>
</li>
<li><p>Outstanding Internship at JD Eeplore Academy, 2021</p>
</li>
<li><p>National Scholarship (University of Science and Technology of China), 2019</p>
</li>
<li><p>Outstanding Graduate of Southwest Jiaotong University, 2017</p>
</li>
<li><p>National Scholarship (Southwest Jiaotong University), 2016</p>
</li>
</ul>


<h2>Teaching Assistants</h2>
<ul>
<li>
<p>Image Processing. (Autumn, 2019)</p>
</li>
<li>
<p>Computer Vision. (Autumn, 2020)</p>
</li>
</ul> 


<p style="text-align:right;font-size:small;"> Website adapted from <a href="https://saurabhg.web.illinois.edu/">Saurabh Gupta</a> </p>

</body></html>
